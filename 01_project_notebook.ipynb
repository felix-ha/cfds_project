{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macroeconomic forecasting: Can machine learning methods outperform traditional approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup of the notebook\n",
    "\n",
    "\n",
    "### Loading packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "# pytorch\n",
    "from torch import nn, no_grad, save, load\n",
    "from torch import from_numpy, zeros\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Real gross domestic product \n",
    "\n",
    "The gross domestic product (GDP) is the variable of interest.\n",
    "\n",
    "Source of the data public availabe on the website of the IMF [here](https://www.imf.org/en/Publications/WEO/weo-database/2020/October/download-entire-database) and provided via an Excel file called `WEOApr2020all.xls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"C:\\Users\\hauer\\Dropbox\\CFDS\\Project\\data\\WEOApr2020all.csv\"\n",
    "df_weo_real_gdp = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several types of data in this file. This is the description of the relevant subject, the growth of the GDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Annual percentages of constant price GDP are year-on-year changes; the base year is country-specific . Expenditure-based GDP is total final expenditures at purchasers? prices (including the f.o.b. value of exports of goods and services), less the f.o.b. value of imports of goods and services. [SNA 1993]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = df_weo_real_gdp['Subject Descriptor'] == 'Gross domestic product, constant prices'\n",
    "df_weo_real_gdp['Subject Notes'] \n",
    "\n",
    "df_weo_real_gdp.loc[idx, 'Subject Notes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Percent change'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weo_real_gdp.loc[idx, 'Units'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subject code given by the IMF is 'NGDP_RPCH'. This code will also occour in for the weo prediction data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NGDP_RPCH'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weo_real_gdp.loc[idx, 'WEO Subject Code'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data for the years are present in the columns of the dataframe, i exctract the relevant information and transpose it afterwars. I want the years as the rows and the variables as the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WEO Country Code', 'ISO', 'WEO Subject Code', 'Country',\n",
       "       'Subject Descriptor', 'Subject Notes', 'Units', 'Scale',\n",
       "       'Country/Series-specific Notes', '1980', '1981', '1982', '1983', '1984',\n",
       "       '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993',\n",
       "       '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002',\n",
       "       '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011',\n",
       "       '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020',\n",
       "       '2021', 'Estimates Start After'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weo_real_gdp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done with the function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_imf_woe_data(df_weo_real_gdp, country, remove_na=False):\n",
    "\n",
    "    df = df_weo_real_gdp[df_weo_real_gdp['Country'] == country]\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    available_variables = df['Subject Descriptor'].unique()\n",
    "    \n",
    "    for variable in available_variables:\n",
    "        df_curr = df[df['Subject Descriptor'] == variable]\n",
    "        df_curr = df_curr.iloc[:, 9:49]\n",
    "        df_curr = df_curr.transpose()\n",
    "        df_curr = df_curr.rename({df_curr.columns[0]: variable}, axis='columns')\n",
    "        result = pd.concat([result, df_curr], axis=1)\n",
    "        \n",
    "        \n",
    "    if remove_na:\n",
    "        result = result.dropna(axis=1) \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the GDP data i use this function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gdp_real(df_weo_real_gdp, country):\n",
    "    df = get_imf_woe_data(df_weo_real_gdp, country, remove_na=False)\n",
    "    df.index = df.index.astype(dtype='int64')   \n",
    "    df['GDP real'] = df['Gross domestic product, constant prices']  \n",
    "    df['GDP real'] = df['GDP real'].str.replace(',', '').astype('float')\n",
    "    return df['GDP real']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is for example the real GDP growth for germany:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980    1.272\n",
       "1981    0.110\n",
       "1982   -0.788\n",
       "1983    1.555\n",
       "1984    2.826\n",
       "1985    2.192\n",
       "1986    2.417\n",
       "1987    1.469\n",
       "1988    3.736\n",
       "1989    3.913\n",
       "1990    5.723\n",
       "1991    5.011\n",
       "1992    1.925\n",
       "1993   -0.976\n",
       "1994    2.395\n",
       "1995    1.541\n",
       "1996    0.814\n",
       "1997    1.790\n",
       "1998    2.019\n",
       "1999    1.885\n",
       "2000    2.905\n",
       "2001    1.689\n",
       "2002   -0.201\n",
       "2003   -0.708\n",
       "2004    1.186\n",
       "2005    0.728\n",
       "2006    3.815\n",
       "2007    2.975\n",
       "2008    0.965\n",
       "2009   -5.694\n",
       "2010    4.185\n",
       "2011    3.913\n",
       "2012    0.428\n",
       "2013    0.431\n",
       "2014    2.218\n",
       "2015    1.742\n",
       "2016    2.230\n",
       "2017    2.465\n",
       "2018    1.522\n",
       "2019    0.565\n",
       "Name: GDP real, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gdp_real(df_weo_real_gdp, 'Germany')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real GDP is availabe for the following 194 (one occurrence is Nan) countries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_woe_real = df_weo_real_gdp['Country'].unique()\n",
    "len(countries_woe_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Afghanistan', 'Albania', 'Algeria', 'Angola',\n",
       "       'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba',\n",
       "       'Australia', 'Austria', 'Azerbaijan', 'The Bahamas', 'Bahrain',\n",
       "       'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin',\n",
       "       'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana',\n",
       "       'Brazil', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso',\n",
       "       'Burundi', 'Cabo Verde', 'Cambodia', 'Cameroon', 'Canada',\n",
       "       'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia',\n",
       "       'Comoros', 'Democratic Republic of the Congo', 'Republic of Congo',\n",
       "       'Costa Rica', \"Côte d'Ivoire\", 'Croatia', 'Cyprus',\n",
       "       'Czech Republic', 'Denmark', 'Djibouti', 'Dominica',\n",
       "       'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador',\n",
       "       'Equatorial Guinea', 'Eritrea', 'Estonia', 'Eswatini', 'Ethiopia',\n",
       "       'Fiji', 'Finland', 'France', 'Gabon', 'The Gambia', 'Georgia',\n",
       "       'Germany', 'Ghana', 'Greece', 'Grenada', 'Guatemala', 'Guinea',\n",
       "       'Guinea-Bissau', 'Guyana', 'Haiti', 'Honduras', 'Hong Kong SAR',\n",
       "       'Hungary', 'Iceland', 'India', 'Indonesia',\n",
       "       'Islamic Republic of Iran', 'Iraq', 'Ireland', 'Israel', 'Italy',\n",
       "       'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati',\n",
       "       'Korea', 'Kosovo', 'Kuwait', 'Kyrgyz Republic', 'Lao P.D.R.',\n",
       "       'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Lithuania',\n",
       "       'Luxembourg', 'Macao SAR', 'Madagascar', 'Malawi', 'Malaysia',\n",
       "       'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Mauritania',\n",
       "       'Mauritius', 'Mexico', 'Micronesia', 'Moldova', 'Mongolia',\n",
       "       'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia',\n",
       "       'Nauru', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua',\n",
       "       'Niger', 'Nigeria', 'North Macedonia', 'Norway', 'Oman',\n",
       "       'Pakistan', 'Palau', 'Panama', 'Papua New Guinea', 'Paraguay',\n",
       "       'Peru', 'Philippines', 'Poland', 'Portugal', 'Puerto Rico',\n",
       "       'Qatar', 'Romania', 'Russia', 'Rwanda', 'Samoa', 'San Marino',\n",
       "       'São Tomé and Príncipe', 'Saudi Arabia', 'Senegal', 'Serbia',\n",
       "       'Seychelles', 'Sierra Leone', 'Singapore', 'Slovak Republic',\n",
       "       'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa',\n",
       "       'South Sudan', 'Spain', 'Sri Lanka', 'St. Kitts and Nevis',\n",
       "       'St. Lucia', 'St. Vincent and the Grenadines', 'Sudan', 'Suriname',\n",
       "       'Sweden', 'Switzerland', 'Syria', 'Taiwan Province of China',\n",
       "       'Tajikistan', 'Tanzania', 'Thailand', 'Timor-Leste', 'Togo',\n",
       "       'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey',\n",
       "       'Turkmenistan', 'Tuvalu', 'Uganda', 'Ukraine',\n",
       "       'United Arab Emirates', 'United Kingdom', 'United States',\n",
       "       'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Vietnam',\n",
       "       'Yemen', 'Zambia', 'Zimbabwe', nan], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_woe_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 The variables X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 IMF\n",
    "\n",
    "The IMF proivdes some economic variables along with the realised GDP, that are already in the `df_weo_real_gdp` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gross domestic product, constant prices</th>\n",
       "      <th>Gross domestic product, current prices</th>\n",
       "      <th>Gross domestic product per capita, constant prices</th>\n",
       "      <th>Inflation, average consumer prices</th>\n",
       "      <th>Inflation, end of period consumer prices</th>\n",
       "      <th>Unemployment rate</th>\n",
       "      <th>General government net lending/borrowing</th>\n",
       "      <th>Current account balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1.272</td>\n",
       "      <td>867.363</td>\n",
       "      <td>0.931</td>\n",
       "      <td>5.447</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>0.11</td>\n",
       "      <td>950.471</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>6.324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>-0.788</td>\n",
       "      <td>1,001.24</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>5.256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>1.555</td>\n",
       "      <td>1,056.63</td>\n",
       "      <td>1.91</td>\n",
       "      <td>3.284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>2.826</td>\n",
       "      <td>1,125.70</td>\n",
       "      <td>3.243</td>\n",
       "      <td>2.396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gross domestic product, constant prices  \\\n",
       "1980                                   1.272   \n",
       "1981                                    0.11   \n",
       "1982                                  -0.788   \n",
       "1983                                   1.555   \n",
       "1984                                   2.826   \n",
       "\n",
       "     Gross domestic product, current prices  \\\n",
       "1980                                867.363   \n",
       "1981                                950.471   \n",
       "1982                               1,001.24   \n",
       "1983                               1,056.63   \n",
       "1984                               1,125.70   \n",
       "\n",
       "     Gross domestic product per capita, constant prices  \\\n",
       "1980                                              0.931   \n",
       "1981                                             -0.078   \n",
       "1982                                             -0.717   \n",
       "1983                                               1.91   \n",
       "1984                                              3.243   \n",
       "\n",
       "     Inflation, average consumer prices  \\\n",
       "1980                              5.447   \n",
       "1981                              6.324   \n",
       "1982                              5.256   \n",
       "1983                              3.284   \n",
       "1984                              2.396   \n",
       "\n",
       "     Inflation, end of period consumer prices Unemployment rate  \\\n",
       "1980                                      NaN             3.359   \n",
       "1981                                      NaN             4.831   \n",
       "1982                                      NaN             6.734   \n",
       "1983                                      NaN             8.099   \n",
       "1984                                      NaN             8.058   \n",
       "\n",
       "     General government net lending/borrowing Current account balance  \n",
       "1980                                      NaN                  -1.782  \n",
       "1981                                      NaN                  -0.684  \n",
       "1982                                      NaN                   0.866  \n",
       "1983                                      NaN                   0.666  \n",
       "1984                                      NaN                   1.423  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imf_woe_data =  get_imf_woe_data(df_weo_real_gdp, 'Germany', remove_na=False)\n",
    "df_imf_woe_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the following quantities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflation, average consumer prices\n",
      "['Annual percentages of average consumer prices are year-on-year changes.']\n",
      "['Percent change']\n",
      "\n",
      "Unemployment rate\n",
      "['Unemployment rate can be defined by either the national definition, the ILO harmonized definition, or the OECD harmonized definition. The OECD harmonized unemployment rate gives the number of unemployed persons as a percentage of the labor force (the total number of people employed plus unemployed). [OECD Main Economic Indicators, OECD, monthly] As defined by the International Labour Organization, unemployed workers are those who are currently not working but are willing and able to work for pay, currently available to work, and have actively searched for work. [ILO, http://www.ilo.org/public/english/bureau/stat/res/index.htm]']\n",
      "['Percent of total labor force']\n",
      "\n",
      "General government net lending/borrowing\n",
      "['Net lending (+)/ borrowing (?) is calculated as revenue minus total expenditure. This is a core GFS balance that measures the extent to which general government is either putting financial resources at the disposal of other sectors in the economy and nonresidents (net lending), or utilizing the financial resources generated by other sectors and nonresidents (net borrowing). This balance may be viewed as an indicator of the financial impact of general government activity on the rest of the economy and nonresidents (GFSM 2001, paragraph 4.17). Note: Net lending (+)/borrowing (?) is also equal to net acquisition of financial assets minus net incurrence of liabilities.']\n",
      "['Percent of GDP']\n",
      "\n",
      "Current account balance\n",
      "['Current account is all transactions other than those in financial and capital items. The major classifications are goods and services, income and current transfers. The focus of the BOP is on transactions (between an economy and the rest of the world) in goods, services, and income.']\n",
      "['Percent of GDP']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imf_woe_variables = ['Inflation, average consumer prices', 'Unemployment rate', 'General government net lending/borrowing', 'Current account balance']\n",
    "\n",
    "for x in imf_woe_variables:\n",
    "    idx = df_weo_real_gdp['Subject Descriptor'] == x\n",
    "    print(x)\n",
    "    print(df_weo_real_gdp.loc[idx, 'Subject Notes'].unique())\n",
    "    print(df_weo_real_gdp.loc[idx, 'Units'].unique())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So only `Inflation, average consumer prices` is given by an annual percentage change, the other variables needs to be transformed later. \n",
    "\n",
    "The dataframe for example for germany will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inflation, average consumer prices</th>\n",
       "      <th>Unemployment rate</th>\n",
       "      <th>General government net lending/borrowing</th>\n",
       "      <th>Current account balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>5.447</td>\n",
       "      <td>3.359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>6.324</td>\n",
       "      <td>4.831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>5.256</td>\n",
       "      <td>6.734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>3.284</td>\n",
       "      <td>8.099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>2.396</td>\n",
       "      <td>8.058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>2.084</td>\n",
       "      <td>8.124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>-0.125</td>\n",
       "      <td>7.834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>0.242</td>\n",
       "      <td>7.843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>1.274</td>\n",
       "      <td>7.735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>2.778</td>\n",
       "      <td>6.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>2.687</td>\n",
       "      <td>6.155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>3.474</td>\n",
       "      <td>5.47</td>\n",
       "      <td>-3.189</td>\n",
       "      <td>-1.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>5.046</td>\n",
       "      <td>6.592</td>\n",
       "      <td>-2.605</td>\n",
       "      <td>-1.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>4.476</td>\n",
       "      <td>7.775</td>\n",
       "      <td>-3.095</td>\n",
       "      <td>-1.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>2.717</td>\n",
       "      <td>8.425</td>\n",
       "      <td>-2.524</td>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1.733</td>\n",
       "      <td>8.233</td>\n",
       "      <td>-9.432</td>\n",
       "      <td>-1.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1.274</td>\n",
       "      <td>8.908</td>\n",
       "      <td>-3.57</td>\n",
       "      <td>-0.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1.496</td>\n",
       "      <td>9.658</td>\n",
       "      <td>-2.942</td>\n",
       "      <td>-0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.607</td>\n",
       "      <td>9.383</td>\n",
       "      <td>-2.567</td>\n",
       "      <td>-0.711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.657</td>\n",
       "      <td>8.558</td>\n",
       "      <td>-1.719</td>\n",
       "      <td>-1.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>1.413</td>\n",
       "      <td>7.95</td>\n",
       "      <td>-1.585</td>\n",
       "      <td>-1.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>1.911</td>\n",
       "      <td>7.8</td>\n",
       "      <td>-3.025</td>\n",
       "      <td>-0.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>1.295</td>\n",
       "      <td>8.6</td>\n",
       "      <td>-3.875</td>\n",
       "      <td>1.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>1.084</td>\n",
       "      <td>9.708</td>\n",
       "      <td>-3.704</td>\n",
       "      <td>1.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>1.75</td>\n",
       "      <td>10.333</td>\n",
       "      <td>-3.334</td>\n",
       "      <td>4.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>1.939</td>\n",
       "      <td>11.008</td>\n",
       "      <td>-3.319</td>\n",
       "      <td>4.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>1.795</td>\n",
       "      <td>10.042</td>\n",
       "      <td>-1.653</td>\n",
       "      <td>5.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>2.29</td>\n",
       "      <td>8.567</td>\n",
       "      <td>0.261</td>\n",
       "      <td>6.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>2.726</td>\n",
       "      <td>7.383</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>5.692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0.246</td>\n",
       "      <td>7.667</td>\n",
       "      <td>-3.151</td>\n",
       "      <td>5.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1.119</td>\n",
       "      <td>6.933</td>\n",
       "      <td>-4.379</td>\n",
       "      <td>5.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>2.501</td>\n",
       "      <td>5.858</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>6.213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>2.141</td>\n",
       "      <td>5.367</td>\n",
       "      <td>0.009</td>\n",
       "      <td>7.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>1.607</td>\n",
       "      <td>5.233</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6.557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0.761</td>\n",
       "      <td>5.008</td>\n",
       "      <td>0.58</td>\n",
       "      <td>7.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0.671</td>\n",
       "      <td>4.625</td>\n",
       "      <td>0.943</td>\n",
       "      <td>8.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.375</td>\n",
       "      <td>4.158</td>\n",
       "      <td>1.184</td>\n",
       "      <td>8.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>1.711</td>\n",
       "      <td>3.758</td>\n",
       "      <td>1.242</td>\n",
       "      <td>7.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>1.951</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.867</td>\n",
       "      <td>7.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>1.346</td>\n",
       "      <td>3.158</td>\n",
       "      <td>1.449</td>\n",
       "      <td>7.146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Inflation, average consumer prices Unemployment rate  \\\n",
       "1980                              5.447             3.359   \n",
       "1981                              6.324             4.831   \n",
       "1982                              5.256             6.734   \n",
       "1983                              3.284             8.099   \n",
       "1984                              2.396             8.058   \n",
       "1985                              2.084             8.124   \n",
       "1986                             -0.125             7.834   \n",
       "1987                              0.242             7.843   \n",
       "1988                              1.274             7.735   \n",
       "1989                              2.778              6.79   \n",
       "1990                              2.687             6.155   \n",
       "1991                              3.474              5.47   \n",
       "1992                              5.046             6.592   \n",
       "1993                              4.476             7.775   \n",
       "1994                              2.717             8.425   \n",
       "1995                              1.733             8.233   \n",
       "1996                              1.274             8.908   \n",
       "1997                              1.496             9.658   \n",
       "1998                              0.607             9.383   \n",
       "1999                              0.657             8.558   \n",
       "2000                              1.413              7.95   \n",
       "2001                              1.911               7.8   \n",
       "2002                              1.295               8.6   \n",
       "2003                              1.084             9.708   \n",
       "2004                               1.75            10.333   \n",
       "2005                              1.939            11.008   \n",
       "2006                              1.795            10.042   \n",
       "2007                               2.29             8.567   \n",
       "2008                              2.726             7.383   \n",
       "2009                              0.246             7.667   \n",
       "2010                              1.119             6.933   \n",
       "2011                              2.501             5.858   \n",
       "2012                              2.141             5.367   \n",
       "2013                              1.607             5.233   \n",
       "2014                              0.761             5.008   \n",
       "2015                              0.671             4.625   \n",
       "2016                              0.375             4.158   \n",
       "2017                              1.711             3.758   \n",
       "2018                              1.951               3.4   \n",
       "2019                              1.346             3.158   \n",
       "\n",
       "     General government net lending/borrowing Current account balance  \n",
       "1980                                      NaN                  -1.782  \n",
       "1981                                      NaN                  -0.684  \n",
       "1982                                      NaN                   0.866  \n",
       "1983                                      NaN                   0.666  \n",
       "1984                                      NaN                   1.423  \n",
       "1985                                      NaN                   2.662  \n",
       "1986                                      NaN                   4.024  \n",
       "1987                                      NaN                   3.709  \n",
       "1988                                      NaN                   4.317  \n",
       "1989                                      NaN                   4.689  \n",
       "1990                                      NaN                   3.131  \n",
       "1991                                   -3.189                  -1.422  \n",
       "1992                                   -2.605                  -1.183  \n",
       "1993                                   -3.095                  -1.037  \n",
       "1994                                   -2.524                    -1.5  \n",
       "1995                                   -9.432                  -1.246  \n",
       "1996                                    -3.57                  -0.676  \n",
       "1997                                   -2.942                   -0.51  \n",
       "1998                                   -2.567                  -0.711  \n",
       "1999                                   -1.719                  -1.422  \n",
       "2000                                   -1.585                  -1.761  \n",
       "2001                                   -3.025                  -0.368  \n",
       "2002                                   -3.875                   1.891  \n",
       "2003                                   -3.704                   1.414  \n",
       "2004                                   -3.334                    4.52  \n",
       "2005                                   -3.319                   4.673  \n",
       "2006                                   -1.653                   5.772  \n",
       "2007                                    0.261                   6.861  \n",
       "2008                                   -0.116                   5.692  \n",
       "2009                                   -3.151                   5.836  \n",
       "2010                                   -4.379                   5.744  \n",
       "2011                                   -0.881                   6.213  \n",
       "2012                                    0.009                   7.129  \n",
       "2013                                     0.04                   6.557  \n",
       "2014                                     0.58                   7.204  \n",
       "2015                                    0.943                    8.59  \n",
       "2016                                    1.184                   8.509  \n",
       "2017                                    1.242                   7.824  \n",
       "2018                                    1.867                   7.397  \n",
       "2019                                    1.449                   7.146  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imf_woe_data =  get_imf_woe_data(df_weo_real_gdp, 'Germany', remove_na=False)\n",
    "df_imf_woe_data[imf_woe_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 OECD\n",
    "\n",
    "The Organisation for Economic Co-operation and Development (OECD) provied also macroeconomic data in its [iLibrary]( https://www.oecd-ilibrary.org). The indicators can be browsed by theme and I choose 19 to use for my forecast. Each one is available by an `.csv` file. I load all of them together into one dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oecd_data(path_oecd, country): \n",
    " \n",
    "    result = pd.DataFrame()\n",
    "  \n",
    "    \n",
    "    for file_name in os.listdir(path_oecd):\n",
    "    \n",
    "        \n",
    "        file = os.path.join(path_oecd, file_name)\n",
    "        \n",
    "        df_orig = pd.read_csv(file)\n",
    "        unique_subjects = df_orig['SUBJECT'].unique()\n",
    "    \n",
    "        \n",
    "        for subject in unique_subjects:\n",
    "            \n",
    "            \n",
    "            df = df_orig.copy()\n",
    "            df = df[df['LOCATION'] == country] \n",
    "            df = df[df['SUBJECT'] == subject]\n",
    "            \n",
    "            # if there is only one unique subject, the name is TOT\n",
    "            if(len(unique_subjects) == 1):\n",
    "                subject = file_name[:-4]\n",
    "            \n",
    "            \n",
    "            df = df.rename({df.columns[6]: subject}, axis='columns')\n",
    "            \n",
    "            df = df.set_index('TIME')\n",
    "            \n",
    "            result = pd.concat([result, df[subject]], axis=1)           \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Air_Pollution</th>\n",
       "      <th>BuiltArea</th>\n",
       "      <th>RICE</th>\n",
       "      <th>WHEAT</th>\n",
       "      <th>MAIZE</th>\n",
       "      <th>SOYBEAN</th>\n",
       "      <th>CurrentAccountBalance</th>\n",
       "      <th>ExchangeR</th>\n",
       "      <th>GHG</th>\n",
       "      <th>Gini</th>\n",
       "      <th>...</th>\n",
       "      <th>POULTRY</th>\n",
       "      <th>SHEEP</th>\n",
       "      <th>PPP</th>\n",
       "      <th>ProtectedArea</th>\n",
       "      <th>STINT</th>\n",
       "      <th>TermsOfTrade</th>\n",
       "      <th>TradeGoodsExport</th>\n",
       "      <th>TradeGoodsImport</th>\n",
       "      <th>TradeServicesExport</th>\n",
       "      <th>TradeServicesImports</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>7.37633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.639440</td>\n",
       "      <td>3.545315</td>\n",
       "      <td>10.966925</td>\n",
       "      <td>3.493525</td>\n",
       "      <td>-2.109880</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.391</td>\n",
       "      <td>...</td>\n",
       "      <td>48.546693</td>\n",
       "      <td>0.452834</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.54</td>\n",
       "      <td>0.644167</td>\n",
       "      <td>101.649388</td>\n",
       "      <td>1451.022</td>\n",
       "      <td>2187.599</td>\n",
       "      <td>780531.0</td>\n",
       "      <td>511898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>7.36365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.865637</td>\n",
       "      <td>3.112192</td>\n",
       "      <td>11.084471</td>\n",
       "      <td>3.314508</td>\n",
       "      <td>-1.871310</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.390</td>\n",
       "      <td>...</td>\n",
       "      <td>48.995385</td>\n",
       "      <td>0.486937</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.54</td>\n",
       "      <td>1.152500</td>\n",
       "      <td>101.989233</td>\n",
       "      <td>1546.472</td>\n",
       "      <td>2339.885</td>\n",
       "      <td>830387.0</td>\n",
       "      <td>544836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.989749</td>\n",
       "      <td>3.199564</td>\n",
       "      <td>11.078778</td>\n",
       "      <td>3.503500</td>\n",
       "      <td>-2.185078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>49.682862</td>\n",
       "      <td>0.468909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.54</td>\n",
       "      <td>2.188333</td>\n",
       "      <td>102.492082</td>\n",
       "      <td>1665.688</td>\n",
       "      <td>2537.730</td>\n",
       "      <td>862434.0</td>\n",
       "      <td>562069.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.903469</td>\n",
       "      <td>3.214364</td>\n",
       "      <td>11.146953</td>\n",
       "      <td>3.297754</td>\n",
       "      <td>-2.241142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>50.071359</td>\n",
       "      <td>0.448355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1643.161</td>\n",
       "      <td>2497.532</td>\n",
       "      <td>875825.0</td>\n",
       "      <td>588359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Air_Pollution  BuiltArea      RICE     WHEAT      MAIZE   SOYBEAN  \\\n",
       "2016        7.37633        NaN  5.639440  3.545315  10.966925  3.493525   \n",
       "2017        7.36365        NaN  5.865637  3.112192  11.084471  3.314508   \n",
       "2018            NaN        NaN  5.989749  3.199564  11.078778  3.503500   \n",
       "2019            NaN        NaN  5.903469  3.214364  11.146953  3.297754   \n",
       "2020            NaN        NaN       NaN       NaN        NaN       NaN   \n",
       "\n",
       "      CurrentAccountBalance  ExchangeR   GHG   Gini  ...    POULTRY     SHEEP  \\\n",
       "2016              -2.109880        1.0  14.9  0.391  ...  48.546693  0.452834   \n",
       "2017              -1.871310        1.0  14.6  0.390  ...  48.995385  0.486937   \n",
       "2018              -2.185078        1.0   NaN    NaN  ...  49.682862  0.468909   \n",
       "2019              -2.241142        1.0   NaN    NaN  ...  50.071359  0.448355   \n",
       "2020                    NaN        NaN   NaN    NaN  ...        NaN       NaN   \n",
       "\n",
       "      PPP  ProtectedArea     STINT  TermsOfTrade  TradeGoodsExport  \\\n",
       "2016  1.0          12.54  0.644167    101.649388          1451.022   \n",
       "2017  1.0          12.54  1.152500    101.989233          1546.472   \n",
       "2018  1.0          12.54  2.188333    102.492082          1665.688   \n",
       "2019  1.0          12.54       NaN           NaN          1643.161   \n",
       "2020  NaN          12.54       NaN           NaN               NaN   \n",
       "\n",
       "      TradeGoodsImport  TradeServicesExport  TradeServicesImports  \n",
       "2016          2187.599             780531.0              511898.0  \n",
       "2017          2339.885             830387.0              544836.0  \n",
       "2018          2537.730             862434.0              562069.0  \n",
       "2019          2497.532             875825.0              588359.0  \n",
       "2020               NaN                  NaN                   NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_oecd = r'C:\\Users\\hauer\\Dropbox\\CFDS\\Project\\data\\OECD'\n",
    "\n",
    "df_oecd = get_oecd_data(path_oecd, 'USA')\n",
    "df_oecd.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the variables are given absolute values for the specific year, so every variable needs to be transformed afterwards.\n",
    "There are 182 countries or aggregated country groups available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_oecd = set()\n",
    "\n",
    "for file_name in os.listdir(path_oecd):\n",
    "    file = os.path.join(path_oecd, file_name)\n",
    "    df = pd.read_csv(file)\n",
    "    countries_current = set(df['LOCATION'].unique())\n",
    "    \n",
    "    countries_oecd = countries_oecd.union(countries_current)\n",
    "\n",
    "    \n",
    "countries_oecd = list(countries_oecd)\n",
    "len(countries_oecd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GHA',\n",
       " 'MKD',\n",
       " 'IDN',\n",
       " 'EST',\n",
       " 'BTN',\n",
       " 'NAM',\n",
       " 'DOM',\n",
       " 'TUN',\n",
       " 'GNQ',\n",
       " 'MEX',\n",
       " 'DEU',\n",
       " 'VNM',\n",
       " 'ARG',\n",
       " 'BIH',\n",
       " 'POL',\n",
       " 'HKG',\n",
       " 'KEN',\n",
       " 'MWI',\n",
       " 'DNK',\n",
       " 'SVN',\n",
       " 'USA',\n",
       " 'CRI',\n",
       " 'HTI',\n",
       " 'KGZ',\n",
       " 'NOR',\n",
       " 'LAO',\n",
       " 'DEW',\n",
       " 'BRN',\n",
       " 'NIC',\n",
       " 'CMR',\n",
       " 'TZA',\n",
       " 'ROU',\n",
       " 'ITA',\n",
       " 'GEO',\n",
       " 'LSO',\n",
       " 'ARE',\n",
       " 'IRL',\n",
       " 'CHE',\n",
       " 'LBN',\n",
       " 'UKR',\n",
       " 'NER',\n",
       " 'LIE',\n",
       " 'BRICS',\n",
       " 'CUB',\n",
       " 'GTM',\n",
       " 'QAT',\n",
       " 'GRC',\n",
       " 'SWZ',\n",
       " 'BLR',\n",
       " 'UGA',\n",
       " 'AZE',\n",
       " 'EU27_2020',\n",
       " 'MOZ',\n",
       " 'THA',\n",
       " 'SLE',\n",
       " 'PER',\n",
       " 'EU',\n",
       " 'BEN',\n",
       " 'AUS',\n",
       " 'AFG',\n",
       " 'TGO',\n",
       " 'MYS',\n",
       " 'MDA',\n",
       " 'ISR',\n",
       " 'AGO',\n",
       " 'SOM',\n",
       " 'ZMB',\n",
       " 'SLV',\n",
       " 'LVA',\n",
       " 'RUS',\n",
       " 'CAF',\n",
       " 'OAVG',\n",
       " 'NZL',\n",
       " 'JOR',\n",
       " 'PHL',\n",
       " 'IRQ',\n",
       " 'CZE',\n",
       " 'ZAF',\n",
       " 'LTU',\n",
       " 'ETH',\n",
       " 'GBR',\n",
       " 'MNG',\n",
       " 'G7M',\n",
       " 'BDI',\n",
       " 'URY',\n",
       " 'EA',\n",
       " 'NPL',\n",
       " 'HND',\n",
       " 'NLD',\n",
       " 'LUX',\n",
       " 'GMB',\n",
       " 'KWT',\n",
       " 'BGR',\n",
       " 'IND',\n",
       " 'MAR',\n",
       " 'MAC',\n",
       " 'SEN',\n",
       " 'OEU',\n",
       " 'EA19',\n",
       " 'JAM',\n",
       " 'G-20',\n",
       " 'BWA',\n",
       " 'BFA',\n",
       " 'AUT',\n",
       " 'PRY',\n",
       " 'RWA',\n",
       " 'BRA',\n",
       " 'VEN',\n",
       " 'OECD',\n",
       " 'MLI',\n",
       " 'COG',\n",
       " 'ECU',\n",
       " 'ALB',\n",
       " 'SWE',\n",
       " 'EU28',\n",
       " 'KOR',\n",
       " 'GAB',\n",
       " 'ERI',\n",
       " 'TLS',\n",
       " 'PAN',\n",
       " 'PRK',\n",
       " 'IRN',\n",
       " 'COD',\n",
       " 'SYR',\n",
       " 'MUS',\n",
       " 'KHM',\n",
       " 'TWN',\n",
       " 'TUR',\n",
       " 'G20',\n",
       " 'ARM',\n",
       " 'JPN',\n",
       " 'SGP',\n",
       " 'ISL',\n",
       " 'EGY',\n",
       " 'CHL',\n",
       " 'FJI',\n",
       " 'PRT',\n",
       " 'OECDE',\n",
       " 'BGD',\n",
       " 'ZWE',\n",
       " 'TJK',\n",
       " 'CHN',\n",
       " 'MNE',\n",
       " 'GNB',\n",
       " 'TKM',\n",
       " 'GIN',\n",
       " 'TTO',\n",
       " 'TCD',\n",
       " 'BEL',\n",
       " 'FRA',\n",
       " 'SVK',\n",
       " 'YEM',\n",
       " 'SRB',\n",
       " 'SDN',\n",
       " 'ESP',\n",
       " 'OMN',\n",
       " 'DZA',\n",
       " 'EU27',\n",
       " 'LKA',\n",
       " 'FIN',\n",
       " 'MDG',\n",
       " 'PAK',\n",
       " 'CYP',\n",
       " 'COL',\n",
       " 'MMR',\n",
       " 'G-7',\n",
       " 'HRV',\n",
       " 'WLD',\n",
       " 'PNG',\n",
       " 'SAU',\n",
       " 'BOL',\n",
       " 'LBY',\n",
       " 'UZB',\n",
       " 'MRT',\n",
       " 'CIV',\n",
       " 'NGA',\n",
       " 'LBR',\n",
       " 'MLT',\n",
       " 'HUN',\n",
       " 'BHR',\n",
       " 'CAN',\n",
       " 'KAZ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_oecd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the mapping from the ISO country code that used the OECD to the country names that are used by the IMF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ISO</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>DZA</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>AGO</td>\n",
       "      <td>Angola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ATG</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>191</td>\n",
       "      <td>VNM</td>\n",
       "      <td>Vietnam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>192</td>\n",
       "      <td>YEM</td>\n",
       "      <td>Yemen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>193</td>\n",
       "      <td>ZMB</td>\n",
       "      <td>Zambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>194</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  ISO              Country\n",
       "0      1  AFG          Afghanistan\n",
       "1      2  ALB              Albania\n",
       "2      3  DZA              Algeria\n",
       "3      4  AGO               Angola\n",
       "4      5  ATG  Antigua and Barbuda\n",
       "..   ...  ...                  ...\n",
       "190  191  VNM              Vietnam\n",
       "191  192  YEM                Yemen\n",
       "192  193  ZMB               Zambia\n",
       "193  194  ZWE             Zimbabwe\n",
       "194  195  NaN                  NaN\n",
       "\n",
       "[195 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'C:\\Users\\hauer\\Dropbox\\CFDS\\Project\\data\\Mapping_country_codes.csv'\n",
    "df_country_mapping =  pd.read_csv(path, sep = '\\t')\n",
    "\n",
    "df_country_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3  World Economic Outlook \n",
    "\n",
    "\n",
    "The International Monetary Fund publishes predictions of the GDP growth in its World Economic Outlook. The data can is taken from [here](https://www.imf.org/en/Publications/WEO/weo-database/2020/October) in the related links Historical WEO Forecasts Database. The data is provided in an Excel file called `WEOhistorical.xlsx`. The IMF publishes the WEO twice a year in spring and in fall. I will use the prediction of the fall, as this closer to the next year and therefore the prediction should be more precise. The data is formated the following: \n",
    "\n",
    "| country | year   |F1990ngdp_rpch|\n",
    "|------|--------|--------------|\n",
    "|   germany  | 1988  | 4.08 |\n",
    "|   germany  | 1989  | 2.96 |\n",
    "|   germany  | 1990  | 1.98 |\n",
    "|   germany  | 1991  | 2.44 |\n",
    "|   germany  | 1992  | 3.42 |\n",
    "|   germany  | 1993  | 3.45 |\n",
    "|   germany  | 1994  | 3.42 |\n",
    "|   germany  | 1995  | 3.40 |\n",
    "\n",
    "This is for example the WEO in fall of 1990 for germany. There are two years of historical data and 6 years of forecast data. The forecast can be found in the column `F1990ngdp_rpch`. This is the same subject code as for the realised GDP. I will only use the forecast for the next year, so for 1990 if will use the predicted growth of the GDP in 1991. I extract the forecast for a certain country and prediciton horizon with the following function. First I load the Excel file into an pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\hauer\\Dropbox\\CFDS\\Project\\data\\WEOhistorical.xlsx'\n",
    "df_weo =  pd.read_excel(path,sheet_name='ngdp_rpch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for the extraction of the WEO is called `get_predictions_weo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_weo(df_weo, country, start_forecast, end_forecast):\n",
    "       \n",
    "    df = df_weo[df_weo['country'] == country]\n",
    "    \n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'S' in col:\n",
    "            del df[col] \n",
    "            \n",
    "    del df['WEO_Country_Code']     \n",
    "    \n",
    "    \n",
    "    df = df[df['year'] >= start_forecast]\n",
    "    \n",
    "    \n",
    "    predictions_weo = []\n",
    "    years = np.arange(start_forecast, end_forecast+1, 1)\n",
    "    \n",
    "    for year in years:\n",
    "       \n",
    "        df_curr = df[df['year'] == year]\n",
    "        \n",
    "        year_WEO = year - 1 \n",
    "        column = 'F' + str(year_WEO) + 'ngdp_rpch'\n",
    "        y_pred_year = df_curr[column].values[0]\n",
    "        \n",
    "        predictions_weo.append(y_pred_year)\n",
    "    \n",
    "    predictions_weo = pd.Series(data = predictions_weo, index = years)\n",
    "    \n",
    "    return predictions_weo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is for example the WEO for germany for the years 2010 to 2018:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010    0.335834\n",
       "2011    2.021567\n",
       "2012    1.273123\n",
       "2013    0.852179\n",
       "2014    1.399657\n",
       "2015    1.451330\n",
       "2016    1.573023\n",
       "2017    1.425094\n",
       "2018    1.843451\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions_weo(df_weo, country = 'Germany', start_forecast =  2010, end_forecast = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WEO is available from 1980 for the following countries or aggregated country groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_woe = df_weo['country'].unique()\n",
    "len(countries_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['World', 'Advanced Economies', 'United States', 'United Kingdom',\n",
       "       'Austria', 'Belgium', 'Denmark', 'France', 'Germany', 'San Marino',\n",
       "       'Italy', 'Luxembourg', 'Netherlands', 'Norway', 'Sweden',\n",
       "       'Switzerland', 'Canada', 'Japan', 'Euro area', 'Finland', 'Greece',\n",
       "       'Iceland', 'Ireland', 'Malta', 'Portugal', 'Spain', 'Turkey',\n",
       "       'Australia', 'New Zealand', 'South Africa',\n",
       "       'Emerging Market and Developing Economies', 'Argentina', 'Bolivia',\n",
       "       'Brazil', 'Chile', 'Colombia', 'Costa Rica', 'Dominican Republic',\n",
       "       'Ecuador', 'El Salvador', 'Guatemala', 'Haiti', 'Honduras',\n",
       "       'Mexico', 'Nicaragua', 'Panama', 'Paraguay', 'Peru', 'Uruguay',\n",
       "       'Venezuela', 'Antigua and Barbuda', 'Bahamas, The', 'Aruba',\n",
       "       'Barbados', 'Dominica', 'Grenada', 'Guyana', 'Belize', 'Jamaica',\n",
       "       'Puerto Rico', 'St. Kitts and Nevis', 'St. Lucia',\n",
       "       'St. Vincent and the Grenadines', 'Suriname',\n",
       "       'Trinidad and Tobago', 'Bahrain', 'Cyprus', 'Iran', 'Iraq',\n",
       "       'Israel', 'Jordan', 'Kuwait', 'Lebanon', 'Oman', 'Qatar',\n",
       "       'Saudi Arabia', 'Syria', 'United Arab Emirates', 'Egypt', 'Yemen',\n",
       "       'West Bank and Gaza', 'Afghanistan', 'Bangladesh', 'Bhutan',\n",
       "       'Brunei Darussalam', 'Myanmar', 'Cambodia', 'Sri Lanka',\n",
       "       'Taiwan Province of China', 'Hong Kong SAR', 'India', 'Indonesia',\n",
       "       'Timor-Leste', 'Korea', 'Lao P.D.R.', 'Macao SAR', 'Malaysia',\n",
       "       'Maldives', 'Nepal', 'Pakistan', 'Palau', 'Philippines',\n",
       "       'Singapore', 'Thailand', 'Vietnam', 'Djibouti', 'Algeria',\n",
       "       'Angola', 'Botswana', 'Burundi', 'Cameroon', 'Cabo Verde',\n",
       "       'Central African Republic', 'Chad', 'Comoros',\n",
       "       'Congo, Republic of', 'Congo, Democratic Republic of the', 'Benin',\n",
       "       'Equatorial Guinea', 'Eritrea', 'Ethiopia', 'Gabon', 'Gambia, The',\n",
       "       'Ghana', 'Guinea-Bissau', 'Guinea', \"Côte d'Ivoire\", 'Kenya',\n",
       "       'Lesotho', 'Liberia', 'Libya', 'Madagascar', 'Malawi', 'Mali',\n",
       "       'Mauritania', 'Mauritius', 'Morocco', 'Mozambique', 'Niger',\n",
       "       'Nigeria', 'Zimbabwe', 'Rwanda', 'São Tomé and Príncipe',\n",
       "       'Seychelles', 'Senegal', 'Sierra Leone', 'Somalia', 'Namibia',\n",
       "       'Sudan', 'South Sudan', 'Eswatini', 'Tanzania', 'Togo', 'Tunisia',\n",
       "       'Uganda', 'Burkina Faso', 'Zambia', 'Solomon Islands', 'Fiji',\n",
       "       'Kiribati', 'Nauru', 'Vanuatu', 'Papua New Guinea', 'Samoa',\n",
       "       'Tonga', 'Marshall Islands', 'Micronesia', 'Tuvalu', 'Armenia',\n",
       "       'Azerbaijan', 'Belarus', 'Albania', 'Georgia', 'Kazakhstan',\n",
       "       'Kyrgyz Republic', 'Bulgaria', 'Moldova', 'Russia', 'Tajikistan',\n",
       "       'China', 'Turkmenistan', 'Ukraine', 'Uzbekistan', 'Czech Republic',\n",
       "       'Slovak Republic', 'Estonia', 'Latvia', 'Serbia',\n",
       "       'Montenegro, Rep. of', 'Hungary', 'Lithuania', 'Mongolia',\n",
       "       'Croatia', 'Slovenia', 'North Macedonia', 'Bosnia and Herzegovina',\n",
       "       'Poland', 'Kosovo', 'Romania'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_woe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Joinig the datasets\n",
    "\n",
    "\n",
    "Here I join the different data sets to get a dataframe for each country. The data is available from 1980 to 2019 expect for the OECD data sets. These are provided from 1970 to 2017 and hence I filter them to receive the time from 1980 to 2017. I use the pythonic try except block to select only countries, that have an correspondent ISO code in the OECD dataset.\n",
    "I will save each individual dataframe in a dictionary. For later convenience I also rename the column of the real gdp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errorindex 0 is out of bounds for axis 0 with size 0 for nan\n",
      "Errorcould not convert string to float: '--' for Mauritania\n",
      "Errorindex 0 is out of bounds for axis 0 with size 0 for Iran\n",
      "Errorcould not convert string to float: '--' for Ethiopia\n",
      "Errorindex 0 is out of bounds for axis 0 with size 0 for World\n",
      "Errorcould not convert string to float: '--' for Greece\n",
      "Errorindex 0 is out of bounds for axis 0 with size 0 for Bahamas, The\n",
      "Errorindex 0 is out of bounds for axis 0 with size 0 for Euro area\n",
      "Errorindex 0 is out of bounds for axis 0 with size 0 for Gambia, The\n",
      "Errorindex 0 is out of bounds for axis 0 with size 0 for Congo, Republic of\n",
      "Errorcould not convert string to float: '--' for Gabon\n",
      "Errorindex 0 is out of bounds for axis 0 with size 0 for Montenegro, Rep. of\n",
      "Errorindex 0 is out of bounds for axis 0 with size 0 for West Bank and Gaza\n",
      "Errorindex 0 is out of bounds for axis 0 with size 0 for Emerging Market and Developing Economies\n"
     ]
    }
   ],
   "source": [
    "database = {}\n",
    "\n",
    "availabe_countries_woe = set(countries_woe_real).union(countries_woe)\n",
    "\n",
    "for country in availabe_countries_woe:\n",
    "    try:\n",
    "        iso = df_country_mapping[df_country_mapping['Country'] == country]['ISO']\n",
    "        iso = iso.values[0]\n",
    "        \n",
    "        df_real_gdp = get_gdp_real(df_weo_real_gdp, country)\n",
    "        df_real_gdp = df_real_gdp[df_real_gdp.index <= 2017]\n",
    "        \n",
    "        df_imf_woe_data =  get_imf_woe_data(df_weo_real_gdp, country, remove_na=False)\n",
    "        df_imf_woe_data = df_imf_woe_data[imf_woe_variables]\n",
    "        df_imf_woe_data.index = df_imf_woe_data.index.astype(int)\n",
    "        df_imf_woe_data = df_imf_woe_data[df_imf_woe_data.index <= 2017]\n",
    "        \n",
    "        df_oecd = get_oecd_data(path_oecd, iso)\n",
    "        df_oecd = df_oecd[df_oecd.index >= 1980]\n",
    "        df_oecd = df_oecd[df_oecd.index <= 2017]\n",
    "        \n",
    "        \n",
    "        df = pd.concat([df_real_gdp, df_imf_woe_data, df_oecd], axis=1)\n",
    "        \n",
    "        df = df.rename(columns={\"GDP real\": \"y\"})\n",
    "        \n",
    "        database[country] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error' + str(e) + ' for ' + str(country))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have 189 dataframes that are ready to be analysed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(database.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing data type\n",
    "\n",
    "During the later analysis there occured problems with wrong datatypes. For example some vaules where saved as strings: `unsupported operand type(s) for /: 'str' and 'str'`. In this section I fix all kind of this problems. \n",
    "\n",
    "Here is a list of the issues which i fix in the same order in the following code: \n",
    "* Netherlands could not convert string to float: '--'\n",
    "* Brazil could not convert string to float: '1,430.72'\n",
    "* Hungary unsupported operand type(s) for /: 'str' and 'str'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_series(x):\n",
    "    try:\n",
    "        return x.str.replace(',', '').astype(float)\n",
    "    \n",
    "    # this operation only works with string vaules, i was not able to filter the other dtypes\n",
    "    # (that should only be float) but it did not work. \n",
    "    \n",
    "    except:\n",
    "        return x\n",
    "\n",
    "\n",
    "database_fixed = {}\n",
    "\n",
    "\n",
    "for country in database.keys():\n",
    "    try: \n",
    "        # Netherlands could not convert string to float: '--'\n",
    "        database_fixed[country] =  database[country].replace('--', np.nan)\n",
    "\n",
    "        # Brazil could not convert string to float: '1,430.72'\n",
    "        database_fixed[country] =  database[country].replace(',', '')\n",
    "\n",
    "        # Hungary unsupported operand type(s) for /: 'str' and 'str'\n",
    "        database_fixed[country]  = database[country].apply(convert_string_series)\n",
    "        \n",
    "       \n",
    "    except Exception as e:\n",
    "        print(country + \" \" + str(e))\n",
    "\n",
    "\n",
    "database = database_fixed.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filitering missing values\n",
    "\n",
    "First I define variables that i need later on. I will describe them before I use them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_missing_percentage = 0.6\n",
    "number_of_qualified_variables = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next i want to analyse how many values are missing. So I can decide which of the variables I will use for the models. To get an overview I count the number of missing values for all variables of all countries. To do this, I create a dictionary with all varibales available as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = database['France'].columns\n",
    "missing_dict = {var:0 for var in variables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_observations = 0\n",
    "for country in database.keys():\n",
    "    df =  database[country]   \n",
    "    number_of_observations += df.shape[0]\n",
    "\n",
    "    for column in df.columns:\n",
    "        column_current = df[column]\n",
    "        number_of_missing_observations = sum(column_current.isnull())\n",
    "        missing_dict[column] += number_of_missing_observations   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = pd.DataFrame.from_dict(missing_dict, orient='index')\n",
    "df_missing.columns = ['Number of missing entries']\n",
    "df_missing['Percent of missing entries'] = df_missing['Number of missing entries'] / number_of_observations * 100\n",
    "df_missing.sort_values(by='Percent of missing entries', ascending=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same analysis for a single country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'France'\n",
    "\n",
    "def get_overview_missing_values(country):\n",
    "    variables = database[country].columns\n",
    "    missing_dict = {var:0 for var in variables}\n",
    "\n",
    "\n",
    "    df =  database[country]   \n",
    "    number_of_observations = df.shape[0]\n",
    "\n",
    "    for column in df.columns:\n",
    "        column_current = df[column]\n",
    "        number_of_missing_observations = sum(column_current.isnull())\n",
    "        missing_dict[column] += number_of_missing_observations \n",
    "\n",
    "    df_missing = pd.DataFrame.from_dict(missing_dict, orient='index')\n",
    "    df_missing.columns = ['Number of missing entries']\n",
    "    df_missing['Percent of missing entries'] = df_missing['Number of missing entries'] / number_of_observations * 100\n",
    "    df_missing = df_missing.sort_values(by='Percent of missing entries', ascending=1)\n",
    "    \n",
    "    return df_missing\n",
    "\n",
    "df_missing = get_overview_missing_values(country) \n",
    "df_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want set a trheshold, when to not use a variable, because of too much missing data. A threshold `t_missing_percentage` means that every variable with more then 30% missing data will not qualify to be included for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_missing[df_missing['Percent of missing entries'] <= t_missing_percentage * 100]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_variables_filtered_na = df_filtered.shape[0]\n",
    "number_of_variables_filtered_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe of France has 16 variables that would qualify. Now I will see this number for all countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dict = {var:0 for var in database.keys()}\n",
    "number_of_variables = len(database['France'].columns)\n",
    "\n",
    "for country in database.keys():\n",
    "\n",
    "    df_missing = get_overview_missing_values(country) \n",
    "    df_filtered = df_missing[df_missing['Percent of missing entries'] <= t_missing_percentage * 100]\n",
    "    number_of_variables_filtered_na = df_filtered.shape[0]\n",
    "    \n",
    "    missing_dict[country] = number_of_variables_filtered_na \n",
    "    \n",
    "df_qualified_var_by_country = pd.DataFrame.from_dict(missing_dict, orient='index')\n",
    "df_qualified_var_by_country.columns = ['number of qualified variables']\n",
    "df_qualified_var_by_country['percent of qualified variables'] = df_qualified_var_by_country['number of qualified variables'] / number_of_variables * 100\n",
    "df_qualified_var_by_country = df_qualified_var_by_country.sort_values(by='percent of qualified variables', ascending=0)\n",
    "df_qualified_var_by_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I select only those countries, that have at least 15 qualifed variables. I use the variable `number_of_qualified_variables`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_countries =df_qualified_var_by_country[df_qualified_var_by_country['number of qualified variables'] >= number_of_qualified_variables]\n",
    "selected_countries = df_selected_countries.index\n",
    "df_selected_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I compute the intersection of those variables to ensure that the qualifed variables are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_variables = set(database['France'].columns)\n",
    "\n",
    "for country in selected_countries:\n",
    "\n",
    "    df_curr = get_overview_missing_values(country)   \n",
    "    df_curr = df_curr[df_curr['Percent of missing entries'] <= t_missing_percentage * 100]\n",
    "    final_variables = final_variables.intersection(set(df_curr.index))\n",
    "\n",
    "final_variables = list(final_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can filter the whole database so that only countries with the selected variables are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_clear_na = {}\n",
    "\n",
    "for country in database.keys():\n",
    "    \n",
    "    if country not in selected_countries:\n",
    "        continue\n",
    "        \n",
    "    df_curr = database[country]\n",
    "    df_curr = df_curr[final_variables]\n",
    "   \n",
    "    database_clear_na[country] = df_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the dataset\n",
    "\n",
    "The reference for this section is the book \"The Elements of Statistical Learning\" from Hastie et. al. \n",
    "\n",
    "To obtain an accurate Data Science process, it is necessary to split the whole dataset in certain subsets.  This is important for two reasons:\n",
    "\n",
    "* Model selection: estimating the performance of different models in order to choose the best one. The term model selection also includes the tuning of the hyperparameters, if you define a model as the tupel consisting of the data used for training, the concrete typ of model or algorithm and the hyperparameters of the later. \n",
    "* Model assessment: having chosen a final model, estimating its prediction error (generalization error) on new data.\n",
    "\n",
    "The data is split into a training, validation and test set. The training set is used to fit the model and the validation set is used to calculate the validation error. This error gives an estimate of its prediction error. The test is kept in an \"vault\" and is brought out only at the very end of the analysis. After using the test set, no changes in any step is allowed. Otherwise the test error will underestimate the true test error. With this test error the selection of the best model is done. \n",
    "\n",
    "Another thing in this procedure is very important: Every step in the analysis needs to be performed on the training set only. For example in the next section I will impute missing values. I will fit the algorithim that performes the imputation on the training set and predict the values then for both the training and test set. Otherwise the wrong applicaton of the impuation would also underestimate the validation or test error, because for the imputation there would be more information available than in an real life scenario. New data is completly unseen and only the training data is available for fitting the model in generall. \n",
    "\n",
    "This is also noted in the section 7.10.2 The Wrong and Right Way to Do Cross-validation. Even though I am not doing cross validation, this description suits the application in this project. \n",
    "\n",
    "\n",
    "I will use the years 2014 - 2017 as the test set. I will calculate  the performance on the very end and after i will not change anythin in the procedure. The years  2010 - 2013 will be the validation set and will be use for tuning the hypterparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(database):\n",
    "    database_training = {}\n",
    "    database_validation = {}\n",
    "    database_test = {}\n",
    "\n",
    "\n",
    "    for country in database.keys():\n",
    "\n",
    "\n",
    "        df = database[country]\n",
    "\n",
    "        #df_test = df[df.index > 2013]\n",
    "        #df_validation = df[(df.index > 2009) & (df.index <= 2013)]\n",
    "        #df_training = df[df.index <= 2009]\n",
    "        \n",
    "#         df_test = df[df.index > 2012]\n",
    "#         df_validation = df[(df.index > 2008) & (df.index <= 2012)]\n",
    "#         df_training = df[df.index <= 2008]\n",
    "        \n",
    "        df_test = df[df.index > 2010]\n",
    "        df_validation = df[(df.index > 2004) & (df.index <= 2010)]\n",
    "        df_training = df[df.index <= 2004]\n",
    "\n",
    "        database_training[country] = df_training\n",
    "        database_validation[country] = df_validation\n",
    "        database_test[country] = df_test\n",
    "        \n",
    "    return (database_training, database_validation, database_test)\n",
    "\n",
    "\n",
    "database_training, database_validation, database_test = split_dataset(database_clear_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing values\n",
    "\n",
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "d = {'col1': [np.nan, 2, 2, 3, 4, 1, np.nan, 2, 1, 5], 'col2': [np.nan, np.nan, 3, 2, 1, 99, np.nan, 9999, 34, 56]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "imputer.fit(df)\n",
    "\n",
    "df_data = imputer.transform(df)\n",
    "\n",
    "df = pd.DataFrame(data = df_data, columns = df.columns, index = df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "imputer = KNNImputer(missing_values=np.nan, n_neighbors=2, weights=\"uniform\")\n",
    "\n",
    "imputer.fit(df)\n",
    "\n",
    "df_data = imputer.transform(df)\n",
    "\n",
    "df = pd.DataFrame(data = df_data, columns = df.columns, index = df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for 'Latvia'. knn performes as mean, if all values are NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_training['Latvia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = database_training['Latvia']\n",
    "\n",
    "imputer = KNNImputer(missing_values=np.nan, n_neighbors=5, weights=\"uniform\")\n",
    "\n",
    "imputer.fit(df)\n",
    "\n",
    "df_data = imputer.transform(df)\n",
    "\n",
    "df = pd.DataFrame(data = df_data, columns = df.columns, index = df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = database_training['Latvia']\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "imputer.fit(df)\n",
    "\n",
    "df_data = imputer.transform(df)\n",
    "\n",
    "df = pd.DataFrame(data = df_data, columns = df.columns, index = df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the validation and training set the `transform` function is also applied to impute missing values for these datapoints: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = database_validation['Latvia']\n",
    "df_data = imputer.transform(df)\n",
    "df_validation = pd.DataFrame(data = df_data, columns = df.columns, index = df.index)\n",
    "df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = database_test['Latvia']\n",
    "df_data = imputer.transform(df)\n",
    "df_test = pd.DataFrame(data = df_data, columns = df.columns, index = df.index)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally this step is done for the whole dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_training_imputed = {}\n",
    "database_validation_imputed = {}\n",
    "database_test_imputed = {}\n",
    "\n",
    "imputer = KNNImputer(missing_values=np.nan, n_neighbors=5, weights=\"uniform\")\n",
    "\n",
    "for country in database_clear_na.keys():   \n",
    "  \n",
    "    try:\n",
    "        \n",
    "        df_current = database_training[country]\n",
    "        imputer.fit(df_current)\n",
    "        df_data = imputer.transform(df_current)\n",
    "        database_training_imputed[country] = pd.DataFrame(data = df_data,\n",
    "                                                          columns = df_current.columns,\n",
    "                                                          index = df_current.index)\n",
    "        \n",
    "        df_current = database_validation[country]\n",
    "        df_data = imputer.transform(df_current)\n",
    "        database_validation_imputed[country] = pd.DataFrame(data = df_data,\n",
    "                                                            columns = df_current.columns,\n",
    "                                                            index = df_current.index)\n",
    "        \n",
    "        df_current = database_test[country]\n",
    "        df_data = imputer.transform(df_current)\n",
    "        database_test_imputed[country] = pd.DataFrame(data = df_data,\n",
    "                                                      columns = df_current.columns,\n",
    "                                                      index = df_current.index)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(country + \" \" + str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different variables have different absolute vaules. Some machine learning models will have problems with this type of input. That is the reason I preprocess the data as follows:\n",
    "\n",
    "First I want to have only growth rates. For a time series $ X = (x_{1}, ..., x_{n})$ the growth rate for $x_{i}$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{x}_{i} = \\frac{x_{i}}{x_{i-1}}\n",
    "\\end{equation}\n",
    "    \n",
    "The first value of the original serires will not be mapped. Another issue occoures when using only this transformation. I compare a 10 % increase and a 10 % decrease of a certain vaule. Let's denothe the increase with $\\hat{x}_{I}$ and the decrease with $\\hat{x}_{D}$. Now the following applies:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{x}_{I} = 1.1 \\\\\n",
    "\\hat{x}_{D} = 0.9\n",
    "\\end{equation}\n",
    " \n",
    " so\n",
    " \n",
    " \\begin{equation}\n",
    "|\\hat{x}_{I}| \\neq |\\hat{x}_{D}|.\n",
    "\\end{equation}\n",
    "\n",
    "In order to obation the same absolute value for both directions, I will apply a logarithmic transformation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{x}_{i} = \\ln(\\frac{x_{i}}{x_{i-1}})\n",
    "\\end{equation}\n",
    "\n",
    "To avoid arguments that are not defined for the logarithm I shift the fraction by $|\\min_i(x_{i})|  + 0.001$:\n",
    " \n",
    "\\begin{equation}\n",
    "\\hat{x}_{i} = \\ln(\\frac{x_{i}}{x_{i-1}} + |\\min_i(x_{i})| + 0.001)\n",
    "\\end{equation}\n",
    "\n",
    "In same cases there is $x_{i} = x_{i-1} = 0$ which will cause an error because of divisin by zero. I add a some constant to avoid this problem.  \n",
    "\n",
    "As there are no statistics from the dataset involved, this transformation can be appplied to the whole dataset. Therefore I first glue the training, validation and test dataset together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets(database_training, database_validation, database_test):\n",
    "    database_complete = {}\n",
    "\n",
    "    for country in database_training.keys():\n",
    "        database_complete[country] = database_training[country].append(database_validation[country]).append(database_test[country])\n",
    "        \n",
    "    return database_complete\n",
    "\n",
    "database_imputed = combine_datasets(database_training_imputed, database_validation_imputed, database_test_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function I transform the whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_series_to_relative(df):\n",
    "    # Assings each t the Values of ln(X_t / X_(t-1))\n",
    "    # X_0 will be dropped\n",
    "    \n",
    "    df_new = df.iloc[1:, :].copy()\n",
    "    \n",
    "    for variable in df.columns:\n",
    "        try:\n",
    "            \n",
    "            if not (df[variable] != 0).all():\n",
    "                df[variable] = df[variable] + 0.001\n",
    "                \n",
    "            #Check if this tranformation is correct!\n",
    "           # df_new[variable] = df[variable].iloc[:-1].values / df[variable].iloc[1:].values\n",
    "            df_new[variable] = df[variable].iloc[1:].values / df[variable].iloc[:-1].values \n",
    "            \n",
    "            df_new[variable] = np.log(df_new[variable] + 0.001 + np.abs(np.min(df_new[variable])))\n",
    "        except Exception as e:\n",
    "            print(country + \" \" + str(e))\n",
    "        \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and apply it to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_transformed = {}\n",
    "\n",
    "for country in database_imputed.keys():\n",
    "    \n",
    "    database_transformed[country] = convert_time_series_to_relative(database_imputed[country])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Shift of the data\n",
    "\n",
    "reference: paper crystal ball; shifting index for supervised learning, tuples look like (x_t-1, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_supervised_learning(df):\n",
    "     \n",
    "    df_y = df.loc[:, 'y']\n",
    "    df_variables = df.drop(['y'], axis=1)\n",
    "    \n",
    "    df_variables.index = df_variables.index - 1 \n",
    "   \n",
    "    df = pd.DataFrame(df_y).join(df_variables, how='inner')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = database_transformed['Germany']\n",
    "df_supervised = preprocess_for_supervised_learning(df)\n",
    "df_supervised.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying it to the whole database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_supervised = {}\n",
    "\n",
    "for country in database_transformed.keys():\n",
    "    \n",
    "    database_supervised[country] = preprocess_for_supervised_learning(database_transformed[country])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I split the dataset again to obtain the training, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_training, database_validation, database_test = split_dataset(database_transformed)\n",
    "database_training_sv, database_validation_sv, database_test_sv = split_dataset(database_supervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization \n",
    "\n",
    "Finally, transform data so that every colun has 0 mean and sdtdev 1. This is especially important for neural networks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(df_training, df_validation, df_test):\n",
    "     \n",
    "    scaler = preprocessing.StandardScaler().fit(df_training)\n",
    "    \n",
    "    return scaler.transform(df_training), scaler.transform(df_validation), scaler.transform(df_test), scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying it to the whole dataset and keeping track of the scaler instance for each country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_training_sv_standard = {}\n",
    "database_validation_sv_standard = {}\n",
    "database_test_sv_standard = {}\n",
    "database_scaler = {}\n",
    "\n",
    "for country in database_training_sv.keys():\n",
    "    \n",
    "    tr, val, test, scaler = standardization(database_training_sv[country], database_validation_sv[country], database_test_sv[country])\n",
    "    \n",
    "    database_training_sv_standard[country] = pd.DataFrame(tr)\n",
    "    database_validation_sv_standard[country] = pd.DataFrame(val)\n",
    "    database_test_sv_standard[country]  = pd.DataFrame(test)\n",
    "    database_scaler[country] = scaler\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also serialize the whole database, so I can start coding without rerunning the whole data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir = os.path.join(r'C:/Users/hauer/Documents/Repositories/cfds_project', 'database.pickle')\n",
    "with open(database_dir, 'wb') as f:\n",
    "    save = {\n",
    "        'database_training': database_training,\n",
    "        'database_validation': database_validation,\n",
    "        'database_test': database_test,\n",
    "        'database_training_sv': database_training_sv,\n",
    "        'database_validation_sv': database_validation_sv,\n",
    "        'database_test_sv': database_test_sv,\n",
    "        \n",
    "        'database_training_sv_standard': database_training_sv_standard,\n",
    "        'database_validation_sv_standard': database_validation_sv_standard,\n",
    "        'database_test_sv_standard': database_test_sv_standard,\n",
    "        \n",
    "        'database_scaler': database_scaler,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(database_dir,'rb') as f: \n",
    "    db = pickle.load(f)\n",
    "    \n",
    "database_training = db['database_training']\n",
    "database_validation = db['database_validation']\n",
    "database_test = db['database_test']\n",
    "\n",
    "database_training_sv = db['database_training_sv']\n",
    "database_validation_sv = db['database_validation_sv']\n",
    "database_test_sv = db['database_test_sv']\n",
    "\n",
    "database_training_sv_standard = db['database_training_sv_standard']\n",
    "database_validation_sv_standard = db['database_validation_sv_standard']\n",
    "database_test_sv_standard = db['database_test_sv_standard']\n",
    "\n",
    "database_scaler = db['database_scaler']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function: MSE. To determine a to determine a good choise for the hyperparameter, a grid search is done for some of the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all models I will use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For models that are in the sklearn framework with the `fit` and `predict` methods, I use this function to perform the evaulation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, df_training, df_prediction):\n",
    "   \n",
    "    X_train = df_training.iloc[:,1:].values\n",
    "    y_train = df_training.iloc[:,0].values\n",
    "    \n",
    "    X_test = df_prediction.iloc[:,1:].values\n",
    "    # y_test = df_prediction.iloc[:,0].values\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_predicted = model.predict(X_test)\n",
    "  \n",
    "    result = pd.Series(data=y_predicted, index = df_prediction.index)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'France'\n",
    "\n",
    "df_training = database_training[country]\n",
    "df_validation = database_validation[country]\n",
    "df_test = database_test[country]\n",
    "\n",
    "df_training_sv = database_training_sv[country]\n",
    "df_validation_sv = database_validation_sv[country]\n",
    "df_test_sv = database_test_sv[country]\n",
    "\n",
    "\n",
    "t_forecast_test = df_test_sv.index.values\n",
    "\n",
    "start_forecast = df_validation_sv.index.values[0]\n",
    "t_forecast_validation = df_validation_sv.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check forecasting for supervised learning in the evaliation. values from 2011 forecast value for 2012 \n",
    "\n",
    "print(start_forecast)\n",
    "print(t_forecast_validation)\n",
    "print(t_forecast_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'WOE'\n",
    "y_forecast = get_predictions_weo(df_weo, country = country,\n",
    "                                 start_forecast =  t_forecast_validation[0],\n",
    "                                 end_forecast = t_forecast_validation[-1])\n",
    "mse = mean_squared_error(y_forecast, df_validation_sv['y'].values)\n",
    "models.append( (name, y_forecast, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS\n",
    "\n",
    "The ordinary least squares regression is the most famous and basic model in econometrics. The depentend variable is shown as:\n",
    "\n",
    "\\begin{equation}\n",
    "y = x_1\\beta_1 + x_2\\beta_2 + ... x_N\\beta_N + \\beta_{N+1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'OLS'\n",
    "model = LinearRegression()\n",
    "y_forecast = forecast(model, df_training_sv, df_validation_sv)\n",
    "y_forecast = y_forecast.values\n",
    "mse = mean_squared_error(y_forecast, df_validation_sv['y'].values)\n",
    "models.append( (name, y_forecast, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA\n",
    "\n",
    "The autoregressive integrated moving average ARIMA($p$, $d$, $q$) model is used in time series analysis.\n",
    "\n",
    "\\begin{equation}\n",
    "X_t - \\alpha_1X_{t-1} - ... - \\alpha_pX_{t-p} =  \\epsilon_t + \\theta_1\\epsilon_{t-1} + ... + \\theta_q\\epsilon_{t-q}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\alpha _{i}$ are the parameters of the autoregressive part of the model, $\\theta _{i}$ are the parameters of the moving average part, $d$ is the degree of differencing and $\\epsilon _{t}$ are error terms. There is an implementaion in python in the pmdarima package, which automatically discovers the optimal order for an ARIMA model with exogenous variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ARIMA'\n",
    "\n",
    "y_train = df_training_sv.iloc[:, 0]\n",
    "X_train = df_training_sv.iloc[:, 1:]\n",
    "y_validation = df_validation_sv.iloc[:, 0]\n",
    "X_validation = df_validation_sv.iloc[:, 1:]\n",
    "\n",
    "model = auto_arima(y = y_train,\n",
    "                   trace=True, \n",
    "                   start_p=0,\n",
    "                   max_p=3,\n",
    "                   start_q=0,\n",
    "                   max_q=3,\n",
    "                   seasonal = False,\n",
    "                   stepwise= True,\n",
    "                   exogenous=X_train) \n",
    "\n",
    "model.fit(y= y_train, exogenous=X_train)\n",
    "\n",
    "y_forecast = model.predict(n_periods=y_validation.shape[0],\n",
    "                      exogenous = X_validation)\n",
    "mse = mean_squared_error(y_forecast, y_validation)\n",
    "models.append( (name, y_forecast, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {'alpha': [0.1, 0.25, 1.0, 2.0, 4.0, 8.0, 16, 32, 128]}\n",
    "model = Lasso()\n",
    "\n",
    "clf = GridSearchCV(model, parameters)\n",
    "\n",
    "X_train = df_training_sv.iloc[:,1:].values\n",
    "y_train = df_training_sv.iloc[:,0].values\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'LASSO'\n",
    "model = Lasso(alpha = 0.25)\n",
    "y_forecast = forecast(model, df_training_sv, df_validation_sv)\n",
    "y_forecast = y_forecast.values\n",
    "mse = mean_squared_error(y_forecast, df_validation_sv['y'].values)\n",
    "models.append( (name, y_forecast, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression\n",
    "\n",
    "https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'C': [0.1, 0.25, 1.0, 2.0, 4.0, 8.0],\n",
    "             'epsilon': [0.01, 0.05, 0.1, 0.25, 0.5]}\n",
    "model = SVR()\n",
    "clf = GridSearchCV(model, parameters)\n",
    "\n",
    "X_train = df_training_sv.iloc[:,1:].values\n",
    "y_train = df_training_sv.iloc[:,0].values\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'SVR'\n",
    "model = SVR(C=0.1, epsilon=0.05, kernel = 'sigmoid')\n",
    "y_forecast = forecast(model, df_training_sv, df_validation_sv)\n",
    "y_forecast = y_forecast.values\n",
    "mse = mean_squared_error(y_forecast, df_validation_sv['y'].values)\n",
    "models.append( (name, y_forecast, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [2,3,4,5,10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "     'min_impurity_decrease': [0, 0.0001, 0.001, 0.01, 0.01] }\n",
    "model = DecisionTreeRegressor()\n",
    "clf = GridSearchCV(model, parameters)\n",
    "\n",
    "X_train = df_training_sv.iloc[:,1:].values\n",
    "y_train = df_training_sv.iloc[:,0].values\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Tree'\n",
    "model = DecisionTreeRegressor(max_depth = 2,\n",
    "                              min_impurity_decrease =  0.01, \n",
    "                              min_samples_leaf = 2,\n",
    "                              min_samples_split =  10)\n",
    "y_forecast = forecast(model, df_training_sv, df_validation_sv)\n",
    "y_forecast = y_forecast.values\n",
    "mse = mean_squared_error(y_forecast, df_validation_sv['y'].values)\n",
    "models.append( (name, y_forecast, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':[5, 10, 20, 50, 100], \n",
    "              'max_depth':[1, 2, 4],\n",
    "             'learning_rate': [0.01, 0.03, 0.1, 0.25],\n",
    "             'min_samples_split':[2, 10]}\n",
    "model = GradientBoostingRegressor()\n",
    "clf = GridSearchCV(model, parameters)\n",
    "\n",
    "X_train = df_training_sv.iloc[:,1:].values\n",
    "y_train = df_training_sv.iloc[:,0].values\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'GBM'\n",
    "model = GradientBoostingRegressor(n_estimators = 5, max_depth = 4, \n",
    "                                  min_samples_split=2, learning_rate = 0.03)\n",
    "y_forecast = forecast(model, df_training_sv, df_validation_sv)\n",
    "y_forecast = y_forecast.values\n",
    "mse = mean_squared_error(y_forecast, df_validation_sv['y'].values)\n",
    "models.append( (name, y_forecast, mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "see Notebook 02."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see Notebook 03 and 04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = database_training[country].append(database_validation[country]).append(database_test[country])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df['y'], label='real')\n",
    "\n",
    "for model in models:\n",
    "    name = model[0]\n",
    "    y_forecast = model[1]\n",
    "    mse = model[2]\n",
    "    \n",
    "    label = name + ' ' + str(round(mse,3))\n",
    "    \n",
    "    ax.plot(t_forecast_validation,  y_forecast, label=label, alpha=0.5)\n",
    "\n",
    "ax.axvline(x=start_forecast, ymin=0, ymax=1, color='black',linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('year') \n",
    "ax.set_ylabel('GDP growth change in %') \n",
    "ax.set_title(\"GDP growth - real vs. forecast\")\n",
    "legend  = ax.legend(bbox_to_anchor=(1.05, 1))\n",
    "fig.autofmt_xdate()\n",
    "plt.grid()\n",
    "\n",
    "wdir= r'C:/Users/hauer/Documents/Repositories/cfds_project'\n",
    "save_dir = os.path.join(wdir, 'forecast_out_of_time.png')\n",
    "\n",
    "#plt.savefig(save_dir, dpi = 500, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the models\n",
    "\n",
    "This is the final evaluation, so models will be trained on the training set combined with the test set and the MSE is calculated with the test set. \n",
    "\n",
    "## Training on every country on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecast_WEO = {}\n",
    "y_forecast_ARIMA = {}\n",
    "y_forecast_OLS = {}\n",
    "y_forecast_LASSO = {}\n",
    "y_forecast_SVR = {}\n",
    "y_forecast_TREE = {}\n",
    "y_forecast_GBM = {}\n",
    "\n",
    "df_result_single_countries = pd.DataFrame(columns=['country', 'WEO', 'ARIMA', 'OLS', 'LASSO', 'SVR', 'TREE', 'GBM'])\n",
    "\n",
    "for country in database_training_sv.keys():\n",
    "\n",
    "    df_training_sv = database_training_sv[country]\n",
    "    df_validation_sv = database_validation_sv[country]\n",
    "    \n",
    "    df_training_sv = pd.concat([df_training_sv, df_validation_sv])\n",
    "    \n",
    "    df_test_sv = database_test_sv[country]\n",
    "    \n",
    "    t_forecast_test = df_test_sv.index.values\n",
    "    start_forecast = df_test_sv.index.values[0]\n",
    "\n",
    "    \n",
    "    # WEO\n",
    "    \n",
    "    y_forecast = get_predictions_weo(df_weo, country = country,\n",
    "                                     start_forecast =  t_forecast_test[0],\n",
    "                                     end_forecast = t_forecast_test[-1])\n",
    "    mse_WEO = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_WEO[country] = y_forecast\n",
    "    \n",
    "    # ARIMA\n",
    "    #ValueError: Could not successfully fit ARIMA to input data. \n",
    "    #It is likely your data is non-stationary. Please induce stationarity or try a different range of model order params. \n",
    "    #If your data is seasonal, check the period (m) of the data.\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        y_train = df_training_sv.iloc[:, 0]\n",
    "        X_train = df_training_sv.iloc[:, 1:]\n",
    "        y_validation = df_test_sv.iloc[:, 0]\n",
    "        X_validation = df_test_sv.iloc[:, 1:]\n",
    "\n",
    "        model = auto_arima(y = y_train,\n",
    "                           trace=True, \n",
    "                           start_p=0,\n",
    "                           max_p=3,\n",
    "                           start_q=0,\n",
    "                           max_q=3,\n",
    "                           seasonal = False,\n",
    "                           stepwise= True,\n",
    "                           exogenous=X_train) \n",
    "\n",
    "        model.fit(y= y_train, exogenous=X_train)\n",
    "\n",
    "        y_forecast = model.predict(n_periods=y_validation.shape[0],\n",
    "                              exogenous = X_validation)\n",
    "        y_forecast_ARIMA[country] = y_forecast\n",
    "        mse_ARIMA = mean_squared_error(y_forecast, y_validation)\n",
    "        \n",
    "    except Exception as e:\n",
    "        mse_ARIMA = -1\n",
    "        y_forecast_ARIMA[country] = [0 for i in range(len(y_forecast))]\n",
    "        \n",
    "\n",
    "    \n",
    "    # OLS\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    y_forecast = forecast(model, df_training_sv, df_test_sv)\n",
    "    y_forecast = y_forecast.values\n",
    "    mse_OLS = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_OLS[country] = y_forecast\n",
    "    \n",
    "    # LASSO\n",
    "\n",
    "    model = Lasso(alpha = 0.25)\n",
    "    y_forecast = forecast(model, df_training_sv, df_test_sv)\n",
    "    y_forecast = y_forecast.values\n",
    "    mse_LASSO = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_LASSO[country] = y_forecast\n",
    "    \n",
    "    \n",
    "    # SVR\n",
    "    \n",
    "    model = SVR(C=0.1, epsilon=0.05, kernel = 'sigmoid')\n",
    "    y_forecast = forecast(model, df_training_sv, df_test_sv)\n",
    "    y_forecast = y_forecast.values\n",
    "    mse_SVR = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_SVR[country] = y_forecast\n",
    "    \n",
    "    \n",
    "    # Regression Tree\n",
    "    \n",
    "    model = DecisionTreeRegressor(max_depth = 2,\n",
    "                              min_impurity_decrease =  0.01, \n",
    "                              min_samples_leaf = 2,\n",
    "                              min_samples_split =  10)\n",
    "    y_forecast = forecast(model, df_training_sv, df_test_sv)\n",
    "    y_forecast = y_forecast.values\n",
    "    mse_TREE = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_TREE[country] = y_forecast\n",
    "\n",
    "\n",
    "    \n",
    "    # GBM\n",
    "    \n",
    "    model = GradientBoostingRegressor(n_estimators = 5, max_depth = 2, \n",
    "                                      min_samples_split=10, learning_rate = 0.01)\n",
    "    y_forecast = forecast(model, df_training_sv, df_test_sv)\n",
    "    y_forecast = y_forecast.values\n",
    "    mse_GBM = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_GBM[country] = y_forecast\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_result_single_countries = pd.concat([pd.DataFrame([[country ,mse_WEO, mse_ARIMA, mse_OLS, mse_LASSO, mse_SVR, mse_TREE, mse_GBM]],\n",
    "                                                         columns=df_result_single_countries.columns),\n",
    "                                            df_result_single_countries],ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in database_training_sv.keys():\n",
    "\n",
    "    df = database_training_sv[country].append(database_validation_sv[country]).append(database_test_sv[country])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df['y'], label='real')\n",
    "\n",
    "    row = df_result_single_countries[df_result_single_countries['country'] == country]\n",
    "    for model in ['WEO', 'ARIMA', 'OLS', 'GBM']:\n",
    "        name = model\n",
    "\n",
    "        if model == 'WEO':\n",
    "            y_forecast = y_forecast_WEO[country]\n",
    "        elif model == 'ARIMA':\n",
    "            y_forecast = y_forecast_ARIMA[country]\n",
    "        elif model == 'SVR':\n",
    "            y_forecast = y_forecast_SVR[country]\n",
    "        elif model == 'OLS':\n",
    "            y_forecast = y_forecast_OLS[country]\n",
    "        elif model == 'GBM':\n",
    "            y_forecast = y_forecast_GBM[country]\n",
    "\n",
    "        mse = row[model].values[0]\n",
    "\n",
    "        label = name + ' ' + str(round(mse,3))\n",
    "\n",
    "        ax.plot(t_forecast_test,  y_forecast, label=label, alpha=0.5)\n",
    "\n",
    "    ax.axvline(x=start_forecast, ymin=0, ymax=1, color='black',linestyle='--', alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel('year') \n",
    "    ax.set_ylabel('GDP growth change in %') \n",
    "    ax.set_title(\"GDP growth - real vs. forecast - \" + country)\n",
    "    legend  = ax.legend(bbox_to_anchor=(1.05, 1))\n",
    "    fig.autofmt_xdate()\n",
    "    plt.grid()\n",
    "\n",
    "    wdir= r'C:/Users/hauer/Documents/Repositories/cfds_project/plots_single_countries'\n",
    "    save_dir = os.path.join(wdir, country+'.png')\n",
    "\n",
    "    plt.savefig(save_dir, dpi = 500, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_single_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_models = ['WEO', 'OLS', 'ARIMA']\n",
    "machine_learning_models = [x for x in df_result_single_countries.keys() if x not in ['country', 'WEO', 'OLS', 'ARIMA']]\n",
    "\n",
    "result_single = []\n",
    "\n",
    "for i in range(0,len(df_result_single_countries)):\n",
    "    \n",
    "    df_current = df_result_single_countries.iloc[i,:]\n",
    "    \n",
    "    for classic_model in classic_models:\n",
    "        for machine_learning_model in machine_learning_models:\n",
    "            if df_current[classic_model] <  df_current[machine_learning_model]:\n",
    "                x = 0\n",
    "            else:\n",
    "                x = 1\n",
    "            result_single.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_sv_complete = pd.DataFrame()\n",
    "for country in database_training_sv.keys():\n",
    "    df_training_sv = database_training_sv[country]\n",
    "    df_validation_sv = database_validation_sv[country]\n",
    "    \n",
    "    df_training_sv = pd.concat([df_training_sv, df_validation_sv])\n",
    "\n",
    "    df_training_sv_complete = pd.concat([df_training_sv_complete, df_training_sv]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_1, state_1, hidden_2, state_2):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden_1, state_1))      \n",
    "        r_out, (hidden_out, state_out) = self.lstm2(r_out, (hidden_2, state_2))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecast_WEO = {}\n",
    "y_forecast_OLS = {}\n",
    "y_forecast_LASSO = {}\n",
    "y_forecast_SVR = {}\n",
    "y_forecast_TREE = {}\n",
    "y_forecast_GBM = {}\n",
    "y_forecast_RNN = {}\n",
    "\n",
    "df_result_all_countries = pd.DataFrame(columns=['country', 'WEO', 'OLS', 'LASSO', 'SVR', 'TREE', 'GBM', 'RNN'])\n",
    "N, dummy_dim = database_training_sv_standard['Germany'].shape\n",
    "dummy_dim -= 1\n",
    "\n",
    "\n",
    "for country in database_training_sv.keys():\n",
    "#for country in ['Germany', 'France']:\n",
    "\n",
    "    \n",
    "    df_test_sv = database_test_sv[country]\n",
    "    \n",
    "    t_forecast_test = df_test_sv.index.values\n",
    "    start_forecast = df_test_sv.index.values[0]\n",
    "\n",
    "    \n",
    "    # WEO\n",
    "    \n",
    "    y_forecast = get_predictions_weo(df_weo, country = country,\n",
    "                                     start_forecast =  t_forecast_test[0],\n",
    "                                     end_forecast = t_forecast_test[-1])\n",
    "    mse_WEO = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_WEO[country] = y_forecast\n",
    "    \n",
    "     \n",
    "\n",
    "    \n",
    "    # OLS\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    y_forecast = forecast(model, df_training_sv_complete, df_test_sv)\n",
    "    y_forecast = y_forecast.values\n",
    "    mse_OLS = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_OLS[country] = y_forecast\n",
    "    \n",
    "    \n",
    "     # LASSO\n",
    "\n",
    "    model = Lasso(alpha = 0.25)\n",
    "    y_forecast = forecast(model, df_training_sv_complete, df_test_sv)\n",
    "    y_forecast = y_forecast.values\n",
    "    mse_LASSO = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_LASSO[country] = y_forecast\n",
    "    \n",
    "    \n",
    "    # SVR\n",
    "    \n",
    "    model = SVR(C=0.1, epsilon=0.05, kernel = 'sigmoid')\n",
    "    y_forecast = forecast(model, df_training_sv_complete, df_test_sv)\n",
    "    y_forecast = y_forecast.values\n",
    "    mse_SVR = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_SVR[country] = y_forecast\n",
    "    \n",
    "    \n",
    "    # Regression Tree\n",
    "    \n",
    "    model = DecisionTreeRegressor(max_depth = 2,\n",
    "                              min_impurity_decrease =  0.01, \n",
    "                              min_samples_leaf = 2,\n",
    "                              min_samples_split =  10)\n",
    "    y_forecast = forecast(model, df_training_sv_complete, df_test_sv)\n",
    "    y_forecast = y_forecast.values\n",
    "    mse_TREE = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_TREE[country] = y_forecast\n",
    "\n",
    "    \n",
    "    # GBM\n",
    "    \n",
    "    name = 'GBM'\n",
    "    model = GradientBoostingRegressor(n_estimators = 5, max_depth = 2, \n",
    "                                      min_samples_split=10, learning_rate = 0.01)\n",
    "    y_forecast = forecast(model, df_training_sv_complete, df_test_sv)\n",
    "    y_forecast = y_forecast.values\n",
    "    mse_GBM = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_GBM[country] = y_forecast\n",
    "    \n",
    "    \n",
    "    # RNN\n",
    "    \n",
    "    wdir= r'C:/Users/hauer/Documents/Repositories/cfds_project'\n",
    "    save_dir = os.path.join(wdir, 'pytorch_models')\n",
    "    model_name = 'rnn.torch'\n",
    "    \n",
    "    hidden_dim = 64\n",
    "    model = LSTMNet(input_size=6, seq_len=17, output_size=1, hidden_dim=hidden_dim, n_layers=1)\n",
    "    model.load_state_dict(load( os.path.join(save_dir, model_name)))\n",
    "    \n",
    "    \n",
    "    df = database_training_sv_standard[country].append(database_validation_sv_standard[country]).append(database_test_sv_standard[country])\n",
    "\n",
    "    n_forecast_validation, _ = database_test_sv_standard[country].shape\n",
    "\n",
    "    X_eval = df.iloc[:,1:].values\n",
    "    y_eval = df.iloc[:,0].values\n",
    "    X_eval_T = from_numpy(X_eval).float()\n",
    "    N, _ = X_eval_T.shape\n",
    "    X_eval_T = X_eval_T.view([-1, N, dummy_dim])\n",
    "\n",
    "    hidden_1 = zeros(1, N, hidden_dim)\n",
    "    state_1 = zeros(1, N, hidden_dim)\n",
    "\n",
    "    hidden_2 = zeros(1, N, hidden_dim)\n",
    "    state_2 = zeros(1, N, hidden_dim)\n",
    "\n",
    "    model.eval()\n",
    "    with no_grad():\n",
    "        y_hat = model(X_eval_T, hidden_1, state_1, hidden_2, state_2)\n",
    "\n",
    "    y_hat =  y_hat.view(-1).numpy()\n",
    "    y_forecast = y_hat[-n_forecast_validation:]\n",
    "    \n",
    "    # Inverse tranformation \n",
    "    \n",
    "    scaler = database_scaler[country]\n",
    "    df_output = database_test_sv_standard[country]\n",
    "    df_output.iloc[:,0] = y_forecast\n",
    "    df_output = pd.DataFrame(scaler.inverse_transform(df_output))\n",
    "    y_forecast = df_output.iloc[:,0].values\n",
    "    \n",
    "    \n",
    "    mse_RNN = mean_squared_error(y_forecast, df_test_sv['y'].values)\n",
    "    y_forecast_RNN[country] = y_forecast\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_result_all_countries = pd.concat([pd.DataFrame([[country ,mse_WEO, mse_OLS, mse_LASSO, mse_SVR, mse_TREE, mse_GBM, mse_RNN]],\n",
    "                                                        columns=df_result_all_countries.columns),\n",
    "                                         df_result_all_countries],ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in database_training_sv.keys():\n",
    "\n",
    "    df = database_training_sv[country].append(database_validation_sv[country]).append(database_test_sv[country])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df['y'], label='real')\n",
    "\n",
    "    row = df_result_all_countries[df_result_all_countries['country'] == country]\n",
    "    for model in ['WEO', 'OLS', 'GBM', 'RNN']:\n",
    "        name = model\n",
    "\n",
    "        if model == 'WEO':\n",
    "            y_forecast = y_forecast_WEO[country]\n",
    "        elif model == 'RNN':\n",
    "            y_forecast = y_forecast_RNN[country]\n",
    "        elif model == 'OLS':\n",
    "            y_forecast = y_forecast_OLS[country]\n",
    "        elif model == 'SVR':\n",
    "            y_forecast = y_forecast_SVR[country]\n",
    "        elif model == 'GBM':\n",
    "            y_forecast = y_forecast_GBM[country]\n",
    "\n",
    "        mse = row[model].values[0]\n",
    "\n",
    "        label = name + ' ' + str(round(mse,3))\n",
    "\n",
    "        ax.plot(t_forecast_test,  y_forecast, label=label, alpha=0.5)\n",
    "\n",
    "    ax.axvline(x=start_forecast, ymin=0, ymax=1, color='black',linestyle='--', alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel('year') \n",
    "    ax.set_ylabel('GDP growth change in %') \n",
    "    ax.set_title(\"GDP growth - real vs. forecast - \" + country)\n",
    "    legend  = ax.legend(bbox_to_anchor=(1.05, 1))\n",
    "    fig.autofmt_xdate()\n",
    "    plt.grid()\n",
    "\n",
    "    wdir= r'C:/Users/hauer/Documents/Repositories/cfds_project/plots_all_countries'\n",
    "    save_dir = os.path.join(wdir, country+'.png')\n",
    "\n",
    "    plt.savefig(save_dir, dpi = 500, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_all_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_models = ['WEO', 'OLS']\n",
    "machine_learning_models = [x for x in df_result_all_countries.keys() if x not in ['country', 'WEO', 'OLS', 'ARIMA']]\n",
    "\n",
    "result_all = []\n",
    "\n",
    "for i in range(0,len(df_result_all_countries)):\n",
    "    \n",
    "    df_current = df_result_all_countries.iloc[i,:]\n",
    "    \n",
    "    for classic_model in classic_models:\n",
    "        for machine_learning_model in machine_learning_models:\n",
    "            if df_current[classic_model] <  df_current[machine_learning_model]:\n",
    "                x = 0\n",
    "            else:\n",
    "                x = 1\n",
    "            result_all.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Analyis for statistical significance\n",
    "\n",
    "Drawing bootstrap samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1000\n",
    "\n",
    "bootstrap = np.random.choice(result_single, (len(result),B), replace = True)\n",
    "bootstrap_means_single = np.mean(bootstrap, axis = 1)\n",
    "approch_single = ['single' for i in range(len(bootstrap_means_single))]\n",
    "\n",
    "bootstrap = np.random.choice(result_all, (len(result),B), replace = True)\n",
    "bootstrap_means_all = np.mean(bootstrap, axis = 1)\n",
    "approch_all = ['all' for i in range(len(bootstrap_means_all))]\n",
    "\n",
    "\n",
    "bootstrap_means = np.concatenate( (bootstrap_means_all, bootstrap_means_single))\n",
    "approach = approch_all + approch_single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame({'means': bootstrap_means, 'approach':approach})\n",
    "\n",
    "ax = sns.boxplot(x='approach', y='means', data=df_result,\n",
    "                width = 0.5)\n",
    "\n",
    "ax.axhline(y=0.5, color='red',linestyle='--', alpha=0.75)\n",
    "\n",
    "plt.title(\"final test for statistical significance\")\n",
    "\n",
    "\n",
    "wdir= r'C:\\Users\\hauer\\Documents\\Repositories\\cfds_project\\plot_result'\n",
    "save_dir = os.path.join(wdir, 'result.png')\n",
    "\n",
    "plt.savefig(save_dir, dpi = 500, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Deep Reinforcement Learning framework\n",
    "\n",
    "Runnig the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, \n",
    "            n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "\n",
    "        self.optimizer = Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "            max_mem_size=100000, eps_end=0.05, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.iter_cntr = 0\n",
    "        self.replace_target = 100\n",
    "\n",
    "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
    "                                    fc1_dims=64, fc2_dims=32)\n",
    "\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = terminal\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def get_q_valid(self, q, valid_actions):\n",
    "        q_valid = [np.nan] * len(q)\n",
    "        for action in valid_actions:\n",
    "            q_valid[action] = q[action]\n",
    "        \n",
    "        return q_valid\n",
    "\n",
    "    def choose_action(self, observation, valid_actions):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation], dtype=T.float32).to(self.Q_eval.device)\n",
    "            q = self.Q_eval.forward(state)\n",
    "            q = q.detach().numpy().squeeze()\n",
    "            q = self.get_q_valid(q, valid_actions)\n",
    "            action = np.nanargmax(q)\n",
    "        else:\n",
    "            action = np.random.choice(valid_actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch + self.gamma*T.max(q_next,dim=1)[0]\n",
    "\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "\n",
    "        self.iter_cntr += 1\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
    "                       else self.eps_min\n",
    "\n",
    "def get_prediction(action, empty_status):\n",
    "    # Determines prediction on a given empty_status and action\n",
    "    # if empty, i. e. no stock is in depot, if action == 1 (buying) you bet on rising price\n",
    "    # if not empty, i. e. stock is in depot, if action == 0 (selling) you bet on falling price\n",
    "    \n",
    "    if empty_status:\n",
    "        if action == 1:\n",
    "            return 1\n",
    "        return -1\n",
    "    else:\n",
    "        if action == 0:\n",
    "            return -1\n",
    "        return 1\n",
    "\n",
    "class Environment():\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_epochs, window_size_observation, size_time_series, database):\n",
    "        self.name = 'gao'\n",
    "        self.n_epochs = n_epochs\n",
    "        self.window_size_observation = window_size_observation \n",
    "        self.size_time_series = size_time_series\n",
    "        self.current_game = 0\n",
    "        self.t_current = 0\n",
    "        self.empty = True\n",
    "        self.open_cost = 0\n",
    "        self.database = database\n",
    "        \n",
    "        self.list_of_games = self.get_list_of_games()\n",
    "        self.n_games = len(self.list_of_games)\n",
    "        self.t_max = self.size_time_series - self.window_size_observation - 1\n",
    "\n",
    "      \n",
    "    def get_status_emtpy(self):\n",
    "        return self.empty\n",
    "\n",
    "    \n",
    "    def get_current_df(self):\n",
    "        return self.list_of_games[self.current_game]\n",
    "\n",
    "        \n",
    "    def get_list_of_games(self):\n",
    "        list_of_games = []\n",
    "        \n",
    "        for i in range(self.n_epochs):\n",
    "            for country in self.database.keys():\n",
    "                list_of_games.append(self.database[country])\n",
    "                   \n",
    "        return list_of_games\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        if self.current_game == self.n_games - 1:\n",
    "            self.current_game = 0\n",
    "        else:\n",
    "            self.current_game += 1\n",
    "            \n",
    "        self.t_current = 0\n",
    "        \n",
    "        observation = self.get_observation()\n",
    "        \n",
    "        return observation\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        done = False\n",
    "        if action == 0:\t\t# wait/close\n",
    "            reward = 0.\n",
    "            self.empty = True\n",
    "        elif action == 1:\t# open\n",
    "            reward = self.get_reward_noncash()\n",
    "            self.empty = False\n",
    "        elif action == 2:\t# keep\n",
    "            reward = self.get_reward_noncash()\n",
    "        else:\n",
    "            raise ValueError('no valid action: ' + str(action))\n",
    "        \n",
    "        self.t_current += 1\n",
    "        #return self.get_state(), reward, self.t == self.t_max, self.get_valid_actions()\n",
    "        \n",
    "        \n",
    "        done = self.t_current == self.t_max\n",
    "        observation = self.get_observation()\n",
    "        info = self.get_valid_actions()\n",
    "        \n",
    "        return observation, reward, done, info\n",
    " \n",
    "    \n",
    "    def get_reward_noncash(self):\n",
    "        df_current = self.list_of_games[self.current_game]\n",
    "               \n",
    "        t_1 = self.t_current + self.window_size_observation + 1\n",
    "        t = self.t_current + self.window_size_observation \n",
    "\n",
    "        price_t_1 = df_current.iloc[t_1, 0]\n",
    "        price_t = df_current.iloc[t, 0]\n",
    "        \n",
    "        reward = price_t_1 - price_t\n",
    "        \n",
    "        if self.empty:\n",
    "            reward -= self.open_cost\n",
    "        \n",
    "        return reward \n",
    "       \n",
    "    \n",
    "    \n",
    "    def get_observation(self):\n",
    "        df_current = self.list_of_games[self.current_game]\n",
    "        \n",
    "        observation = df_current.iloc[self.t_current:(self.t_current + self.window_size_observation), :]\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        if self.empty:\n",
    "            return [0, 1]\t# wait, open\n",
    "        else:\n",
    "            return [0, 2]\t# close, keep \n",
    "        \n",
    "    def set_list_of_games(self, df):\n",
    "        self.list_of_games = [df]\n",
    "        self.n_games = 1\n",
    "  \n",
    "        \n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare final training database, i. e. the database that consits of the original training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_training_RL = {}\n",
    "\n",
    "for country in database_training_sv_standard.keys():\n",
    "    df_to_add = database_training_sv_standard[country].append(database_validation_sv_standard[country])\n",
    "    df_to_add = df_to_add.reset_index()\n",
    "    del df_to_add['index']\n",
    "\n",
    "    database_training_RL[country] = df_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, p = database_training_sv_standard['Germany'].shape\n",
    "\n",
    "\n",
    "n_epochs = 1\n",
    "window_size_observation = 15\n",
    "size_time_series = N\n",
    "\n",
    "\n",
    "agent = Agent(gamma=0.8, epsilon=1, batch_size=64, n_actions=3, eps_end=0.01, \n",
    "              input_dims=[window_size_observation * p], lr=0.001, eps_dec=2e-5)\n",
    "\n",
    "env = Environment(n_epochs, window_size_observation, size_time_series, database_training_RL)\n",
    "n_games = env.n_games\n",
    "\n",
    "n_countries = len(database_training_sv_standard.keys())\n",
    "\n",
    "scores, eps_history = [], []\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "\n",
    "    observation = env.reset()\n",
    "    valid_actions = [0, 1]\n",
    "\n",
    "    # take only signal as observation for now: \n",
    "    #observation = observation.iloc[:, 1:].values.squeeze()\n",
    "    observation= np.squeeze(observation.values.transpose().reshape((1, -1)))\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation, valid_actions)\n",
    "        observation_, reward, done, valid_actions = env.step(action)\n",
    "\n",
    "\n",
    "        # take only signal as observation for now: \n",
    "        #observation_ = observation_.iloc[:, 1:].values.squeeze()\n",
    "        observation_= np.squeeze(observation_.values.transpose().reshape((1, -1)))\n",
    "\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, \n",
    "                                observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    \n",
    "    if i % n_countries == 0:\n",
    "        print('episode ', i / n_countries, 'score %.2f' % score,\n",
    "                'average score %.2f' % avg_score,\n",
    "                'epsilon %.2f' % agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecast_GAO = {}\n",
    "\n",
    "\n",
    "for country in database_training_sv_standard.keys():\n",
    "\n",
    "    df_to_predict = database_training_sv_standard[country].append(database_validation_sv_standard[country]).append(database_test_sv_standard[country])\n",
    "    df_to_predict = df_to_predict.reset_index()\n",
    "    del df_to_predict['index']\n",
    "    n_forecast_validation = 6\n",
    "\n",
    "    N, p = df_to_predict.shape\n",
    "    size_time_series = N\n",
    "\n",
    "    env = Environment(n_epochs, window_size_observation, size_time_series, database_training_sv_standard)\n",
    "\n",
    "    env.set_list_of_games(df_to_predict)\n",
    "\n",
    "\n",
    "\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    actions, empty_status, rewards = [], [], []\n",
    "\n",
    "    for i in range(1):\n",
    "        score = 0\n",
    "        done = False\n",
    "\n",
    "        observation = env.reset()\n",
    "        valid_actions = [0, 1]\n",
    "\n",
    "        # take only signal as observation for now: \n",
    "        observation= np.squeeze(observation.values.transpose().reshape((1, -1)))\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation, valid_actions)\n",
    "            observation_, reward, done, valid_actions = env.step(action)\n",
    "\n",
    "\n",
    "            # take only signal as observation for now: \n",
    "            observation_= np.squeeze(observation_.values.transpose().reshape((1, -1)))\n",
    "\n",
    "            score += reward\n",
    "            observation = observation_\n",
    "\n",
    "\n",
    "            empty_status.append(env.get_status_emtpy())\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "\n",
    "\n",
    "        df = env.get_current_df()\n",
    "        predictions = [get_prediction(action, empty_status) for action, empty in zip(actions, empty_status)]\n",
    "\n",
    "        predictions = [0 for i in range(window_size_observation)] + predictions\n",
    "        rewards = [0 for i in range(window_size_observation)] + rewards\n",
    "        actions = [0 for i in range(window_size_observation)] + actions\n",
    "\n",
    "\n",
    "    y_forecast = predictions[-n_forecast_validation:]\n",
    "    y_forecast_GAO[country] = y_forecast\n",
    "\n",
    "\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "\n",
    "    ax.plot(df.iloc[:,0], label='price')\n",
    "    ax.axvline(x= len(df) - n_forecast_validation - 1, ymin=-100, ymax=500, color='black',linestyle='--', alpha=1)\n",
    "    ax.grid()\n",
    "    ax.set_ylabel('price') \n",
    "    ax.set_title(\"Price and predictions\")\n",
    "\n",
    "\n",
    "\n",
    "    ax2.plot(predictions, 'ro-')\n",
    "    ax2.set_xlabel('t') \n",
    "    ax2.set_ylabel('predictions') \n",
    "\n",
    "\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "    wdir= r'C:/Users/hauer/Documents/Repositories/cfds_project/plots_drl_gao'\n",
    "    save_dir = os.path.join(wdir, country+'.png')\n",
    "\n",
    "    plt.savefig(save_dir, dpi = 500, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the prediction from the models, that where trained with the complete data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecast_WEO_RL = {}\n",
    "y_forecast_OLS_RL = {}\n",
    "y_forecast_GBM_RL = {}\n",
    "y_forecast_RNN_RL = {}\n",
    "\n",
    "\n",
    "for country in y_forecast_WEO.keys():\n",
    "    y_forecast_WEO_RL[country] = np.where(y_forecast_WEO[country].values > 0, 1, -1)\n",
    "    y_forecast_OLS_RL[country]  = np.where(y_forecast_OLS[country] > 0, 1, -1)\n",
    "    y_forecast_GBM_RL[country]  = np.where(y_forecast_GBM[country] > 0, 1, -1)\n",
    "    y_forecast_RNN_RL[country]  = np.where(y_forecast_RNN[country] > 0, 1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_RL = pd.DataFrame(columns=['country', 'WEO', 'OLS', 'GBM', 'RNN', 'GAO'])\n",
    "\n",
    "for country in y_forecast_OLS_RL.keys():\n",
    "    \n",
    "    df_test_sv = database_test_sv[country]\n",
    "    y_real = df_test_sv['y'].values\n",
    "    y_real = np.where(y_real > 0, 1, -1)\n",
    "   \n",
    "   \n",
    "    mse_WEO = sum(y_real == y_forecast_WEO_RL[country]) / 6\n",
    "    mse_OLS = sum(y_real == y_forecast_OLS_RL[country]) / 6\n",
    "    mse_GBM = sum(y_real == y_forecast_GBM_RL[country]) / 6\n",
    "    mse_RNN = sum(y_real == y_forecast_RNN_RL[country]) / 6\n",
    "    mse_GAO = sum(y_real == y_forecast_GAO[country]) / 6\n",
    "\n",
    "\n",
    "    df_result_RL = pd.concat([pd.DataFrame([[country ,mse_WEO,mse_OLS,mse_GBM, mse_RNN, mse_GAO]],\n",
    "                                                          columns=df_result_RL.columns),\n",
    "                                             df_result_RL],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_RL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
