{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for printing the definition of custom functions\n",
    "import inspect\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "# pytorch\n",
    "from torch import nn, no_grad, save, load\n",
    "from torch import from_numpy, zeros\n",
    "from torch.optim import SGD\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark')\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir = os.path.join(r'C:/Users/hauer/Documents/Repositories/cfds_project', 'database.pickle')\n",
    "\n",
    "with open(database_dir,'rb') as f: \n",
    "    db = pickle.load(f)\n",
    "    \n",
    "database_training = db['database_training']\n",
    "database_validation = db['database_validation']\n",
    "database_test = db['database_test']\n",
    "\n",
    "database_training_sv = db['database_training_sv']\n",
    "database_validation_sv = db['database_validation_sv']\n",
    "database_test_sv = db['database_test_sv']\n",
    "\n",
    "database_training_sv_standard = db['database_training_sv_standard']\n",
    "database_validation_sv_standard = db['database_validation_sv_standard']\n",
    "database_test_sv_standard = db['database_test_sv_standard']\n",
    "\n",
    "database_scaler = db['database_scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RNN start\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# # Prepare Data for RNN\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "N, dummy_dim = database_training_sv_standard['Germany'].shape\n",
    "dummy_dim -= 1\n",
    "\n",
    "time_steps = 16\n",
    "horizon = 1\n",
    "sequence_length = time_steps + horizon \n",
    "\n",
    "\n",
    "max_index = N - sequence_length + 1\n",
    "\n",
    "number_of_countries = len(database_training_sv_standard.keys())\n",
    "\n",
    "X = np.empty([0, sequence_length,dummy_dim])\n",
    "y = np.empty([0, sequence_length])\n",
    "\n",
    " \n",
    "\n",
    "for country in database_training.keys():\n",
    "    df_training_current = database_training_sv_standard[country]\n",
    "\n",
    "    X_current = np.empty([max_index, sequence_length,dummy_dim])\n",
    "    y_current = np.empty([max_index, sequence_length])\n",
    "\n",
    "    for i in range(max_index):\n",
    "\n",
    "        X_current[i] = df_training_current.iloc[i:i+sequence_length,1:].values\n",
    "        y_current[i] = df_training_current.iloc[i:i+sequence_length,0].values\n",
    "        \n",
    "    X = np.concatenate((X, X_current))\n",
    "    y = np.concatenate((y, y_current))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "N, seq_len, dummy_dim = X.shape\n",
    "\n",
    "input_size=dummy_dim\n",
    "n_layers=1\n",
    "output_size=1\n",
    "test_size = 0.20\n",
    "batch_size = 25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=123)\n",
    "\n",
    "\n",
    "X_train_T = from_numpy(X_train).float()\n",
    "y_train_T = from_numpy(y_train).float()\n",
    "X_val_T = from_numpy(X_val).float()\n",
    "y_val_T = from_numpy(y_val).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_T, y_train_T)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size)  \n",
    "\n",
    "valid_ds = TensorDataset(X_val_T, y_val_T)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 14.36 valid loss: 1.831\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN'\n",
    "hidden_dim=3\n",
    "lr = 0.03\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 17.42 valid loss: 2.618\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN_Adam'\n",
    "hidden_dim=3\n",
    "lr = 1e-06\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    " \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 11.98 valid loss: 1.771\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN_Large_Adam'\n",
    "hidden_dim=64\n",
    "lr = 1e-06\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    " \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 12.24 valid loss: 1.765\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, state):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden, state))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM'\n",
    "hidden_dim=10\n",
    "lr = 0.03\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "state_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0, state_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0, state_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Adam Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 11.85 valid loss: 1.747\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, state):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden, state))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM_Large_Adam'\n",
    "hidden_dim=64\n",
    "lr = 1e-06\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "state_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0, state_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0, state_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 11.840154 valid loss: 1.7219682\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_1, state_1, hidden_2, state_2):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden_1, state_1))      \n",
    "        r_out, (hidden_out, state_out) = self.lstm2(r_out, (hidden_2, state_2))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM_Stacked'\n",
    "hidden_dim=32\n",
    "lr = 0.1\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_01 = zeros(1, seq_len, hidden_dim)\n",
    "state_01 = zeros(1, seq_len, hidden_dim)\n",
    "\n",
    "hidden_02 = zeros(1, seq_len, hidden_dim)\n",
    "state_02 = zeros(1, seq_len, hidden_dim)\n",
    "\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_01, state_01, hidden_02, state_02)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_01, state_01, hidden_02, state_02)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {}: train loss: {:.8} valid loss: {:.8}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(n_epochs)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(12,10))\n",
    "\n",
    "for model_ in models:\n",
    "    name = model_[0]\n",
    "    training_loss = model_[1]\n",
    "    valid_loss =model_[2] \n",
    "\n",
    "    \n",
    "    label = name \n",
    "    \n",
    "    ax[0].plot(x,  training_loss, label=label)\n",
    "    ax[1].plot(x,  valid_loss, label=label)\n",
    "    #ax[2].plot([i for i in range(n_epochs-1)],  valid_loss[1:] / valid_loss[:-1] * 100, label=label)\n",
    "\n",
    "\n",
    "#ax[2].set_xlabel('epoch') \n",
    "\n",
    "ax[0].set_ylabel('loss') \n",
    "ax[1].set_ylabel('loss')\n",
    "\n",
    "ax[0].set_title(\"training loss\")\n",
    "ax[1].set_title(\"validation loss\")\n",
    "#ax[2].set_title(\"validation loss change in %\")\n",
    "\n",
    "legend  = ax[0].legend(bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "#ax[2].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# # Serializing model \n",
    "# =============================================================================\n",
    "\n",
    "wdir= r'C:/Users/hauer/Documents/Repositories/cfds_project'\n",
    "save_dir = os.path.join(wdir, 'pytorch_models')\n",
    "model_name = 'rnn.torch'\n",
    "\n",
    "if(not os.path.isdir(save_dir)):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "save(model.state_dict(), os.path.join(save_dir, model_name))\n",
    "\n",
    "#model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "#model.load_state_dict(load( os.path.join(save_dir, model_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01369219, 0.00042881, 0.00120457, 0.06186881, 0.01417604,\n",
       "       0.01045175], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = 'Germany'\n",
    "\n",
    "df = database_training_sv_standard[country].append(database_validation_sv_standard[country])\n",
    "\n",
    "n_forecast_validation, _ = database_validation_sv_standard[country].shape\n",
    "\n",
    "X_eval = df.iloc[:,1:].values\n",
    "y_eval = df.iloc[:,0].values\n",
    "X_eval_T = from_numpy(X_eval).float()\n",
    "N, _ = X_eval_T.shape\n",
    "X_eval_T = X_eval_T.view([-1, N, dummy_dim])\n",
    "\n",
    "hidden_1 = zeros(1, N, hidden_dim)\n",
    "state_1 = zeros(1, N, hidden_dim)\n",
    "\n",
    "hidden_2 = zeros(1, N, hidden_dim)\n",
    "state_2 = zeros(1, N, hidden_dim)\n",
    "\n",
    "model.eval()\n",
    "with no_grad():\n",
    "    y_hat = model(X_eval_T, hidden_1, state_1, hidden_2, state_2)\n",
    "    \n",
    "y_hat =  y_hat.view(-1).numpy()\n",
    "y_forecast = y_hat[-n_forecast_validation:]\n",
    "y_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing with the ground truth.\n",
    "\n",
    "First check right application of scaling. The unscaled data must equal the scaled data after appling the inverse_transform method from sklearn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>GHG</th>\n",
       "      <th>Current account balance</th>\n",
       "      <th>General government net lending/borrowing</th>\n",
       "      <th>PPP</th>\n",
       "      <th>Inflation, average consumer prices</th>\n",
       "      <th>ExchangeR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>2.051359</td>\n",
       "      <td>0.661859</td>\n",
       "      <td>1.852348</td>\n",
       "      <td>-0.058174</td>\n",
       "      <td>0.654957</td>\n",
       "      <td>1.051777</td>\n",
       "      <td>0.547797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>2.518101</td>\n",
       "      <td>0.640522</td>\n",
       "      <td>1.845025</td>\n",
       "      <td>-1.246360</td>\n",
       "      <td>0.662903</td>\n",
       "      <td>1.167132</td>\n",
       "      <td>0.503668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>2.072474</td>\n",
       "      <td>0.661971</td>\n",
       "      <td>1.786614</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>0.658295</td>\n",
       "      <td>1.140200</td>\n",
       "      <td>0.513078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>2.013436</td>\n",
       "      <td>0.617975</td>\n",
       "      <td>1.818870</td>\n",
       "      <td>3.318150</td>\n",
       "      <td>0.662904</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.583932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.690894</td>\n",
       "      <td>1.812187</td>\n",
       "      <td>0.607133</td>\n",
       "      <td>0.665526</td>\n",
       "      <td>1.869612</td>\n",
       "      <td>0.582607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1.860920</td>\n",
       "      <td>0.634447</td>\n",
       "      <td>1.827969</td>\n",
       "      <td>-0.435978</td>\n",
       "      <td>0.659129</td>\n",
       "      <td>1.428403</td>\n",
       "      <td>0.525331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             y       GHG  Current account balance  \\\n",
       "2005  2.051359  0.661859                 1.852348   \n",
       "2006  2.518101  0.640522                 1.845025   \n",
       "2007  2.072474  0.661971                 1.786614   \n",
       "2008  2.013436  0.617975                 1.818870   \n",
       "2009  0.234375  0.690894                 1.812187   \n",
       "2010  1.860920  0.634447                 1.827969   \n",
       "\n",
       "      General government net lending/borrowing       PPP  \\\n",
       "2005                                 -0.058174  0.654957   \n",
       "2006                                 -1.246360  0.662903   \n",
       "2007                                 -6.907755  0.658295   \n",
       "2008                                  3.318150  0.662904   \n",
       "2009                                  0.607133  0.665526   \n",
       "2010                                 -0.435978  0.659129   \n",
       "\n",
       "      Inflation, average consumer prices  ExchangeR  \n",
       "2005                            1.051777   0.547797  \n",
       "2006                            1.167132   0.503668  \n",
       "2007                            1.140200   0.513078  \n",
       "2008                            0.706676   0.583932  \n",
       "2009                            1.869612   0.582607  \n",
       "2010                            1.428403   0.525331  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = database_scaler[country]\n",
    "\n",
    "database_validation_sv[country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.051359</td>\n",
       "      <td>0.661859</td>\n",
       "      <td>1.852348</td>\n",
       "      <td>-0.058174</td>\n",
       "      <td>0.654957</td>\n",
       "      <td>1.051777</td>\n",
       "      <td>0.547797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.518101</td>\n",
       "      <td>0.640522</td>\n",
       "      <td>1.845025</td>\n",
       "      <td>-1.246360</td>\n",
       "      <td>0.662903</td>\n",
       "      <td>1.167132</td>\n",
       "      <td>0.503668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.072474</td>\n",
       "      <td>0.661971</td>\n",
       "      <td>1.786614</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>0.658295</td>\n",
       "      <td>1.140200</td>\n",
       "      <td>0.513078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.013436</td>\n",
       "      <td>0.617975</td>\n",
       "      <td>1.818870</td>\n",
       "      <td>3.318150</td>\n",
       "      <td>0.662904</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.583932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.690894</td>\n",
       "      <td>1.812187</td>\n",
       "      <td>0.607133</td>\n",
       "      <td>0.665526</td>\n",
       "      <td>1.869612</td>\n",
       "      <td>0.582607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.860920</td>\n",
       "      <td>0.634447</td>\n",
       "      <td>1.827969</td>\n",
       "      <td>-0.435978</td>\n",
       "      <td>0.659129</td>\n",
       "      <td>1.428403</td>\n",
       "      <td>0.525331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6\n",
       "0  2.051359  0.661859  1.852348 -0.058174  0.654957  1.051777  0.547797\n",
       "1  2.518101  0.640522  1.845025 -1.246360  0.662903  1.167132  0.503668\n",
       "2  2.072474  0.661971  1.786614 -6.907755  0.658295  1.140200  0.513078\n",
       "3  2.013436  0.617975  1.818870  3.318150  0.662904  0.706676  0.583932\n",
       "4  0.234375  0.690894  1.812187  0.607133  0.665526  1.869612  0.582607\n",
       "5  1.860920  0.634447  1.827969 -0.435978  0.659129  1.428403  0.525331"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(database_validation_sv_standard[country]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the output back to original scale: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = database_validation_sv_standard[country]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwriting the forecast to the dataframe in order to call the inverse_transform method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013692</td>\n",
       "      <td>0.977376</td>\n",
       "      <td>0.233201</td>\n",
       "      <td>-1.540170</td>\n",
       "      <td>-1.811933</td>\n",
       "      <td>0.173565</td>\n",
       "      <td>0.031546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.824086</td>\n",
       "      <td>0.229011</td>\n",
       "      <td>-5.745871</td>\n",
       "      <td>-0.464783</td>\n",
       "      <td>0.244783</td>\n",
       "      <td>-0.691070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.986829</td>\n",
       "      <td>0.195587</td>\n",
       "      <td>-25.784944</td>\n",
       "      <td>-1.245999</td>\n",
       "      <td>0.228155</td>\n",
       "      <td>-0.536981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.061869</td>\n",
       "      <td>-2.727739</td>\n",
       "      <td>0.214045</td>\n",
       "      <td>10.410665</td>\n",
       "      <td>-0.464502</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>0.623240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014176</td>\n",
       "      <td>3.428772</td>\n",
       "      <td>0.210220</td>\n",
       "      <td>0.814749</td>\n",
       "      <td>-0.020067</td>\n",
       "      <td>0.678478</td>\n",
       "      <td>0.601555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010452</td>\n",
       "      <td>-1.337028</td>\n",
       "      <td>0.219251</td>\n",
       "      <td>-2.877446</td>\n",
       "      <td>-1.104656</td>\n",
       "      <td>0.406086</td>\n",
       "      <td>-0.336342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2          3         4         5         6\n",
       "0  0.013692  0.977376  0.233201  -1.540170 -1.811933  0.173565  0.031546\n",
       "1  0.000429 -0.824086  0.229011  -5.745871 -0.464783  0.244783 -0.691070\n",
       "2  0.001205  0.986829  0.195587 -25.784944 -1.245999  0.228155 -0.536981\n",
       "3  0.061869 -2.727739  0.214045  10.410665 -0.464502 -0.039492  0.623240\n",
       "4  0.014176  3.428772  0.210220   0.814749 -0.020067  0.678478  0.601555\n",
       "5  0.010452 -1.337028  0.219251  -2.877446 -1.104656  0.406086 -0.336342"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output.iloc[:,0] = y_forecast\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.691698</td>\n",
       "      <td>0.661859</td>\n",
       "      <td>1.852348</td>\n",
       "      <td>-0.058174</td>\n",
       "      <td>0.654957</td>\n",
       "      <td>1.051777</td>\n",
       "      <td>0.547797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.667858</td>\n",
       "      <td>0.640522</td>\n",
       "      <td>1.845025</td>\n",
       "      <td>-1.246360</td>\n",
       "      <td>0.662903</td>\n",
       "      <td>1.167132</td>\n",
       "      <td>0.503668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.669252</td>\n",
       "      <td>0.661971</td>\n",
       "      <td>1.786614</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>0.658295</td>\n",
       "      <td>1.140200</td>\n",
       "      <td>0.513078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.778293</td>\n",
       "      <td>0.617975</td>\n",
       "      <td>1.818870</td>\n",
       "      <td>3.318150</td>\n",
       "      <td>0.662904</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.583932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.692568</td>\n",
       "      <td>0.690894</td>\n",
       "      <td>1.812187</td>\n",
       "      <td>0.607133</td>\n",
       "      <td>0.665526</td>\n",
       "      <td>1.869612</td>\n",
       "      <td>0.582607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.685873</td>\n",
       "      <td>0.634447</td>\n",
       "      <td>1.827969</td>\n",
       "      <td>-0.435978</td>\n",
       "      <td>0.659129</td>\n",
       "      <td>1.428403</td>\n",
       "      <td>0.525331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6\n",
       "0  1.691698  0.661859  1.852348 -0.058174  0.654957  1.051777  0.547797\n",
       "1  1.667858  0.640522  1.845025 -1.246360  0.662903  1.167132  0.503668\n",
       "2  1.669252  0.661971  1.786614 -6.907755  0.658295  1.140200  0.513078\n",
       "3  1.778293  0.617975  1.818870  3.318150  0.662904  0.706676  0.583932\n",
       "4  1.692568  0.690894  1.812187  0.607133  0.665526  1.869612  0.582607\n",
       "5  1.685873  0.634447  1.827969 -0.435978  0.659129  1.428403  0.525331"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output = pd.DataFrame(scaler.inverse_transform(df_output))\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.69169797, 1.66785757, 1.66925199, 1.7782935 , 1.69256768,\n",
       "       1.6858734 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_forecast = df_output.iloc[:,0].values\n",
    "y_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.05135894, 2.51810131, 2.07247393, 2.01343609, 0.23437483,\n",
       "       1.86092044])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_validation_sv[country].iloc[:,0].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
