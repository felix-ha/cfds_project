{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for printing the definition of custom functions\n",
    "import inspect\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "# pytorch\n",
    "from torch import nn, no_grad, save, load\n",
    "from torch import from_numpy, zeros\n",
    "from torch.optim import SGD\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark')\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "n_epochs = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir = os.path.join(r'C:/Users/hauer/Documents/Repositories/cfds_project', 'database.pickle')\n",
    "\n",
    "with open(database_dir,'rb') as f: \n",
    "    db = pickle.load(f)\n",
    "    \n",
    "database_training = db['database_training']\n",
    "database_validation = db['database_validation']\n",
    "database_test = db['database_test']\n",
    "\n",
    "database_training_sv = db['database_training_sv']\n",
    "database_validation_sv = db['database_validation_sv']\n",
    "database_test_sv = db['database_test_sv']\n",
    "\n",
    "database_training_sv_standard = db['database_training_sv_standard']\n",
    "database_validation_sv_standard = db['database_validation_sv_standard']\n",
    "database_test_sv_standard = db['database_test_sv_standard']\n",
    "\n",
    "database_scaler = db['database_scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RNN start\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# # Prepare Data for RNN\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "N, dummy_dim = database_training_sv_standard['Germany'].shape\n",
    "dummy_dim -= 1\n",
    "\n",
    "time_steps = 16\n",
    "horizon = 1\n",
    "sequence_length = time_steps + horizon \n",
    "\n",
    "\n",
    "max_index = N - sequence_length + 1\n",
    "\n",
    "number_of_countries = len(database_training_sv_standard.keys())\n",
    "\n",
    "X = np.empty([0, sequence_length,dummy_dim])\n",
    "y = np.empty([0, sequence_length])\n",
    "\n",
    " \n",
    "\n",
    "for country in database_training.keys():\n",
    "    df_training_current = database_training_sv_standard[country]\n",
    "\n",
    "    X_current = np.empty([max_index, sequence_length,dummy_dim])\n",
    "    y_current = np.empty([max_index, sequence_length])\n",
    "\n",
    "    for i in range(max_index):\n",
    "\n",
    "        X_current[i] = df_training_current.iloc[i:i+sequence_length,1:].values\n",
    "        y_current[i] = df_training_current.iloc[i:i+sequence_length,0].values\n",
    "        \n",
    "    X = np.concatenate((X, X_current))\n",
    "    y = np.concatenate((y, y_current))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "N, seq_len, dummy_dim = X.shape\n",
    "\n",
    "input_size=dummy_dim\n",
    "n_layers=1\n",
    "output_size=1\n",
    "test_size = 0.20\n",
    "batch_size = 25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=123)\n",
    "\n",
    "\n",
    "X_train_T = from_numpy(X_train).float()\n",
    "y_train_T = from_numpy(y_train).float()\n",
    "X_val_T = from_numpy(X_val).float()\n",
    "y_val_T = from_numpy(y_val).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_T, y_train_T)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size)  \n",
    "\n",
    "valid_ds = TensorDataset(X_val_T, y_val_T)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 19.94 valid loss: 3.222\n",
      "Epoch 100: train loss: 17.97 valid loss: 3.028\n",
      "Epoch 200: train loss: 17.71 valid loss: 3.01\n",
      "Epoch 300: train loss: 17.64 valid loss: 3.005\n",
      "Epoch 400: train loss: 17.6 valid loss: 2.995\n",
      "Epoch 500: train loss: 17.59 valid loss: 2.986\n",
      "Epoch 600: train loss: 17.57 valid loss: 2.98\n",
      "Epoch 700: train loss: 17.57 valid loss: 2.975\n",
      "Epoch 800: train loss: 17.56 valid loss: 2.971\n",
      "Epoch 900: train loss: 17.55 valid loss: 2.966\n",
      "Epoch 1000: train loss: 17.53 valid loss: 2.959\n",
      "Epoch 1100: train loss: 17.5 valid loss: 2.946\n",
      "Epoch 1200: train loss: 17.45 valid loss: 2.932\n",
      "Epoch 1300: train loss: 17.4 valid loss: 2.923\n",
      "Epoch 1400: train loss: 17.35 valid loss: 2.915\n",
      "Epoch 1500: train loss: 17.31 valid loss: 2.909\n",
      "Epoch 1600: train loss: 17.27 valid loss: 2.903\n",
      "Epoch 1700: train loss: 17.22 valid loss: 2.897\n",
      "Epoch 1800: train loss: 17.19 valid loss: 2.892\n",
      "Epoch 1900: train loss: 17.15 valid loss: 2.887\n",
      "Epoch 2000: train loss: 17.12 valid loss: 2.883\n",
      "Epoch 2100: train loss: 17.08 valid loss: 2.878\n",
      "Epoch 2200: train loss: 17.06 valid loss: 2.875\n",
      "Epoch 2300: train loss: 17.03 valid loss: 2.871\n",
      "Epoch 2400: train loss: 17.01 valid loss: 2.868\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN'\n",
    "hidden_dim=3\n",
    "lr = 0.03\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 24.13 valid loss: 4.054\n",
      "Epoch 100: train loss: 24.0 valid loss: 4.032\n",
      "Epoch 200: train loss: 23.88 valid loss: 4.011\n",
      "Epoch 300: train loss: 23.76 valid loss: 3.991\n",
      "Epoch 400: train loss: 23.64 valid loss: 3.971\n",
      "Epoch 500: train loss: 23.53 valid loss: 3.951\n",
      "Epoch 600: train loss: 23.41 valid loss: 3.931\n",
      "Epoch 700: train loss: 23.3 valid loss: 3.911\n",
      "Epoch 800: train loss: 23.19 valid loss: 3.892\n",
      "Epoch 900: train loss: 23.08 valid loss: 3.874\n",
      "Epoch 1000: train loss: 22.97 valid loss: 3.855\n",
      "Epoch 1100: train loss: 22.87 valid loss: 3.837\n",
      "Epoch 1200: train loss: 22.76 valid loss: 3.819\n",
      "Epoch 1300: train loss: 22.66 valid loss: 3.801\n",
      "Epoch 1400: train loss: 22.56 valid loss: 3.784\n",
      "Epoch 1500: train loss: 22.46 valid loss: 3.766\n",
      "Epoch 1600: train loss: 22.36 valid loss: 3.749\n",
      "Epoch 1700: train loss: 22.27 valid loss: 3.733\n",
      "Epoch 1800: train loss: 22.17 valid loss: 3.716\n",
      "Epoch 1900: train loss: 22.08 valid loss: 3.7\n",
      "Epoch 2000: train loss: 21.99 valid loss: 3.684\n",
      "Epoch 2100: train loss: 21.89 valid loss: 3.668\n",
      "Epoch 2200: train loss: 21.81 valid loss: 3.653\n",
      "Epoch 2300: train loss: 21.72 valid loss: 3.638\n",
      "Epoch 2400: train loss: 21.63 valid loss: 3.623\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN_Adam'\n",
    "hidden_dim=3\n",
    "lr = 1e-06\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    " \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 18.57 valid loss: 3.122\n",
      "Epoch 100: train loss: 18.5 valid loss: 3.108\n",
      "Epoch 200: train loss: 18.45 valid loss: 3.099\n",
      "Epoch 300: train loss: 18.42 valid loss: 3.091\n",
      "Epoch 400: train loss: 18.39 valid loss: 3.085\n",
      "Epoch 500: train loss: 18.36 valid loss: 3.08\n",
      "Epoch 600: train loss: 18.34 valid loss: 3.076\n",
      "Epoch 700: train loss: 18.32 valid loss: 3.072\n",
      "Epoch 800: train loss: 18.31 valid loss: 3.069\n",
      "Epoch 900: train loss: 18.29 valid loss: 3.066\n",
      "Epoch 1000: train loss: 18.28 valid loss: 3.064\n",
      "Epoch 1100: train loss: 18.27 valid loss: 3.062\n",
      "Epoch 1200: train loss: 18.26 valid loss: 3.06\n",
      "Epoch 1300: train loss: 18.25 valid loss: 3.059\n",
      "Epoch 1400: train loss: 18.25 valid loss: 3.057\n",
      "Epoch 1500: train loss: 18.24 valid loss: 3.056\n",
      "Epoch 1600: train loss: 18.24 valid loss: 3.055\n",
      "Epoch 1700: train loss: 18.24 valid loss: 3.054\n",
      "Epoch 1800: train loss: 18.23 valid loss: 3.054\n",
      "Epoch 1900: train loss: 18.23 valid loss: 3.053\n",
      "Epoch 2000: train loss: 18.23 valid loss: 3.053\n",
      "Epoch 2100: train loss: 18.22 valid loss: 3.052\n",
      "Epoch 2200: train loss: 18.22 valid loss: 3.052\n",
      "Epoch 2300: train loss: 18.22 valid loss: 3.051\n",
      "Epoch 2400: train loss: 18.22 valid loss: 3.051\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN_Large_Adam'\n",
    "hidden_dim=64\n",
    "lr = 1e-06\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    " \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 18.69 valid loss: 3.078\n",
      "Epoch 100: train loss: 18.14 valid loss: 3.053\n",
      "Epoch 200: train loss: 17.62 valid loss: 3.001\n",
      "Epoch 300: train loss: 17.0 valid loss: 2.96\n",
      "Epoch 400: train loss: 16.54 valid loss: 2.933\n",
      "Epoch 500: train loss: 16.12 valid loss: 2.879\n",
      "Epoch 600: train loss: 15.73 valid loss: 2.814\n",
      "Epoch 700: train loss: 15.39 valid loss: 2.768\n",
      "Epoch 800: train loss: 15.08 valid loss: 2.741\n",
      "Epoch 900: train loss: 14.78 valid loss: 2.718\n",
      "Epoch 1000: train loss: 14.52 valid loss: 2.699\n",
      "Epoch 1100: train loss: 14.28 valid loss: 2.683\n",
      "Epoch 1200: train loss: 14.07 valid loss: 2.67\n",
      "Epoch 1300: train loss: 13.85 valid loss: 2.657\n",
      "Epoch 1400: train loss: 13.64 valid loss: 2.638\n",
      "Epoch 1500: train loss: 13.45 valid loss: 2.618\n",
      "Epoch 1600: train loss: 13.26 valid loss: 2.602\n",
      "Epoch 1700: train loss: 13.1 valid loss: 2.592\n",
      "Epoch 1800: train loss: 12.95 valid loss: 2.588\n",
      "Epoch 1900: train loss: 12.81 valid loss: 2.587\n",
      "Epoch 2000: train loss: 12.68 valid loss: 2.587\n",
      "Epoch 2100: train loss: 12.57 valid loss: 2.589\n",
      "Epoch 2200: train loss: 12.47 valid loss: 2.593\n",
      "Epoch 2300: train loss: 12.37 valid loss: 2.598\n",
      "Epoch 2400: train loss: 12.29 valid loss: 2.602\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, state):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden, state))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM'\n",
    "hidden_dim=10\n",
    "lr = 0.03\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "state_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0, state_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0, state_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Adam Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 18.53 valid loss: 3.098\n",
      "Epoch 100: train loss: 18.51 valid loss: 3.095\n",
      "Epoch 200: train loss: 18.5 valid loss: 3.093\n",
      "Epoch 300: train loss: 18.48 valid loss: 3.091\n",
      "Epoch 400: train loss: 18.47 valid loss: 3.089\n",
      "Epoch 500: train loss: 18.45 valid loss: 3.087\n",
      "Epoch 600: train loss: 18.44 valid loss: 3.086\n",
      "Epoch 700: train loss: 18.43 valid loss: 3.084\n",
      "Epoch 800: train loss: 18.42 valid loss: 3.083\n",
      "Epoch 900: train loss: 18.41 valid loss: 3.081\n",
      "Epoch 1000: train loss: 18.4 valid loss: 3.08\n",
      "Epoch 1100: train loss: 18.39 valid loss: 3.078\n",
      "Epoch 1200: train loss: 18.38 valid loss: 3.077\n",
      "Epoch 1300: train loss: 18.37 valid loss: 3.076\n",
      "Epoch 1400: train loss: 18.36 valid loss: 3.075\n",
      "Epoch 1500: train loss: 18.35 valid loss: 3.074\n",
      "Epoch 1600: train loss: 18.34 valid loss: 3.072\n",
      "Epoch 1700: train loss: 18.33 valid loss: 3.071\n",
      "Epoch 1800: train loss: 18.32 valid loss: 3.07\n",
      "Epoch 1900: train loss: 18.32 valid loss: 3.069\n",
      "Epoch 2000: train loss: 18.31 valid loss: 3.068\n",
      "Epoch 2100: train loss: 18.3 valid loss: 3.067\n",
      "Epoch 2200: train loss: 18.29 valid loss: 3.067\n",
      "Epoch 2300: train loss: 18.29 valid loss: 3.066\n",
      "Epoch 2400: train loss: 18.28 valid loss: 3.065\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, state):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden, state))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM_Large_Adam'\n",
    "hidden_dim=64\n",
    "lr = 1e-06\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "state_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0, state_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0, state_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 19.094886 valid loss: 3.2811074\n",
      "Epoch 10: train loss: 19.053558 valid loss: 3.280194\n",
      "Epoch 20: train loss: 19.047066 valid loss: 3.2792656\n",
      "Epoch 30: train loss: 19.040112 valid loss: 3.2782838\n",
      "Epoch 40: train loss: 19.032195 valid loss: 3.2771899\n",
      "Epoch 50: train loss: 19.022929 valid loss: 3.2759476\n",
      "Epoch 60: train loss: 19.012048 valid loss: 3.2745374\n",
      "Epoch 70: train loss: 18.99938 valid loss: 3.2729565\n",
      "Epoch 80: train loss: 18.984718 valid loss: 3.2711992\n",
      "Epoch 90: train loss: 18.967553 valid loss: 3.2692292\n",
      "Epoch 100: train loss: 18.94678 valid loss: 3.2669645\n",
      "Epoch 110: train loss: 18.920457 valid loss: 3.2642727\n",
      "Epoch 120: train loss: 18.885518 valid loss: 3.2609653\n",
      "Epoch 130: train loss: 18.837322 valid loss: 3.2567624\n",
      "Epoch 140: train loss: 18.769374 valid loss: 3.2512263\n",
      "Epoch 150: train loss: 18.674439 valid loss: 3.2438012\n",
      "Epoch 160: train loss: 18.548902 valid loss: 3.2343116\n",
      "Epoch 170: train loss: 18.397898 valid loss: 3.2234988\n",
      "Epoch 180: train loss: 18.23024 valid loss: 3.2116029\n",
      "Epoch 190: train loss: 18.04835 valid loss: 3.1970742\n",
      "Epoch 200: train loss: 17.850964 valid loss: 3.1786928\n",
      "Epoch 210: train loss: 17.642005 valid loss: 3.1575236\n",
      "Epoch 220: train loss: 17.431494 valid loss: 3.1340677\n",
      "Epoch 230: train loss: 17.223305 valid loss: 3.106964\n",
      "Epoch 240: train loss: 17.01832 valid loss: 3.0766712\n",
      "Epoch 250: train loss: 16.821011 valid loss: 3.0466594\n",
      "Epoch 260: train loss: 16.632218 valid loss: 3.0190745\n",
      "Epoch 270: train loss: 16.44696 valid loss: 2.9939139\n",
      "Epoch 280: train loss: 16.258242 valid loss: 2.9705665\n",
      "Epoch 290: train loss: 16.05833 valid loss: 2.9480969\n",
      "Epoch 300: train loss: 15.838396 valid loss: 2.9249169\n",
      "Epoch 310: train loss: 15.585406 valid loss: 2.897945\n",
      "Epoch 320: train loss: 15.280065 valid loss: 2.8680407\n",
      "Epoch 330: train loss: 14.917189 valid loss: 2.8457622\n",
      "Epoch 340: train loss: 14.50674 valid loss: 2.8286216\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_1, state_1, hidden_2, state_2):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden_1, state_1))      \n",
    "        r_out, (hidden_out, state_out) = self.lstm2(r_out, (hidden_2, state_2))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM_Stacked'\n",
    "hidden_dim=32\n",
    "lr = 0.1\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_01 = zeros(1, seq_len, hidden_dim)\n",
    "state_01 = zeros(1, seq_len, hidden_dim)\n",
    "\n",
    "hidden_02 = zeros(1, seq_len, hidden_dim)\n",
    "state_02 = zeros(1, seq_len, hidden_dim)\n",
    "\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_01, state_01, hidden_02, state_02)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_01, state_01, hidden_02, state_02)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {}: train loss: {:.8} valid loss: {:.8}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(n_epochs)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(12,10))\n",
    "\n",
    "for model_ in models:\n",
    "    name = model_[0]\n",
    "    training_loss = model_[1]\n",
    "    valid_loss =model_[2] \n",
    "\n",
    "    \n",
    "    label = name \n",
    "    \n",
    "    ax[0].plot(x,  training_loss, label=label)\n",
    "    ax[1].plot(x,  valid_loss, label=label)\n",
    "    #ax[2].plot([i for i in range(n_epochs-1)],  valid_loss[1:] / valid_loss[:-1] * 100, label=label)\n",
    "\n",
    "\n",
    "#ax[2].set_xlabel('epoch') \n",
    "\n",
    "ax[0].set_ylabel('loss') \n",
    "ax[1].set_ylabel('loss')\n",
    "\n",
    "ax[0].set_title(\"training loss\")\n",
    "ax[1].set_title(\"validation loss\")\n",
    "#ax[2].set_title(\"validation loss change in %\")\n",
    "\n",
    "legend  = ax[0].legend(bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "#ax[2].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# # Serializing model \n",
    "# =============================================================================\n",
    "\n",
    "wdir= r'C:/Users/hauer/Documents/Repositories/cfds_project'\n",
    "save_dir = os.path.join(wdir, 'pytorch_models')\n",
    "model_name = 'rnn.torch'\n",
    "\n",
    "if(not os.path.isdir(save_dir)):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "save(model.state_dict(), os.path.join(save_dir, model_name))\n",
    "\n",
    "#model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "#model.load_state_dict(load( os.path.join(save_dir, model_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09360281, -0.04636378, -0.18608621,  0.00946208], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = 'Germany'\n",
    "\n",
    "df = database_training_sv_standard[country].append(database_validation_sv_standard[country])\n",
    "\n",
    "n_forecast_validation, _ = database_validation_sv_standard[country].shape\n",
    "\n",
    "X_eval = df.iloc[:,1:].values\n",
    "y_eval = df.iloc[:,0].values\n",
    "X_eval_T = from_numpy(X_eval).float()\n",
    "N, _ = X_eval_T.shape\n",
    "X_eval_T = X_eval_T.view([-1, N, dummy_dim])\n",
    "\n",
    "hidden_1 = zeros(1, N, hidden_dim)\n",
    "state_1 = zeros(1, N, hidden_dim)\n",
    "\n",
    "hidden_2 = zeros(1, N, hidden_dim)\n",
    "state_2 = zeros(1, N, hidden_dim)\n",
    "\n",
    "model.eval()\n",
    "with no_grad():\n",
    "    y_hat = model(X_eval_T, hidden_1, state_1, hidden_2, state_2)\n",
    "    \n",
    "y_hat =  y_hat.view(-1).numpy()\n",
    "y_forecast = y_hat[-n_forecast_validation:]\n",
    "y_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing with the ground truth.\n",
    "\n",
    "First check right application of scaling. The unscaled data must equal the scaled data after appling the inverse_transform method from sklearn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>Inflation, average consumer prices</th>\n",
       "      <th>ExchangeR</th>\n",
       "      <th>General government net lending/borrowing</th>\n",
       "      <th>GHG</th>\n",
       "      <th>Current account balance</th>\n",
       "      <th>PPP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0.234375</td>\n",
       "      <td>1.869612</td>\n",
       "      <td>0.582607</td>\n",
       "      <td>0.607133</td>\n",
       "      <td>0.690894</td>\n",
       "      <td>1.812187</td>\n",
       "      <td>0.665526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1.860920</td>\n",
       "      <td>1.428403</td>\n",
       "      <td>0.525331</td>\n",
       "      <td>-0.435978</td>\n",
       "      <td>0.634447</td>\n",
       "      <td>1.827969</td>\n",
       "      <td>0.659129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>2.091820</td>\n",
       "      <td>1.027137</td>\n",
       "      <td>0.599944</td>\n",
       "      <td>-0.831883</td>\n",
       "      <td>0.667861</td>\n",
       "      <td>1.838487</td>\n",
       "      <td>0.668243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>1.984308</td>\n",
       "      <td>0.988643</td>\n",
       "      <td>0.534140</td>\n",
       "      <td>1.587170</td>\n",
       "      <td>0.667618</td>\n",
       "      <td>1.801603</td>\n",
       "      <td>0.661074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             y  Inflation, average consumer prices  ExchangeR  \\\n",
       "2009  0.234375                            1.869612   0.582607   \n",
       "2010  1.860920                            1.428403   0.525331   \n",
       "2011  2.091820                            1.027137   0.599944   \n",
       "2012  1.984308                            0.988643   0.534140   \n",
       "\n",
       "      General government net lending/borrowing       GHG  \\\n",
       "2009                                  0.607133  0.690894   \n",
       "2010                                 -0.435978  0.634447   \n",
       "2011                                 -0.831883  0.667861   \n",
       "2012                                  1.587170  0.667618   \n",
       "\n",
       "      Current account balance       PPP  \n",
       "2009                 1.812187  0.665526  \n",
       "2010                 1.827969  0.659129  \n",
       "2011                 1.838487  0.668243  \n",
       "2012                 1.801603  0.661074  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = database_scaler[country]\n",
    "\n",
    "database_validation_sv[country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.234375</td>\n",
       "      <td>1.869612</td>\n",
       "      <td>0.582607</td>\n",
       "      <td>0.607133</td>\n",
       "      <td>0.690894</td>\n",
       "      <td>1.812187</td>\n",
       "      <td>0.665526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.860920</td>\n",
       "      <td>1.428403</td>\n",
       "      <td>0.525331</td>\n",
       "      <td>-0.435978</td>\n",
       "      <td>0.634447</td>\n",
       "      <td>1.827969</td>\n",
       "      <td>0.659129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.091820</td>\n",
       "      <td>1.027137</td>\n",
       "      <td>0.599944</td>\n",
       "      <td>-0.831883</td>\n",
       "      <td>0.667861</td>\n",
       "      <td>1.838487</td>\n",
       "      <td>0.668243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.984308</td>\n",
       "      <td>0.988643</td>\n",
       "      <td>0.534140</td>\n",
       "      <td>1.587170</td>\n",
       "      <td>0.667618</td>\n",
       "      <td>1.801603</td>\n",
       "      <td>0.661074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6\n",
       "0  0.234375  1.869612  0.582607  0.607133  0.690894  1.812187  0.665526\n",
       "1  1.860920  1.428403  0.525331 -0.435978  0.634447  1.827969  0.659129\n",
       "2  2.091820  1.027137  0.599944 -0.831883  0.667861  1.838487  0.668243\n",
       "3  1.984308  0.988643  0.534140  1.587170  0.667618  1.801603  0.661074"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(database_validation_sv_standard[country]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the output back to original scale: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = database_validation_sv_standard[country]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwriting the forecast to the dataframe in order to call the inverse_transform method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.850967</td>\n",
       "      <td>0.707503</td>\n",
       "      <td>0.656411</td>\n",
       "      <td>0.298846</td>\n",
       "      <td>3.164935</td>\n",
       "      <td>0.192775</td>\n",
       "      <td>0.120834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.968357</td>\n",
       "      <td>0.414082</td>\n",
       "      <td>-0.333327</td>\n",
       "      <td>-0.384181</td>\n",
       "      <td>-1.162583</td>\n",
       "      <td>0.202497</td>\n",
       "      <td>-0.950455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.799334</td>\n",
       "      <td>0.147225</td>\n",
       "      <td>0.955985</td>\n",
       "      <td>-0.643419</td>\n",
       "      <td>1.399100</td>\n",
       "      <td>0.208976</td>\n",
       "      <td>0.575925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.652370</td>\n",
       "      <td>0.121625</td>\n",
       "      <td>-0.181108</td>\n",
       "      <td>0.940573</td>\n",
       "      <td>1.380516</td>\n",
       "      <td>0.186256</td>\n",
       "      <td>-0.624739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6\n",
       "0  0.850967  0.707503  0.656411  0.298846  3.164935  0.192775  0.120834\n",
       "1  0.968357  0.414082 -0.333327 -0.384181 -1.162583  0.202497 -0.950455\n",
       "2  0.799334  0.147225  0.955985 -0.643419  1.399100  0.208976  0.575925\n",
       "3  0.652370  0.121625 -0.181108  0.940573  1.380516  0.186256 -0.624739"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output.iloc[:,0] = y_forecast\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.163407</td>\n",
       "      <td>1.869612</td>\n",
       "      <td>0.582607</td>\n",
       "      <td>0.607133</td>\n",
       "      <td>0.690894</td>\n",
       "      <td>1.812187</td>\n",
       "      <td>0.665526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.360033</td>\n",
       "      <td>1.428403</td>\n",
       "      <td>0.525331</td>\n",
       "      <td>-0.435978</td>\n",
       "      <td>0.634447</td>\n",
       "      <td>1.827969</td>\n",
       "      <td>0.659129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.076922</td>\n",
       "      <td>1.027137</td>\n",
       "      <td>0.599944</td>\n",
       "      <td>-0.831883</td>\n",
       "      <td>0.667861</td>\n",
       "      <td>1.838487</td>\n",
       "      <td>0.668243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.830761</td>\n",
       "      <td>0.988643</td>\n",
       "      <td>0.534140</td>\n",
       "      <td>1.587170</td>\n",
       "      <td>0.667618</td>\n",
       "      <td>1.801603</td>\n",
       "      <td>0.661074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6\n",
       "0  3.163407  1.869612  0.582607  0.607133  0.690894  1.812187  0.665526\n",
       "1  3.360033  1.428403  0.525331 -0.435978  0.634447  1.827969  0.659129\n",
       "2  3.076922  1.027137  0.599944 -0.831883  0.667861  1.838487  0.668243\n",
       "3  2.830761  0.988643  0.534140  1.587170  0.667618  1.801603  0.661074"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output = pd.DataFrame(scaler.inverse_transform(df_output))\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.16340733, 3.36003337, 3.07692156, 2.83076093])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_forecast = df_output.iloc[:,0].values\n",
    "y_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23437483, 1.86092044, 2.0918199 , 1.98430846])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_validation_sv[country].iloc[:,0].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
