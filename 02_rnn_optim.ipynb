{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for printing the definition of custom functions\n",
    "import inspect\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "# pytorch\n",
    "from torch import nn, no_grad, save, load\n",
    "from torch import from_numpy, zeros\n",
    "from torch.optim import SGD\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark')\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir = os.path.join(r'C:/Users/hauer/Documents/Repositories/cfds_project', 'database_new.pickle')\n",
    "\n",
    "with open(database_dir,'rb') as f: \n",
    "    db = pickle.load(f)\n",
    "    \n",
    "database_training = db['database_training']\n",
    "database_validation = db['database_validation']\n",
    "database_test = db['database_test']\n",
    "\n",
    "database_training_sv = db['database_training_sv']\n",
    "database_validation_sv = db['database_validation_sv']\n",
    "database_test_sv = db['database_test_sv']\n",
    "\n",
    "database_training_sv_standard = db['database_training_sv_standard']\n",
    "database_validation_sv_standard = db['database_validation_sv_standard']\n",
    "database_test_sv_standard = db['database_test_sv_standard']\n",
    "\n",
    "database_scaler = db['database_scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RNN start\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# # Prepare Data for RNN\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Combining orignal training and \n",
    "database_training = {}\n",
    "\n",
    "for country in database_training_sv_standard.keys():\n",
    "    df_to_add = database_training_sv_standard[country].append(database_validation_sv_standard[country])\n",
    "    df_to_add = df_to_add.reset_index()\n",
    "    del df_to_add['index']\n",
    "\n",
    "    database_training[country] = df_to_add\n",
    "\n",
    "\n",
    "N, dummy_dim = database_training['Germany'].shape\n",
    "dummy_dim -= 1\n",
    "\n",
    "time_steps = 16\n",
    "horizon = 1\n",
    "sequence_length = time_steps + horizon \n",
    "\n",
    "\n",
    "max_index = N - sequence_length + 1\n",
    "\n",
    "number_of_countries = len(database_training.keys())\n",
    "\n",
    "X = np.empty([0, sequence_length,dummy_dim])\n",
    "y = np.empty([0, sequence_length])\n",
    "\n",
    " \n",
    "\n",
    "for country in database_training.keys():\n",
    "    df_training_current = database_training[country]\n",
    "\n",
    "    X_current = np.empty([max_index, sequence_length,dummy_dim])\n",
    "    y_current = np.empty([max_index, sequence_length])\n",
    "\n",
    "    for i in range(max_index):\n",
    "\n",
    "        X_current[i] = df_training_current.iloc[i:i+sequence_length,1:].values\n",
    "        y_current[i] = df_training_current.iloc[i:i+sequence_length,0].values\n",
    "        \n",
    "    X = np.concatenate((X, X_current))\n",
    "    y = np.concatenate((y, y_current))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "N, seq_len, dummy_dim = X.shape\n",
    "\n",
    "input_size=dummy_dim\n",
    "n_layers=1\n",
    "output_size=1\n",
    "test_size = 0.20\n",
    "batch_size = 25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=123)\n",
    "\n",
    "\n",
    "X_train_T = from_numpy(X_train).float()\n",
    "y_train_T = from_numpy(y_train).float()\n",
    "X_val_T = from_numpy(X_val).float()\n",
    "y_val_T = from_numpy(y_val).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_T, y_train_T)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size)  \n",
    "\n",
    "valid_ds = TensorDataset(X_val_T, y_val_T)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 140.1 valid loss: 4.413\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN'\n",
    "hidden_dim=3\n",
    "lr = 0.03\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 147.7 valid loss: 6.04\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN_Adam'\n",
    "hidden_dim=3\n",
    "lr = 1e-06\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    " \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 139.8 valid loss: 4.463\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN_Large_Adam'\n",
    "hidden_dim=64\n",
    "lr = 1e-06\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    " \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 138.0 valid loss: 4.323\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, state):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden, state))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM'\n",
    "hidden_dim=10\n",
    "lr = 0.03\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "state_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0, state_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0, state_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Adam Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 138.4 valid loss: 4.329\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, state):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden, state))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM_Large_Adam'\n",
    "hidden_dim=64\n",
    "lr = 1e-06\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "state_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0, state_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0, state_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 138.13081 valid loss: 4.2863927\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_1, state_1, hidden_2, state_2):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden_1, state_1))      \n",
    "        r_out, (hidden_out, state_out) = self.lstm2(r_out, (hidden_2, state_2))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM_Stacked'\n",
    "hidden_dim=64\n",
    "lr = 0.05\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_01 = zeros(1, seq_len, hidden_dim)\n",
    "state_01 = zeros(1, seq_len, hidden_dim)\n",
    "\n",
    "hidden_02 = zeros(1, seq_len, hidden_dim)\n",
    "state_02 = zeros(1, seq_len, hidden_dim)\n",
    "\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_01, state_01, hidden_02, state_02)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_01, state_01, hidden_02, state_02)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 25 == 0:\n",
    "        print('Epoch {}: train loss: {:.8} valid loss: {:.8}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(n_epochs)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(12,10))\n",
    "\n",
    "for model_ in models:\n",
    "    name = model_[0]\n",
    "    training_loss = model_[1]\n",
    "    valid_loss =model_[2] \n",
    "\n",
    "    \n",
    "    label = name \n",
    "    \n",
    "    ax[0].plot(x,  training_loss, label=label)\n",
    "    ax[1].plot(x,  valid_loss, label=label)\n",
    "    #ax[2].plot([i for i in range(n_epochs-1)],  valid_loss[1:] / valid_loss[:-1] * 100, label=label)\n",
    "\n",
    "\n",
    "#ax[2].set_xlabel('epoch') \n",
    "\n",
    "ax[0].set_ylabel('loss') \n",
    "ax[1].set_ylabel('loss')\n",
    "\n",
    "ax[0].set_title(\"training loss\")\n",
    "ax[1].set_title(\"validation loss\")\n",
    "#ax[2].set_title(\"validation loss change in %\")\n",
    "\n",
    "legend  = ax[0].legend(bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "#ax[2].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# # Serializing model \n",
    "# =============================================================================\n",
    "\n",
    "wdir= r'C:/Users/hauer/Documents/Repositories/cfds_project'\n",
    "save_dir = os.path.join(wdir, 'pytorch_models')\n",
    "model_name = 'rnn.torch'\n",
    "\n",
    "if(not os.path.isdir(save_dir)):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "save(model.state_dict(), os.path.join(save_dir, model_name))\n",
    "\n",
    "#model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "#model.load_state_dict(load( os.path.join(save_dir, model_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03215392, -0.06455398, -0.11108989, -0.00174986, -0.01445875,\n",
       "       -0.03995248], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = 'Germany'\n",
    "\n",
    "df = database_training_sv_standard[country].append(database_validation_sv_standard[country])\n",
    "\n",
    "n_forecast_validation, _ = database_validation_sv_standard[country].shape\n",
    "\n",
    "X_eval = df.iloc[:,1:].values\n",
    "y_eval = df.iloc[:,0].values\n",
    "X_eval_T = from_numpy(X_eval).float()\n",
    "N, _ = X_eval_T.shape\n",
    "X_eval_T = X_eval_T.view([-1, N, dummy_dim])\n",
    "\n",
    "hidden_1 = zeros(1, N, hidden_dim)\n",
    "state_1 = zeros(1, N, hidden_dim)\n",
    "\n",
    "hidden_2 = zeros(1, N, hidden_dim)\n",
    "state_2 = zeros(1, N, hidden_dim)\n",
    "\n",
    "model.eval()\n",
    "with no_grad():\n",
    "    y_hat = model(X_eval_T, hidden_1, state_1, hidden_2, state_2)\n",
    "    \n",
    "y_hat =  y_hat.view(-1).numpy()\n",
    "y_forecast = y_hat[-n_forecast_validation:]\n",
    "y_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing with the ground truth.\n",
    "\n",
    "First check right application of scaling. The unscaled data must equal the scaled data after appling the inverse_transform method from sklearn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>GHG</th>\n",
       "      <th>Crude_oil_production</th>\n",
       "      <th>Inflation, average consumer prices</th>\n",
       "      <th>Current account balance</th>\n",
       "      <th>ExchangeR</th>\n",
       "      <th>Population</th>\n",
       "      <th>Working_age_population</th>\n",
       "      <th>Fertility_rates</th>\n",
       "      <th>Unemployment rate</th>\n",
       "      <th>Patents_on_environment_technologies</th>\n",
       "      <th>PPP</th>\n",
       "      <th>TOTMAT</th>\n",
       "      <th>Material_consumption</th>\n",
       "      <th>General government net lending/borrowing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>2.051359</td>\n",
       "      <td>0.661859</td>\n",
       "      <td>0.627183</td>\n",
       "      <td>1.051777</td>\n",
       "      <td>1.852348</td>\n",
       "      <td>0.547797</td>\n",
       "      <td>0.683857</td>\n",
       "      <td>0.688932</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.564285</td>\n",
       "      <td>0.724778</td>\n",
       "      <td>0.654957</td>\n",
       "      <td>0.662340</td>\n",
       "      <td>0.674629</td>\n",
       "      <td>-0.058174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>2.518101</td>\n",
       "      <td>0.640522</td>\n",
       "      <td>0.636976</td>\n",
       "      <td>1.167132</td>\n",
       "      <td>1.845025</td>\n",
       "      <td>0.503668</td>\n",
       "      <td>0.683752</td>\n",
       "      <td>0.689639</td>\n",
       "      <td>0.665051</td>\n",
       "      <td>0.530076</td>\n",
       "      <td>0.688231</td>\n",
       "      <td>0.662903</td>\n",
       "      <td>0.670703</td>\n",
       "      <td>0.662517</td>\n",
       "      <td>-1.246360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>2.072474</td>\n",
       "      <td>0.661971</td>\n",
       "      <td>0.586901</td>\n",
       "      <td>1.140200</td>\n",
       "      <td>1.786614</td>\n",
       "      <td>0.513078</td>\n",
       "      <td>0.683467</td>\n",
       "      <td>0.690186</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.535171</td>\n",
       "      <td>0.742456</td>\n",
       "      <td>0.658295</td>\n",
       "      <td>0.663577</td>\n",
       "      <td>0.659425</td>\n",
       "      <td>-6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>2.013436</td>\n",
       "      <td>0.617975</td>\n",
       "      <td>0.590802</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>1.818870</td>\n",
       "      <td>0.583932</td>\n",
       "      <td>0.683149</td>\n",
       "      <td>0.690032</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.633615</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.662904</td>\n",
       "      <td>0.651967</td>\n",
       "      <td>0.635765</td>\n",
       "      <td>3.318150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.690894</td>\n",
       "      <td>0.585055</td>\n",
       "      <td>1.869612</td>\n",
       "      <td>1.812187</td>\n",
       "      <td>0.582607</td>\n",
       "      <td>0.683654</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.559735</td>\n",
       "      <td>0.685308</td>\n",
       "      <td>0.665526</td>\n",
       "      <td>0.677563</td>\n",
       "      <td>0.661033</td>\n",
       "      <td>0.607133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1.860920</td>\n",
       "      <td>0.634447</td>\n",
       "      <td>0.671210</td>\n",
       "      <td>1.428403</td>\n",
       "      <td>1.827969</td>\n",
       "      <td>0.525331</td>\n",
       "      <td>0.675119</td>\n",
       "      <td>0.690554</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.525254</td>\n",
       "      <td>0.665850</td>\n",
       "      <td>0.659129</td>\n",
       "      <td>0.635187</td>\n",
       "      <td>0.702067</td>\n",
       "      <td>-0.435978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             y       GHG  Crude_oil_production  \\\n",
       "2005  2.051359  0.661859              0.627183   \n",
       "2006  2.518101  0.640522              0.636976   \n",
       "2007  2.072474  0.661971              0.586901   \n",
       "2008  2.013436  0.617975              0.590802   \n",
       "2009  0.234375  0.690894              0.585055   \n",
       "2010  1.860920  0.634447              0.671210   \n",
       "\n",
       "      Inflation, average consumer prices  Current account balance  ExchangeR  \\\n",
       "2005                            1.051777                 1.852348   0.547797   \n",
       "2006                            1.167132                 1.845025   0.503668   \n",
       "2007                            1.140200                 1.786614   0.513078   \n",
       "2008                            0.706676                 1.818870   0.583932   \n",
       "2009                            1.869612                 1.812187   0.582607   \n",
       "2010                            1.428403                 1.827969   0.525331   \n",
       "\n",
       "      Population  Working_age_population  Fertility_rates  Unemployment rate  \\\n",
       "2005    0.683857                0.688932         0.624690           0.564285   \n",
       "2006    0.683752                0.689639         0.665051           0.530076   \n",
       "2007    0.683467                0.690186         0.624690           0.535171   \n",
       "2008    0.683149                0.690032         0.624690           0.633615   \n",
       "2009    0.683654                0.691245         0.624690           0.559735   \n",
       "2010    0.675119                0.690554         0.624690           0.525254   \n",
       "\n",
       "      Patents_on_environment_technologies       PPP    TOTMAT  \\\n",
       "2005                             0.724778  0.654957  0.662340   \n",
       "2006                             0.688231  0.662903  0.670703   \n",
       "2007                             0.742456  0.658295  0.663577   \n",
       "2008                             0.699861  0.662904  0.651967   \n",
       "2009                             0.685308  0.665526  0.677563   \n",
       "2010                             0.665850  0.659129  0.635187   \n",
       "\n",
       "      Material_consumption  General government net lending/borrowing  \n",
       "2005              0.674629                                 -0.058174  \n",
       "2006              0.662517                                 -1.246360  \n",
       "2007              0.659425                                 -6.907755  \n",
       "2008              0.635765                                  3.318150  \n",
       "2009              0.661033                                  0.607133  \n",
       "2010              0.702067                                 -0.435978  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = database_scaler[country]\n",
    "\n",
    "database_validation_sv[country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.051359</td>\n",
       "      <td>0.661859</td>\n",
       "      <td>0.627183</td>\n",
       "      <td>1.051777</td>\n",
       "      <td>1.852348</td>\n",
       "      <td>0.547797</td>\n",
       "      <td>0.683857</td>\n",
       "      <td>0.688932</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.564285</td>\n",
       "      <td>0.724778</td>\n",
       "      <td>0.654957</td>\n",
       "      <td>0.662340</td>\n",
       "      <td>0.674629</td>\n",
       "      <td>-0.058174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.518101</td>\n",
       "      <td>0.640522</td>\n",
       "      <td>0.636976</td>\n",
       "      <td>1.167132</td>\n",
       "      <td>1.845025</td>\n",
       "      <td>0.503668</td>\n",
       "      <td>0.683752</td>\n",
       "      <td>0.689639</td>\n",
       "      <td>0.665051</td>\n",
       "      <td>0.530076</td>\n",
       "      <td>0.688231</td>\n",
       "      <td>0.662903</td>\n",
       "      <td>0.670703</td>\n",
       "      <td>0.662517</td>\n",
       "      <td>-1.246360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.072474</td>\n",
       "      <td>0.661971</td>\n",
       "      <td>0.586901</td>\n",
       "      <td>1.140200</td>\n",
       "      <td>1.786614</td>\n",
       "      <td>0.513078</td>\n",
       "      <td>0.683467</td>\n",
       "      <td>0.690186</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.535171</td>\n",
       "      <td>0.742456</td>\n",
       "      <td>0.658295</td>\n",
       "      <td>0.663577</td>\n",
       "      <td>0.659425</td>\n",
       "      <td>-6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.013436</td>\n",
       "      <td>0.617975</td>\n",
       "      <td>0.590802</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>1.818870</td>\n",
       "      <td>0.583932</td>\n",
       "      <td>0.683149</td>\n",
       "      <td>0.690032</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.633615</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.662904</td>\n",
       "      <td>0.651967</td>\n",
       "      <td>0.635765</td>\n",
       "      <td>3.318150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.690894</td>\n",
       "      <td>0.585055</td>\n",
       "      <td>1.869612</td>\n",
       "      <td>1.812187</td>\n",
       "      <td>0.582607</td>\n",
       "      <td>0.683654</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.559735</td>\n",
       "      <td>0.685308</td>\n",
       "      <td>0.665526</td>\n",
       "      <td>0.677563</td>\n",
       "      <td>0.661033</td>\n",
       "      <td>0.607133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.860920</td>\n",
       "      <td>0.634447</td>\n",
       "      <td>0.671210</td>\n",
       "      <td>1.428403</td>\n",
       "      <td>1.827969</td>\n",
       "      <td>0.525331</td>\n",
       "      <td>0.675119</td>\n",
       "      <td>0.690554</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.525254</td>\n",
       "      <td>0.665850</td>\n",
       "      <td>0.659129</td>\n",
       "      <td>0.635187</td>\n",
       "      <td>0.702067</td>\n",
       "      <td>-0.435978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  2.051359  0.661859  0.627183  1.051777  1.852348  0.547797  0.683857   \n",
       "1  2.518101  0.640522  0.636976  1.167132  1.845025  0.503668  0.683752   \n",
       "2  2.072474  0.661971  0.586901  1.140200  1.786614  0.513078  0.683467   \n",
       "3  2.013436  0.617975  0.590802  0.706676  1.818870  0.583932  0.683149   \n",
       "4  0.234375  0.690894  0.585055  1.869612  1.812187  0.582607  0.683654   \n",
       "5  1.860920  0.634447  0.671210  1.428403  1.827969  0.525331  0.675119   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0  0.688932  0.624690  0.564285  0.724778  0.654957  0.662340  0.674629   \n",
       "1  0.689639  0.665051  0.530076  0.688231  0.662903  0.670703  0.662517   \n",
       "2  0.690186  0.624690  0.535171  0.742456  0.658295  0.663577  0.659425   \n",
       "3  0.690032  0.624690  0.633615  0.699861  0.662904  0.651967  0.635765   \n",
       "4  0.691245  0.624690  0.559735  0.685308  0.665526  0.677563  0.661033   \n",
       "5  0.690554  0.624690  0.525254  0.665850  0.659129  0.635187  0.702067   \n",
       "\n",
       "         14  \n",
       "0 -0.058174  \n",
       "1 -1.246360  \n",
       "2 -6.907755  \n",
       "3  3.318150  \n",
       "4  0.607133  \n",
       "5 -0.435978  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(database_validation_sv_standard[country]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the output back to original scale: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = database_validation_sv_standard[country]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwriting the forecast to the dataframe in order to call the inverse_transform method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.032154</td>\n",
       "      <td>0.977376</td>\n",
       "      <td>-0.301234</td>\n",
       "      <td>0.173565</td>\n",
       "      <td>0.233201</td>\n",
       "      <td>0.031546</td>\n",
       "      <td>-0.973685</td>\n",
       "      <td>-0.965081</td>\n",
       "      <td>0.095519</td>\n",
       "      <td>-1.125269</td>\n",
       "      <td>2.440244</td>\n",
       "      <td>-1.811933</td>\n",
       "      <td>0.397665</td>\n",
       "      <td>1.451480</td>\n",
       "      <td>-1.581673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.064554</td>\n",
       "      <td>-0.824086</td>\n",
       "      <td>0.048749</td>\n",
       "      <td>0.244783</td>\n",
       "      <td>0.229011</td>\n",
       "      <td>-0.691070</td>\n",
       "      <td>-1.036570</td>\n",
       "      <td>-0.674260</td>\n",
       "      <td>1.468828</td>\n",
       "      <td>-1.683832</td>\n",
       "      <td>1.022371</td>\n",
       "      <td>-0.464783</td>\n",
       "      <td>1.083959</td>\n",
       "      <td>0.283843</td>\n",
       "      <td>-5.845020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.111090</td>\n",
       "      <td>0.986829</td>\n",
       "      <td>-1.740816</td>\n",
       "      <td>0.228155</td>\n",
       "      <td>0.195587</td>\n",
       "      <td>-0.536981</td>\n",
       "      <td>-1.206242</td>\n",
       "      <td>-0.448924</td>\n",
       "      <td>0.095519</td>\n",
       "      <td>-1.600646</td>\n",
       "      <td>3.126048</td>\n",
       "      <td>-1.245999</td>\n",
       "      <td>0.499214</td>\n",
       "      <td>-0.014275</td>\n",
       "      <td>-26.158759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001750</td>\n",
       "      <td>-2.727739</td>\n",
       "      <td>-1.601407</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>0.214045</td>\n",
       "      <td>0.623240</td>\n",
       "      <td>-1.396322</td>\n",
       "      <td>-0.512326</td>\n",
       "      <td>0.095519</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>1.473576</td>\n",
       "      <td>-0.464502</td>\n",
       "      <td>-0.453491</td>\n",
       "      <td>-2.295069</td>\n",
       "      <td>10.532967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.014459</td>\n",
       "      <td>3.428772</td>\n",
       "      <td>-1.806776</td>\n",
       "      <td>0.678478</td>\n",
       "      <td>0.210220</td>\n",
       "      <td>0.601555</td>\n",
       "      <td>-1.095145</td>\n",
       "      <td>-0.012993</td>\n",
       "      <td>0.095519</td>\n",
       "      <td>-1.199552</td>\n",
       "      <td>0.908968</td>\n",
       "      <td>-0.020067</td>\n",
       "      <td>1.646865</td>\n",
       "      <td>0.140759</td>\n",
       "      <td>0.805523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.039952</td>\n",
       "      <td>-1.337028</td>\n",
       "      <td>1.272160</td>\n",
       "      <td>0.406086</td>\n",
       "      <td>0.219251</td>\n",
       "      <td>-0.336342</td>\n",
       "      <td>-6.186756</td>\n",
       "      <td>-0.297629</td>\n",
       "      <td>0.095519</td>\n",
       "      <td>-1.762559</td>\n",
       "      <td>0.154082</td>\n",
       "      <td>-1.104656</td>\n",
       "      <td>-1.830473</td>\n",
       "      <td>4.096501</td>\n",
       "      <td>-2.937278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.032154  0.977376 -0.301234  0.173565  0.233201  0.031546 -0.973685   \n",
       "1 -0.064554 -0.824086  0.048749  0.244783  0.229011 -0.691070 -1.036570   \n",
       "2 -0.111090  0.986829 -1.740816  0.228155  0.195587 -0.536981 -1.206242   \n",
       "3 -0.001750 -2.727739 -1.601407 -0.039492  0.214045  0.623240 -1.396322   \n",
       "4 -0.014459  3.428772 -1.806776  0.678478  0.210220  0.601555 -1.095145   \n",
       "5 -0.039952 -1.337028  1.272160  0.406086  0.219251 -0.336342 -6.186756   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0 -0.965081  0.095519 -1.125269  2.440244 -1.811933  0.397665  1.451480   \n",
       "1 -0.674260  1.468828 -1.683832  1.022371 -0.464783  1.083959  0.283843   \n",
       "2 -0.448924  0.095519 -1.600646  3.126048 -1.245999  0.499214 -0.014275   \n",
       "3 -0.512326  0.095519  0.006758  1.473576 -0.464502 -0.453491 -2.295069   \n",
       "4 -0.012993  0.095519 -1.199552  0.908968 -0.020067  1.646865  0.140759   \n",
       "5 -0.297629  0.095519 -1.762559  0.154082 -1.104656 -1.830473  4.096501   \n",
       "\n",
       "          14  \n",
       "0  -1.581673  \n",
       "1  -5.845020  \n",
       "2 -26.158759  \n",
       "3  10.532967  \n",
       "4   0.805523  \n",
       "5  -2.937278  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output.iloc[:,0] = y_forecast\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.609291</td>\n",
       "      <td>0.661859</td>\n",
       "      <td>0.627183</td>\n",
       "      <td>1.051777</td>\n",
       "      <td>1.852348</td>\n",
       "      <td>0.547797</td>\n",
       "      <td>0.683857</td>\n",
       "      <td>0.688932</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.564285</td>\n",
       "      <td>0.724778</td>\n",
       "      <td>0.654957</td>\n",
       "      <td>0.662340</td>\n",
       "      <td>0.674629</td>\n",
       "      <td>-0.058174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.551054</td>\n",
       "      <td>0.640522</td>\n",
       "      <td>0.636976</td>\n",
       "      <td>1.167132</td>\n",
       "      <td>1.845025</td>\n",
       "      <td>0.503668</td>\n",
       "      <td>0.683752</td>\n",
       "      <td>0.689639</td>\n",
       "      <td>0.665051</td>\n",
       "      <td>0.530076</td>\n",
       "      <td>0.688231</td>\n",
       "      <td>0.662903</td>\n",
       "      <td>0.670703</td>\n",
       "      <td>0.662517</td>\n",
       "      <td>-1.246360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.467407</td>\n",
       "      <td>0.661971</td>\n",
       "      <td>0.586901</td>\n",
       "      <td>1.140200</td>\n",
       "      <td>1.786614</td>\n",
       "      <td>0.513078</td>\n",
       "      <td>0.683467</td>\n",
       "      <td>0.690186</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.535171</td>\n",
       "      <td>0.742456</td>\n",
       "      <td>0.658295</td>\n",
       "      <td>0.663577</td>\n",
       "      <td>0.659425</td>\n",
       "      <td>-6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.663942</td>\n",
       "      <td>0.617975</td>\n",
       "      <td>0.590802</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>1.818870</td>\n",
       "      <td>0.583932</td>\n",
       "      <td>0.683149</td>\n",
       "      <td>0.690032</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.633615</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.662904</td>\n",
       "      <td>0.651967</td>\n",
       "      <td>0.635765</td>\n",
       "      <td>3.318150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.641098</td>\n",
       "      <td>0.690894</td>\n",
       "      <td>0.585055</td>\n",
       "      <td>1.869612</td>\n",
       "      <td>1.812187</td>\n",
       "      <td>0.582607</td>\n",
       "      <td>0.683654</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.559735</td>\n",
       "      <td>0.685308</td>\n",
       "      <td>0.665526</td>\n",
       "      <td>0.677563</td>\n",
       "      <td>0.661033</td>\n",
       "      <td>0.607133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.595274</td>\n",
       "      <td>0.634447</td>\n",
       "      <td>0.671210</td>\n",
       "      <td>1.428403</td>\n",
       "      <td>1.827969</td>\n",
       "      <td>0.525331</td>\n",
       "      <td>0.675119</td>\n",
       "      <td>0.690554</td>\n",
       "      <td>0.624690</td>\n",
       "      <td>0.525254</td>\n",
       "      <td>0.665850</td>\n",
       "      <td>0.659129</td>\n",
       "      <td>0.635187</td>\n",
       "      <td>0.702067</td>\n",
       "      <td>-0.435978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.609291  0.661859  0.627183  1.051777  1.852348  0.547797  0.683857   \n",
       "1  1.551054  0.640522  0.636976  1.167132  1.845025  0.503668  0.683752   \n",
       "2  1.467407  0.661971  0.586901  1.140200  1.786614  0.513078  0.683467   \n",
       "3  1.663942  0.617975  0.590802  0.706676  1.818870  0.583932  0.683149   \n",
       "4  1.641098  0.690894  0.585055  1.869612  1.812187  0.582607  0.683654   \n",
       "5  1.595274  0.634447  0.671210  1.428403  1.827969  0.525331  0.675119   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0  0.688932  0.624690  0.564285  0.724778  0.654957  0.662340  0.674629   \n",
       "1  0.689639  0.665051  0.530076  0.688231  0.662903  0.670703  0.662517   \n",
       "2  0.690186  0.624690  0.535171  0.742456  0.658295  0.663577  0.659425   \n",
       "3  0.690032  0.624690  0.633615  0.699861  0.662904  0.651967  0.635765   \n",
       "4  0.691245  0.624690  0.559735  0.685308  0.665526  0.677563  0.661033   \n",
       "5  0.690554  0.624690  0.525254  0.665850  0.659129  0.635187  0.702067   \n",
       "\n",
       "         14  \n",
       "0 -0.058174  \n",
       "1 -1.246360  \n",
       "2 -6.907755  \n",
       "3  3.318150  \n",
       "4  0.607133  \n",
       "5 -0.435978  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output = pd.DataFrame(scaler.inverse_transform(df_output))\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.60929143, 1.55105364, 1.46740721, 1.6639415 , 1.64109779,\n",
       "       1.59527384])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_forecast = df_output.iloc[:,0].values\n",
    "y_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.05135894, 2.51810131, 2.07247393, 2.01343609, 0.23437483,\n",
       "       1.86092044])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_validation_sv[country].iloc[:,0].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
