{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Deep Reinforcement Learning framework\n",
    "\n",
    "Runnig the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 250)\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "# pytorch\n",
    "from torch import nn, no_grad, save, load\n",
    "from torch import from_numpy, zeros\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir = os.path.join(r'C:/Users/hauer/Documents/Repositories/cfds_project', 'database_weo.pickle')\n",
    "with open(database_dir,'rb') as f: \n",
    "    db = pickle.load(f)\n",
    "    \n",
    "database_training = db['database_training']\n",
    "database_validation = db['database_validation']\n",
    "database_test = db['database_test']\n",
    "\n",
    "database_training_sv = db['database_training_sv']\n",
    "database_validation_sv = db['database_validation_sv']\n",
    "database_test_sv = db['database_test_sv']\n",
    "\n",
    "database_training_sv_standard = db['database_training_sv_standard']\n",
    "database_validation_sv_standard = db['database_validation_sv_standard']\n",
    "database_test_sv_standard = db['database_test_sv_standard']\n",
    "\n",
    "database_scaler = db['database_scaler']\n",
    "y_forecast_WEO = db['y_forecast_WEO']\n",
    "y_forecast_OLS = db['y_forecast_WEO']\n",
    "y_forecast_GBM = db['y_forecast_WEO']\n",
    "y_forecast_RNN = db['y_forecast_RNN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\hauer\\Dropbox\\CFDS\\Project\\data2\\WEOhistorical.xlsx'\n",
    "df_weo =  pd.read_excel(path,sheet_name='ngdp_rpch')\n",
    "\n",
    "def get_predictions_weo(df_weo, country, start_forecast, end_forecast):\n",
    "       \n",
    "    df = df_weo[df_weo['country'] == country]\n",
    "    \n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'S' in col:\n",
    "            del df[col] \n",
    "            \n",
    "    del df['WEO_Country_Code']     \n",
    "    \n",
    "    \n",
    "    df = df[df['year'] >= start_forecast]\n",
    "    \n",
    "    \n",
    "    predictions_weo = []\n",
    "    years = np.arange(start_forecast, end_forecast+1, 1)\n",
    "    \n",
    "    for year in years:\n",
    "       \n",
    "        df_curr = df[df['year'] == year]\n",
    "        \n",
    "        year_WEO = year - 1 \n",
    "        column = 'F' + str(year_WEO) + 'ngdp_rpch'\n",
    "        y_pred_year = df_curr[column].values[0]\n",
    "        \n",
    "        predictions_weo.append(y_pred_year)\n",
    "    \n",
    "    predictions_weo = pd.Series(data = predictions_weo, index = years)\n",
    "    \n",
    "    return predictions_weo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, \n",
    "            n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "\n",
    "        self.optimizer = Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "            max_mem_size=100000, eps_end=0.05, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.iter_cntr = 0\n",
    "        self.replace_target = 100\n",
    "\n",
    "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
    "                                    fc1_dims=64, fc2_dims=32)\n",
    "\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = terminal\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def get_q_valid(self, q, valid_actions):\n",
    "        q_valid = [np.nan] * len(q)\n",
    "        for action in valid_actions:\n",
    "            q_valid[action] = q[action]\n",
    "        \n",
    "        return q_valid\n",
    "\n",
    "    def choose_action(self, observation, valid_actions):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation], dtype=T.float32).to(self.Q_eval.device)\n",
    "            q = self.Q_eval.forward(state)\n",
    "            q = q.detach().numpy().squeeze()\n",
    "            q = self.get_q_valid(q, valid_actions)\n",
    "            action = np.nanargmax(q)\n",
    "        else:\n",
    "            action = np.random.choice(valid_actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch + self.gamma*T.max(q_next,dim=1)[0]\n",
    "\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "\n",
    "        self.iter_cntr += 1\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
    "                       else self.eps_min\n",
    "\n",
    "def get_prediction(action, empty_status):\n",
    "    # Determines prediction on a given empty_status and action\n",
    "    # if empty, i. e. no stock is in depot, if action == 1 (buying) you bet on rising price\n",
    "    # if not empty, i. e. stock is in depot, if action == 0 (selling) you bet on falling price\n",
    "    \n",
    "    if empty_status:\n",
    "        if action == 1:\n",
    "            return 1\n",
    "        return -1\n",
    "    else:\n",
    "        if action == 0:\n",
    "            return -1\n",
    "        return 1\n",
    "\n",
    "class Environment():\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_epochs, window_size_observation, size_time_series, database):\n",
    "        self.name = 'gao'\n",
    "        self.n_epochs = n_epochs\n",
    "        self.window_size_observation = window_size_observation \n",
    "        self.size_time_series = size_time_series\n",
    "        self.current_game = 0\n",
    "        self.t_current = 0\n",
    "        self.empty = True\n",
    "        self.open_cost = 0\n",
    "        self.database = database\n",
    "        \n",
    "        self.list_of_games = self.get_list_of_games()\n",
    "        self.n_games = len(self.list_of_games)\n",
    "        self.t_max = self.size_time_series - self.window_size_observation - 1\n",
    "\n",
    "      \n",
    "    def get_status_emtpy(self):\n",
    "        return self.empty\n",
    "\n",
    "    \n",
    "    def get_current_df(self):\n",
    "        return self.list_of_games[self.current_game]\n",
    "\n",
    "        \n",
    "    def get_list_of_games(self):\n",
    "        list_of_games = []\n",
    "        \n",
    "        for i in range(self.n_epochs):\n",
    "            for country in self.database.keys():\n",
    "                list_of_games.append(self.database[country])\n",
    "                   \n",
    "        return list_of_games\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        if self.current_game == self.n_games - 1:\n",
    "            self.current_game = 0\n",
    "        else:\n",
    "            self.current_game += 1\n",
    "            \n",
    "        self.t_current = 0\n",
    "        \n",
    "        observation = self.get_observation()\n",
    "        \n",
    "        return observation\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        done = False\n",
    "        if action == 0:\t\t# wait/close\n",
    "            reward = 0.\n",
    "            self.empty = True\n",
    "        elif action == 1:\t# open\n",
    "            reward = self.get_reward_noncash()\n",
    "            self.empty = False\n",
    "        elif action == 2:\t# keep\n",
    "            reward = self.get_reward_noncash()\n",
    "        else:\n",
    "            raise ValueError('no valid action: ' + str(action))\n",
    "        \n",
    "        self.t_current += 1\n",
    "        #return self.get_state(), reward, self.t == self.t_max, self.get_valid_actions()\n",
    "        \n",
    "        \n",
    "        done = self.t_current == self.t_max\n",
    "        observation = self.get_observation()\n",
    "        info = self.get_valid_actions()\n",
    "        \n",
    "        return observation, reward, done, info\n",
    " \n",
    "    \n",
    "    def get_reward_noncash(self):\n",
    "        df_current = self.list_of_games[self.current_game]\n",
    "               \n",
    "        t_1 = self.t_current + self.window_size_observation + 1\n",
    "        t = self.t_current + self.window_size_observation \n",
    "\n",
    "        price_t_1 = df_current.iloc[t_1, 0]\n",
    "        price_t = df_current.iloc[t, 0]\n",
    "        \n",
    "        reward = price_t_1 - price_t\n",
    "        \n",
    "        if self.empty:\n",
    "            reward -= self.open_cost\n",
    "        \n",
    "        return reward \n",
    "       \n",
    "    \n",
    "    \n",
    "    def get_observation(self):\n",
    "        df_current = self.list_of_games[self.current_game]\n",
    "        \n",
    "        observation = df_current.iloc[self.t_current:(self.t_current + self.window_size_observation), :]\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        if self.empty:\n",
    "            return [0, 1]\t# wait, open\n",
    "        else:\n",
    "            return [0, 2]\t# close, keep \n",
    "        \n",
    "    def set_list_of_games(self, df):\n",
    "        self.list_of_games = [df]\n",
    "        self.n_games = 1\n",
    "  \n",
    "        \n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare final training database, i. e. the database that consits of the original training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_training_RL = {}\n",
    "\n",
    "for country in database_training_sv_standard.keys():\n",
    "    df_to_add = database_training_sv_standard[country].append(database_validation_sv_standard[country])\n",
    "    df_to_add = df_to_add.reset_index()\n",
    "    del df_to_add['index']\n",
    "\n",
    "    database_training_RL[country] = df_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0.0 score 0.30 average score 0.30 epsilon 1.00\n",
      "episode  1.0 score 0.06 average score -0.33 epsilon 0.99\n",
      "episode  2.0 score -0.00 average score -0.13 epsilon 0.99\n",
      "episode  3.0 score 0.48 average score 0.23 epsilon 0.98\n",
      "episode  4.0 score 0.23 average score 0.54 epsilon 0.97\n",
      "episode  5.0 score 0.09 average score 0.63 epsilon 0.96\n",
      "episode  6.0 score 0.15 average score 0.34 epsilon 0.96\n",
      "episode  7.0 score 0.17 average score 0.05 epsilon 0.95\n",
      "episode  8.0 score 0.44 average score 0.37 epsilon 0.94\n",
      "episode  9.0 score 0.15 average score 0.27 epsilon 0.93\n",
      "episode  10.0 score 0.34 average score 0.24 epsilon 0.93\n",
      "episode  11.0 score -0.10 average score 0.29 epsilon 0.92\n",
      "episode  12.0 score -0.40 average score -0.16 epsilon 0.91\n",
      "episode  13.0 score 0.32 average score -0.35 epsilon 0.91\n",
      "episode  14.0 score 0.25 average score -0.19 epsilon 0.90\n",
      "episode  15.0 score -0.17 average score 0.20 epsilon 0.89\n",
      "episode  16.0 score 0.02 average score 0.52 epsilon 0.88\n",
      "episode  17.0 score 0.49 average score 0.32 epsilon 0.88\n",
      "episode  18.0 score 0.23 average score 0.16 epsilon 0.87\n",
      "episode  19.0 score 0.08 average score 0.48 epsilon 0.86\n",
      "episode  20.0 score 0.02 average score 0.44 epsilon 0.85\n",
      "episode  21.0 score 0.23 average score 0.59 epsilon 0.85\n",
      "episode  22.0 score 0.15 average score 0.77 epsilon 0.84\n",
      "episode  23.0 score 0.53 average score 0.84 epsilon 0.83\n",
      "episode  24.0 score -0.02 average score 0.74 epsilon 0.82\n",
      "episode  25.0 score 0.08 average score 0.79 epsilon 0.82\n",
      "episode  26.0 score -0.06 average score 0.78 epsilon 0.81\n",
      "episode  27.0 score -0.17 average score 0.47 epsilon 0.80\n",
      "episode  28.0 score 0.27 average score 0.49 epsilon 0.80\n",
      "episode  29.0 score 0.32 average score 0.89 epsilon 0.79\n",
      "episode  30.0 score 0.12 average score 0.87 epsilon 0.78\n",
      "episode  31.0 score -0.00 average score 0.74 epsilon 0.77\n",
      "episode  32.0 score 0.19 average score 1.08 epsilon 0.77\n",
      "episode  33.0 score 0.02 average score 1.16 epsilon 0.76\n",
      "episode  34.0 score 0.15 average score 0.94 epsilon 0.75\n",
      "episode  35.0 score 0.61 average score 0.78 epsilon 0.74\n",
      "episode  36.0 score 0.46 average score 0.77 epsilon 0.74\n",
      "episode  37.0 score 0.32 average score 0.73 epsilon 0.73\n",
      "episode  38.0 score 0.51 average score 0.68 epsilon 0.72\n",
      "episode  39.0 score 0.02 average score 0.91 epsilon 0.71\n",
      "episode  40.0 score 0.27 average score 0.89 epsilon 0.71\n",
      "episode  41.0 score 0.67 average score 0.99 epsilon 0.70\n",
      "episode  42.0 score 0.15 average score 0.96 epsilon 0.69\n",
      "episode  43.0 score 0.21 average score 0.77 epsilon 0.68\n",
      "episode  44.0 score 0.06 average score 0.74 epsilon 0.68\n",
      "episode  45.0 score 0.49 average score 0.83 epsilon 0.67\n",
      "episode  46.0 score 0.67 average score 0.90 epsilon 0.66\n",
      "episode  47.0 score 0.48 average score 0.88 epsilon 0.66\n",
      "episode  48.0 score 0.30 average score 1.28 epsilon 0.65\n",
      "episode  49.0 score 0.59 average score 1.34 epsilon 0.64\n",
      "episode  50.0 score 0.09 average score 1.27 epsilon 0.63\n",
      "episode  51.0 score 0.29 average score 1.08 epsilon 0.63\n",
      "episode  52.0 score 0.44 average score 0.86 epsilon 0.62\n",
      "episode  53.0 score 0.34 average score 1.03 epsilon 0.61\n",
      "episode  54.0 score 0.40 average score 0.92 epsilon 0.60\n",
      "episode  55.0 score 0.44 average score 0.82 epsilon 0.60\n",
      "episode  56.0 score 0.23 average score 1.11 epsilon 0.59\n",
      "episode  57.0 score 0.53 average score 1.10 epsilon 0.58\n",
      "episode  58.0 score 0.38 average score 1.15 epsilon 0.57\n",
      "episode  59.0 score 0.07 average score 1.19 epsilon 0.57\n",
      "episode  60.0 score 0.34 average score 1.29 epsilon 0.56\n",
      "episode  61.0 score 0.46 average score 1.29 epsilon 0.55\n",
      "episode  62.0 score 0.34 average score 1.15 epsilon 0.54\n",
      "episode  63.0 score 0.59 average score 1.27 epsilon 0.54\n",
      "episode  64.0 score 0.06 average score 1.09 epsilon 0.53\n",
      "episode  65.0 score 0.13 average score 1.07 epsilon 0.52\n",
      "episode  66.0 score 0.08 average score 1.23 epsilon 0.52\n",
      "episode  67.0 score 0.38 average score 1.11 epsilon 0.51\n",
      "episode  68.0 score 0.36 average score 1.45 epsilon 0.50\n",
      "episode  69.0 score 0.25 average score 1.54 epsilon 0.49\n",
      "episode  70.0 score 0.59 average score 1.42 epsilon 0.49\n",
      "episode  71.0 score 0.61 average score 1.42 epsilon 0.48\n",
      "episode  72.0 score 0.04 average score 1.38 epsilon 0.47\n",
      "episode  73.0 score 0.12 average score 1.56 epsilon 0.46\n",
      "episode  74.0 score 0.09 average score 1.63 epsilon 0.46\n",
      "episode  75.0 score 0.53 average score 1.69 epsilon 0.45\n",
      "episode  76.0 score 0.44 average score 1.68 epsilon 0.44\n",
      "episode  77.0 score 0.15 average score 1.52 epsilon 0.43\n",
      "episode  78.0 score -0.02 average score 1.50 epsilon 0.43\n",
      "episode  79.0 score 0.50 average score 1.35 epsilon 0.42\n",
      "episode  80.0 score 0.09 average score 1.49 epsilon 0.41\n",
      "episode  81.0 score 0.44 average score 1.41 epsilon 0.40\n",
      "episode  82.0 score 0.46 average score 1.55 epsilon 0.40\n",
      "episode  83.0 score 0.23 average score 1.60 epsilon 0.39\n",
      "episode  84.0 score 0.67 average score 1.55 epsilon 0.38\n",
      "episode  85.0 score 0.53 average score 1.65 epsilon 0.38\n",
      "episode  86.0 score 0.67 average score 1.55 epsilon 0.37\n",
      "episode  87.0 score 0.38 average score 1.60 epsilon 0.36\n",
      "episode  88.0 score 0.59 average score 1.88 epsilon 0.35\n",
      "episode  89.0 score 0.65 average score 1.58 epsilon 0.35\n",
      "episode  90.0 score 0.50 average score 1.41 epsilon 0.34\n",
      "episode  91.0 score 0.38 average score 1.76 epsilon 0.33\n",
      "episode  92.0 score -0.02 average score 1.83 epsilon 0.32\n",
      "episode  93.0 score 0.67 average score 1.71 epsilon 0.32\n",
      "episode  94.0 score 0.61 average score 1.68 epsilon 0.31\n",
      "episode  95.0 score 0.29 average score 1.55 epsilon 0.30\n",
      "episode  96.0 score 0.53 average score 1.62 epsilon 0.29\n",
      "episode  97.0 score 0.36 average score 1.74 epsilon 0.29\n",
      "episode  98.0 score 0.23 average score 1.80 epsilon 0.28\n",
      "episode  99.0 score 0.44 average score 1.64 epsilon 0.27\n"
     ]
    }
   ],
   "source": [
    "N, p = database_training_sv_standard['Germany'].shape\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "window_size_observation = 15\n",
    "size_time_series = N\n",
    "\n",
    "\n",
    "agent = Agent(gamma=0.8, epsilon=1, batch_size=64, n_actions=3, eps_end=0.01, \n",
    "              input_dims=[window_size_observation * p], lr=0.001, eps_dec=2e-5)\n",
    "\n",
    "env = Environment(n_epochs, window_size_observation, size_time_series, database_training_RL)\n",
    "n_games = env.n_games\n",
    "\n",
    "n_countries = len(database_training_sv_standard.keys())\n",
    "\n",
    "scores, eps_history = [], []\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "\n",
    "    observation = env.reset()\n",
    "    valid_actions = [0, 1]\n",
    "\n",
    "    # take only signal as observation for now: \n",
    "    #observation = observation.iloc[:, 1:].values.squeeze()\n",
    "    observation= np.squeeze(observation.values.transpose().reshape((1, -1)))\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation, valid_actions)\n",
    "        observation_, reward, done, valid_actions = env.step(action)\n",
    "\n",
    "\n",
    "        # take only signal as observation for now: \n",
    "        #observation_ = observation_.iloc[:, 1:].values.squeeze()\n",
    "        observation_= np.squeeze(observation_.values.transpose().reshape((1, -1)))\n",
    "\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, \n",
    "                                observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    \n",
    "    if i % n_countries == 0:\n",
    "        print('episode ', i / n_countries, 'score %.2f' % score,\n",
    "                'average score %.2f' % avg_score,\n",
    "                'epsilon %.2f' % agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecast_GAO = {}\n",
    "\n",
    "\n",
    "for country in database_training_sv_standard.keys():\n",
    "\n",
    "    df_to_predict = database_training_sv_standard[country].append(database_validation_sv_standard[country]).append(database_test_sv_standard[country])\n",
    "    df_to_predict = df_to_predict.reset_index()\n",
    "    del df_to_predict['index']\n",
    "    n_forecast_validation = 6\n",
    "\n",
    "    N, p = df_to_predict.shape\n",
    "    size_time_series = N\n",
    "\n",
    "    env = Environment(n_epochs, window_size_observation, size_time_series, database_training_sv_standard)\n",
    "\n",
    "    env.set_list_of_games(df_to_predict)\n",
    "\n",
    "\n",
    "\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    actions, empty_status, rewards = [], [], []\n",
    "\n",
    "    for i in range(1):\n",
    "        score = 0\n",
    "        done = False\n",
    "\n",
    "        observation = env.reset()\n",
    "        valid_actions = [0, 1]\n",
    "\n",
    "        # take only signal as observation for now: \n",
    "        observation= np.squeeze(observation.values.transpose().reshape((1, -1)))\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation, valid_actions)\n",
    "            observation_, reward, done, valid_actions = env.step(action)\n",
    "\n",
    "\n",
    "            # take only signal as observation for now: \n",
    "            observation_= np.squeeze(observation_.values.transpose().reshape((1, -1)))\n",
    "\n",
    "            score += reward\n",
    "            observation = observation_\n",
    "\n",
    "\n",
    "            empty_status.append(env.get_status_emtpy())\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "\n",
    "\n",
    "        df = env.get_current_df()\n",
    "        predictions = [get_prediction(action, empty_status) for action, empty in zip(actions, empty_status)]\n",
    "\n",
    "        predictions = [0 for i in range(window_size_observation)] + predictions\n",
    "        rewards = [0 for i in range(window_size_observation)] + rewards\n",
    "        actions = [0 for i in range(window_size_observation)] + actions\n",
    "\n",
    "\n",
    "    y_forecast = predictions[-n_forecast_validation:]\n",
    "    y_forecast_GAO[country] = y_forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the prediction from the models, that where trained with the complete data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecast_WEO_RL = {}\n",
    "y_forecast_OLS_RL = {}\n",
    "y_forecast_GBM_RL = {}\n",
    "y_forecast_RNN_RL = {}\n",
    "\n",
    "\n",
    "for country in y_forecast_WEO.keys():\n",
    "    y_forecast_WEO_RL[country] = np.where(y_forecast_WEO[country].values > 0, 1, -1)\n",
    "    y_forecast_OLS_RL[country]  = np.where(y_forecast_OLS[country] > 0, 1, -1)\n",
    "    y_forecast_GBM_RL[country]  = np.where(y_forecast_GBM[country] > 0, 1, -1)\n",
    "    y_forecast_RNN_RL[country]  = np.where(y_forecast_RNN[country] > 0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_RL = pd.DataFrame(columns=['country', 'WEO', 'OLS', 'GBM', 'RNN', 'GAO'])\n",
    "\n",
    "for country in y_forecast_OLS_RL.keys():\n",
    "    \n",
    "    df_test_sv = database_test_sv[country]\n",
    "    y_real = df_test_sv['y'].values\n",
    "    y_real = np.where(y_real > 0, 1, -1)\n",
    "   \n",
    "   \n",
    "    mse_WEO = sum(y_real == y_forecast_WEO_RL[country]) / 6\n",
    "    mse_OLS = sum(y_real == y_forecast_OLS_RL[country]) / 6\n",
    "    mse_GBM = sum(y_real == y_forecast_GBM_RL[country]) / 6\n",
    "    mse_RNN = sum(y_real == y_forecast_RNN_RL[country]) / 6\n",
    "    mse_GAO = sum(y_real == y_forecast_GAO[country]) / 6\n",
    "\n",
    "\n",
    "    df_result_RL = pd.concat([pd.DataFrame([[country ,mse_WEO,mse_OLS,mse_GBM, mse_RNN, mse_GAO]],\n",
    "                                                          columns=df_result_RL.columns),\n",
    "                                             df_result_RL],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>WEO</th>\n",
       "      <th>OLS</th>\n",
       "      <th>GBM</th>\n",
       "      <th>RNN</th>\n",
       "      <th>GAO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slovak Republic</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Korea</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Latvia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Spain</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>China</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Australia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cyprus</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>France</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Slovenia</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Austria</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>United States</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Italy</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chile</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Germany</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Russia</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Israel</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Malta</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Canada</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Japan</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Poland</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Croatia</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Romania</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Estonia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Finland</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Colombia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Lithuania</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            country       WEO       OLS       GBM       RNN       GAO\n",
       "0            Turkey  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "1   Slovak Republic  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "2       Switzerland  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "3      South Africa  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "4             Korea  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "5            Latvia  1.000000  1.000000  1.000000  1.000000  0.500000\n",
       "6           Belgium  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "7            Mexico  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "8             Spain  0.666667  0.666667  0.666667  0.833333  0.333333\n",
       "9          Portugal  0.500000  0.500000  0.500000  1.000000  0.333333\n",
       "10            China  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "11          Ireland  1.000000  1.000000  1.000000  1.000000  0.500000\n",
       "12        Australia  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "13           Cyprus  0.500000  0.500000  0.500000  0.833333  0.666667\n",
       "14   United Kingdom  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "15           France  1.000000  1.000000  1.000000  1.000000  0.500000\n",
       "16          Denmark  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "17         Slovenia  0.666667  0.666667  0.666667  0.666667  0.333333\n",
       "18      New Zealand  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "19       Luxembourg  0.833333  0.833333  0.833333  0.833333  0.333333\n",
       "20          Austria  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "21        Argentina  0.666667  0.666667  0.666667  1.000000  0.166667\n",
       "22    United States  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "23       Costa Rica  1.000000  1.000000  1.000000  1.000000  0.500000\n",
       "24          Hungary  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "25        Indonesia  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "26            Italy  0.666667  0.666667  0.666667  0.833333  0.500000\n",
       "27   Czech Republic  0.833333  0.833333  0.833333  0.833333  0.166667\n",
       "28            Chile  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "29     Saudi Arabia  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "30          Germany  1.000000  1.000000  1.000000  1.000000  0.500000\n",
       "31           Brazil  0.833333  0.833333  0.833333  1.000000  0.166667\n",
       "32          Iceland  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "33           Russia  0.833333  0.833333  0.833333  1.000000  0.166667\n",
       "34           Israel  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "35            Malta  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "36           Canada  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "37            Japan  0.833333  0.833333  0.833333  0.833333  0.333333\n",
       "38           Poland  1.000000  1.000000  1.000000  1.000000  0.500000\n",
       "39          Croatia  0.833333  0.833333  0.833333  0.833333  0.166667\n",
       "40          Romania  1.000000  1.000000  1.000000  1.000000  0.166667\n",
       "41          Estonia  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "42          Finland  1.000000  1.000000  1.000000  1.000000  0.333333\n",
       "43         Colombia  1.000000  1.000000  1.000000  1.000000  0.500000\n",
       "44           Sweden  0.833333  0.833333  0.833333  0.833333  0.500000\n",
       "45        Lithuania  1.000000  1.000000  1.000000  1.000000  0.166667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_RL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
