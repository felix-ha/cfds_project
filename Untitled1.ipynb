{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "C:\\Users\\hauer\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 250)\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "# pytorch\n",
    "from torch import nn, no_grad, save, load\n",
    "from torch import from_numpy, zeros\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir = os.path.join(r'C:/Users/hauer/Documents/Repositories/cfds_project', 'database_new.pickle')\n",
    "with open(database_dir,'rb') as f: \n",
    "    db = pickle.load(f)\n",
    "    \n",
    "database_training = db['database_training']\n",
    "database_validation = db['database_validation']\n",
    "database_test = db['database_test']\n",
    "\n",
    "database_training_sv_new = db['database_training_sv']\n",
    "database_validation_sv_new = db['database_validation_sv']\n",
    "database_test_sv_new = db['database_test_sv']\n",
    "\n",
    "database_training_sv_standard = db['database_training_sv_standard']\n",
    "database_validation_sv_standard = db['database_validation_sv_standard']\n",
    "database_test_sv_standard = db['database_test_sv_standard']\n",
    "\n",
    "database_scaler = db['database_scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(database_training_sv_new.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n",
      "(24, 15)\n"
     ]
    }
   ],
   "source": [
    "for country in database_training_sv_new.keys():\n",
    "    print(database_training_sv_new[country].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir = os.path.join(r'C:/Users/hauer/Documents/Repositories/cfds_project', 'database.pickle')\n",
    "with open(database_dir,'rb') as f: \n",
    "    db = pickle.load(f)\n",
    "    \n",
    "database_training = db['database_training']\n",
    "database_validation = db['database_validation']\n",
    "database_test = db['database_test']\n",
    "\n",
    "database_training_sv_new = db['database_training_sv']\n",
    "database_validation_sv_new = db['database_validation_sv']\n",
    "database_test_sv_new = db['database_test_sv']\n",
    "\n",
    "database_training_sv_standard = db['database_training_sv_standard']\n",
    "database_validation_sv_standard = db['database_validation_sv_standard']\n",
    "database_test_sv_standard = db['database_test_sv_standard']\n",
    "\n",
    "database_scaler = db['database_scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(database_training_sv_new.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n",
      "(24, 7)\n"
     ]
    }
   ],
   "source": [
    "for country in database_training_sv_new.keys():\n",
    "    print(database_training_sv_new[country].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
