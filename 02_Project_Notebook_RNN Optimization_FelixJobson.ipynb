{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXVQdchiRTeQ"
   },
   "source": [
    "# RNN: Hyperparameter Optimization\n",
    "\n",
    "This notebook was run in Google Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "jwZuL8_1RTeo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for printing the definition of custom functions\n",
    "import inspect\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# pytorch\n",
    "from torch import nn, no_grad, save, load\n",
    "from torch import from_numpy, zeros\n",
    "from torch.optim import SGD\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark')\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "VIyQNzzMRTet"
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "n_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eK6c-EVGSWSf",
    "outputId": "bedd5e14-9351-4261-dce0-ba1f8f32a882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "wdir= os.getcwd()\n",
    "print(wdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VfyNpVJKRTev",
    "outputId": "32b98709-2a30-4c1c-ad85-feca6129cb55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "database_dir = os.path.join(wdir, 'database.pickle')\n",
    "\n",
    "with open(database_dir,'rb') as f: \n",
    "    db = pickle.load(f)\n",
    "    \n",
    "database_training = db['database_training']\n",
    "database_validation = db['database_validation']\n",
    "database_test = db['database_test']\n",
    "\n",
    "database_training_sv = db['database_training_sv']\n",
    "database_validation_sv = db['database_validation_sv']\n",
    "database_test_sv = db['database_test_sv']\n",
    "\n",
    "database_training_sv_standard = db['database_training_sv_standard']\n",
    "database_validation_sv_standard = db['database_validation_sv_standard']\n",
    "database_test_sv_standard = db['database_test_sv_standard']\n",
    "\n",
    "database_scaler = db['database_scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "doBvnD0WRTex"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RNN start\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# # Prepare Data for RNN\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Combining orignal training and \n",
    "database_training = {}\n",
    "\n",
    "for country in database_training_sv_standard.keys():\n",
    "    df_to_add = database_training_sv_standard[country].append(database_validation_sv_standard[country])\n",
    "    df_to_add = df_to_add.reset_index()\n",
    "    del df_to_add['index']\n",
    "\n",
    "    database_training[country] = df_to_add\n",
    "\n",
    "\n",
    "N, dummy_dim = database_training['Germany'].shape\n",
    "dummy_dim -= 1\n",
    "\n",
    "time_steps = 10\n",
    "horizon = 1\n",
    "sequence_length = time_steps + horizon \n",
    "\n",
    "\n",
    "max_index = N - sequence_length + 1\n",
    "\n",
    "number_of_countries = len(database_training.keys())\n",
    "\n",
    "X = np.empty([0, sequence_length,dummy_dim])\n",
    "y = np.empty([0, sequence_length])\n",
    "\n",
    " \n",
    "\n",
    "for country in database_training.keys():\n",
    "    df_training_current = database_training[country]\n",
    "\n",
    "    X_current = np.empty([max_index, sequence_length,dummy_dim])\n",
    "    y_current = np.empty([max_index, sequence_length])\n",
    "\n",
    "    for i in range(max_index):\n",
    "\n",
    "        X_current[i] = df_training_current.iloc[i:i+sequence_length,1:].values\n",
    "        y_current[i] = df_training_current.iloc[i:i+sequence_length,0].values\n",
    "        \n",
    "    X = np.concatenate((X, X_current))\n",
    "    y = np.concatenate((y, y_current))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "N, seq_len, dummy_dim = X.shape\n",
    "\n",
    "input_size=dummy_dim\n",
    "n_layers=1\n",
    "output_size=1\n",
    "test_size = 0.2\n",
    "batch_size = 25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=123)\n",
    "\n",
    "\n",
    "X_train_T = from_numpy(X_train).float()\n",
    "y_train_T = from_numpy(y_train).float()\n",
    "X_val_T = from_numpy(X_val).float()\n",
    "y_val_T = from_numpy(y_val).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_T, y_train_T)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size)  \n",
    "\n",
    "valid_ds = TensorDataset(X_val_T, y_val_T)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kdl5am83w0Hm",
    "outputId": "316d67e6-07d3-4846-8581-d61b095bde4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([720, 11, 6])"
      ]
     },
     "execution_count": 206,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3T0TwXIkw4qT",
    "outputId": "112fb0cc-313d-40b6-9207-a90892603cfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([180, 11, 6])"
      ]
     },
     "execution_count": 207,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKs4juorRTe0"
   },
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roKi92wARTe1",
    "outputId": "aa5c5c67-caa3-4a2d-cfcb-04c5a2d1a1e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 165.2 valid loss: 27.19\n",
      "Epoch 100: train loss: 160.1 valid loss: 25.96\n",
      "Epoch 200: train loss: 158.4 valid loss: 25.14\n",
      "Epoch 300: train loss: 157.5 valid loss: 24.63\n",
      "Epoch 400: train loss: 157.0 valid loss: 24.26\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN'\n",
    "hidden_dim=3\n",
    "lr = 0.03\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbO6kKe-RTe5"
   },
   "source": [
    "# Simple RNN Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCrX0rz0RTe5",
    "outputId": "1dfbb65f-5283-482e-eea0-786591e488ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 165.8 valid loss: 27.48\n",
      "Epoch 100: train loss: 165.7 valid loss: 27.47\n",
      "Epoch 200: train loss: 165.6 valid loss: 27.45\n",
      "Epoch 300: train loss: 165.5 valid loss: 27.44\n",
      "Epoch 400: train loss: 165.4 valid loss: 27.43\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN_Adam'\n",
    "hidden_dim=3\n",
    "lr = 1e-06\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    " \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4CWfBEYRTe9"
   },
   "source": [
    "# RNN Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRwqJOa5RTe_",
    "outputId": "4d4ff25e-0698-4a73-b9fd-8653971bba74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 163.7 valid loss: 27.28\n",
      "Epoch 100: train loss: 163.3 valid loss: 27.21\n",
      "Epoch 200: train loss: 163.0 valid loss: 27.16\n",
      "Epoch 300: train loss: 162.9 valid loss: 27.12\n",
      "Epoch 400: train loss: 162.7 valid loss: 27.1\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "    \n",
    "name = 'RNN_Large_Adam'\n",
    "hidden_dim=64\n",
    "lr = 1e-06\n",
    "\n",
    "model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    " \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))   \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weke4lJBRTfC"
   },
   "source": [
    "# Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KUwqrDDRTfD",
    "outputId": "8fb3cea2-0eb6-4d40-829a-bc57810e0a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 163.5 valid loss: 27.07\n",
      "Epoch 100: train loss: 155.2 valid loss: 25.24\n",
      "Epoch 200: train loss: 140.7 valid loss: 24.49\n",
      "Epoch 300: train loss: 112.0 valid loss: 23.16\n",
      "Epoch 400: train loss: 99.03 valid loss: 25.04\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, state):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden, state))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM'\n",
    "hidden_dim=10\n",
    "lr = 0.03\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "state_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0, state_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0, state_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54zh7RHxRTfF"
   },
   "source": [
    "# LSTM Adam Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgGYIfjHRTfF",
    "outputId": "8126bcdb-8165-467f-e92d-2ee0cadf6160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 163.2 valid loss: 27.08\n",
      "Epoch 100: train loss: 163.1 valid loss: 27.07\n",
      "Epoch 200: train loss: 163.0 valid loss: 27.05\n",
      "Epoch 300: train loss: 162.9 valid loss: 27.04\n",
      "Epoch 400: train loss: 162.9 valid loss: 27.03\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, state):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden, state))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM_Large_Adam'\n",
    "hidden_dim=64\n",
    "lr = 1e-06\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "hidden_0 = zeros(1, seq_len, hidden_dim)\n",
    "state_0 = zeros(1, seq_len, hidden_dim)\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_0, state_0)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_0, state_0)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}: train loss: {:.4} valid loss: {:.4}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_ZmtLDqRTfH"
   },
   "source": [
    "# Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4JlgZ02RTfH",
    "outputId": "dacea8a3-1fb8-4636-fa7b-853c720c956f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 163.10186 valid loss: 27.083196\n",
      "Epoch 25: train loss: 161.73081 valid loss: 26.755206\n",
      "Epoch 50: train loss: 157.9535 valid loss: 25.523838\n",
      "Epoch 75: train loss: 152.95524 valid loss: 24.271127\n",
      "Epoch 100: train loss: 144.09151 valid loss: 24.517672\n",
      "Epoch 125: train loss: 155.15516 valid loss: 25.328774\n",
      "Epoch 150: train loss: 144.16084 valid loss: 24.374774\n",
      "Epoch 175: train loss: 96.410418 valid loss: 23.098993\n",
      "Epoch 200: train loss: 53.714845 valid loss: 27.135338\n",
      "Epoch 225: train loss: 37.248569 valid loss: 27.701581\n",
      "Epoch 250: train loss: 33.643367 valid loss: 27.007507\n",
      "Epoch 275: train loss: 26.990821 valid loss: 26.660213\n",
      "Epoch 300: train loss: 25.520219 valid loss: 26.504651\n",
      "Epoch 325: train loss: 23.020821 valid loss: 26.029367\n",
      "Epoch 350: train loss: 20.060436 valid loss: 25.88581\n",
      "Epoch 375: train loss: 18.392862 valid loss: 25.840431\n",
      "Epoch 400: train loss: 17.330164 valid loss: 25.633932\n",
      "Epoch 425: train loss: 16.349625 valid loss: 25.373252\n",
      "Epoch 450: train loss: 14.38269 valid loss: 25.325208\n",
      "Epoch 475: train loss: 13.295862 valid loss: 25.433544\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "               \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_1, state_1, hidden_2, state_2):\n",
    "        r_out, (hidden_out, state_out) = self.lstm1(x, (hidden_1, state_1))      \n",
    "        r_out, (hidden_out, state_out) = self.lstm2(r_out, (hidden_2, state_2))\n",
    "        r_out = self.fc(r_out)\n",
    "        \n",
    "        return r_out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return zeros(1, self.seq_len, self.hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = 'LSTM_Stacked'\n",
    "hidden_dim=64\n",
    "lr = 0.05\n",
    "\n",
    "model = LSTMNet(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "optimizer = SGD(model.parameters(), lr = lr)  \n",
    "\n",
    "hidden_01 = zeros(1, seq_len, hidden_dim)\n",
    "state_01 = zeros(1, seq_len, hidden_dim)\n",
    "\n",
    "hidden_02 = zeros(1, seq_len, hidden_dim)\n",
    "state_02 = zeros(1, seq_len, hidden_dim)\n",
    "\n",
    "training_losses = np.empty(n_epochs)\n",
    "valid_losses = np.empty(n_epochs)\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# # Training loop \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch, hidden_01, state_01, hidden_02, state_02)\n",
    "        \n",
    "        loss = loss_func(y_pred.squeeze(), y_batch)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "       \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in valid_dl:\n",
    "            y_pred = model(X_batch, hidden_01, state_01, hidden_02, state_02)\n",
    "            loss = loss_func(y_pred.squeeze(), y_batch.squeeze()) \n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    training_loss_epoch = training_loss \n",
    "    valid_loss_epoch = valid_loss \n",
    "    \n",
    "    training_losses[epoch] = training_loss_epoch\n",
    "    valid_losses[epoch] = valid_loss_epoch\n",
    "    \n",
    "    if epoch % 25 == 0:\n",
    "        print('Epoch {}: train loss: {:.8} valid loss: {:.8}'\n",
    "              .format(epoch, training_loss_epoch, valid_loss_epoch))  \n",
    "        \n",
    "        \n",
    "models.append( (name, training_losses, valid_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "VWWAIlxkRTfJ"
   },
   "outputs": [],
   "source": [
    "x = [i for i in range(n_epochs)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(12,10))\n",
    "\n",
    "for model_ in models:\n",
    "    name = model_[0]\n",
    "    training_loss = model_[1] / 720\n",
    "    valid_loss =model_[2] /  180\n",
    "\n",
    "    \n",
    "    label = name \n",
    "    \n",
    "    ax[0].plot(x,  training_loss, label=label)\n",
    "    ax[1].plot(x,  valid_loss, label=label)\n",
    "    #ax[2].plot([i for i in range(n_epochs-1)],  valid_loss[1:] / valid_loss[:-1] * 100, label=label)\n",
    "\n",
    "\n",
    "#ax[2].set_xlabel('epoch') \n",
    "\n",
    "ax[0].set_ylabel('loss') \n",
    "ax[1].set_ylabel('loss')\n",
    "\n",
    "ax[0].set_title(\"training loss\")\n",
    "ax[1].set_title(\"validation loss\")\n",
    "#ax[2].set_title(\"validation loss change in %\")\n",
    "\n",
    "legend  = ax[0].legend(bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "#ax[2].grid()\n",
    "\n",
    "\n",
    "\n",
    "save_dir = os.path.join(wdir, 'result_all_models.png')\n",
    "\n",
    "plt.savefig(save_dir, dpi = 500, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlUyz16-RTfK"
   },
   "source": [
    "# Serializing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "2eD7cKuORTfK"
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# # Serializing model \n",
    "# =============================================================================\n",
    "\n",
    "#wdir= r'C:/Users/hauer/Documents/Repositories/cfds_project'\n",
    "save_dir = os.path.join(wdir, 'pytorch_models')\n",
    "model_name = 'rnn.torch'\n",
    "\n",
    "if(not os.path.isdir(save_dir)):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "save(model.state_dict(), os.path.join(save_dir, model_name))\n",
    "\n",
    "#model = RNN(input_size, seq_len, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "#model.load_state_dict(load( os.path.join(save_dir, model_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMTAKfuqRTfL"
   },
   "source": [
    "# Using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymbAwustRTfM",
    "outputId": "db852cb6-5e00-4a94-c5ba-df369951a9af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25307703,  0.38865948,  0.8269038 ,  0.0450716 , -1.069978  ,\n",
       "        0.8227167 ], dtype=float32)"
      ]
     },
     "execution_count": 216,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = 'Germany'\n",
    "\n",
    "df = database_training_sv_standard[country].append(database_validation_sv_standard[country])\n",
    "\n",
    "n_forecast_validation, _ = database_validation_sv_standard[country].shape\n",
    "\n",
    "X_eval = df.iloc[:,1:].values\n",
    "y_eval = df.iloc[:,0].values\n",
    "X_eval_T = from_numpy(X_eval).float()\n",
    "N, _ = X_eval_T.shape\n",
    "X_eval_T = X_eval_T.view([-1, N, dummy_dim])\n",
    "\n",
    "hidden_1 = zeros(1, N, hidden_dim)\n",
    "state_1 = zeros(1, N, hidden_dim)\n",
    "\n",
    "hidden_2 = zeros(1, N, hidden_dim)\n",
    "state_2 = zeros(1, N, hidden_dim)\n",
    "\n",
    "model.eval()\n",
    "with no_grad():\n",
    "    y_hat = model(X_eval_T, hidden_1, state_1, hidden_2, state_2)\n",
    "    \n",
    "y_hat =  y_hat.view(-1).numpy()\n",
    "y_forecast = y_hat[-n_forecast_validation:]\n",
    "y_forecast"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_rnn_optim.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
